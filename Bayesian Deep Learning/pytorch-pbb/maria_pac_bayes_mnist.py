# -*- coding: utf-8 -*-
"""Copy of PAC-Bayes on MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H-r7qSySMO9PybxSA9e5CpdS5kqKPFjr
"""


from __future__ import print_function
import numpy as np
from torchvision.datasets.utils import download_url, download_and_extract_archive, extract_archive, \
    makedir_exist_ok, verify_str_arg
import codecs
import os.path
import os
from PIL import Image
import warnings
import truncnormal
from torchvision.datasets.vision import VisionDataset
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributions as td
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
PMIN = 1e-3
"""# Models"""


class Gaussian(object):
    def __init__(self, mu, rho):
        super().__init__()
        self.mu = mu
        self.rho = rho
        self.normal = td.Normal(0, 1)

    @property
    def sigma(self):
        m = nn.Softplus()
        return m(self.rho)
        # return torch.pow(torch.exp(self.rho), 1/2)

    def sample(self):
        epsilon = self.normal.sample(self.rho.size()).to(DEVICE)
        return self.mu + self.sigma * epsilon

    def log_prob(self, input):
        return (-math.log(math.sqrt(2 * math.pi))
                - torch.log(self.sigma)
                - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()


class GaussianLinear(nn.Module):
    def __init__(self, in_features, out_features, init_sigma):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        init_rho = math.log(math.exp(init_sigma)-1.0)
        # np.log(init_sigma**2)

        # Set sigma for the truncated gaussian of weights
        sigma_weights = 1/np.sqrt(in_features)

        # Weight parameters
        self.weight_mu = nn.Parameter(truncnormal.trunc_normal_(torch.Tensor(
            out_features, in_features), 0, sigma_weights, -2*sigma_weights, 2*sigma_weights))
        self.weight_rho = nn.Parameter(
            torch.ones_like(self.weight_mu) * init_rho)
        self.weight = Gaussian(self.weight_mu, self.weight_rho)

        # Bias parameters
        self.bias_mu = nn.Parameter(torch.zeros(out_features))
        self.bias_rho = nn.Parameter(torch.ones_like(self.bias_mu) * init_rho)
        self.bias = Gaussian(self.bias_mu, self.bias_rho)

    def forward(self, x):
        weight = self.weight.sample()
        bias = self.bias.sample()

        return F.linear(x, weight, bias)


class Net(nn.Module):
    def __init__(self, init_sigma, clipping='relu'):
        super().__init__()
        self.clipping = clipping
        self.l1 = GaussianLinear(784, 600, init_sigma)
        self.l2 = GaussianLinear(600, 600, init_sigma)
        self.l3 = GaussianLinear(600, 600, init_sigma)
        self.lout = GaussianLinear(600, 10, init_sigma)

    def output_transform(self, x):
        # lower bound output prob
        if self.clipping == 'relu':
            x = F.relu(x + 10) - 10
        elif self.clipping == 'tanh':
            x = F.tanh(x/6) * 6
        elif self.clipping == 'hard':
            pass
        elif self.clipping == 'none':
            pass
        else:
            assert False

        output = F.log_softmax(x, dim=1)
        if self.clipping == 'hard':
            output = torch.clamp(output, np.log(PMIN))
        return output

    def forward(self, x):
        x = torch.flatten(x, 1, -1)

        x = self.l1(x)
        x = F.relu(x)
        x = self.l2(x)
        x = F.relu(x)
        x = self.l3(x)
        x = F.relu(x)
        x = self.lout(x)
        return self.output_transform(x)

    # THIS IS A BIT HACKY

    def get_means(self):
        return [p for i, p in enumerate(self.parameters()) if i % 2 == 0]

    def get_sigmas(self):
        return [F.softplus(p) for i, p in enumerate(self.parameters()) if i % 2 == 1]


"""# Utils"""


def kl_to_prior(means, sigmas, prior_means, prior_sigmas):
    means = torch.cat([m.flatten() for m in means])
    prior_means = torch.cat([m.flatten() for m in prior_means])

    sigmas = torch.cat([s.flatten() for s in sigmas])
    prior_sigmas = torch.cat([s.flatten() for s in prior_sigmas])

    mu_diff = means - prior_means
    mu_term = (mu_diff**2 / prior_sigmas**2).sum()

    sigma_tr_term = (sigmas**2 / (prior_sigmas**2)).sum()

    # take the log first to make everything linear,
    # then do the sum-of-differences instead of the difference-of-sums
    # to make the numerics better
    log_det_term = 2 * (torch.log(prior_sigmas) - torch.log(sigmas)).sum()

    k = means.shape[0]

    # group calculation of (sigma_tr + log_det - k) to avoid numerical issues;
    # each term is large but the difference is ~0
    kl = 0.5 * (mu_term + (sigma_tr_term + log_det_term - k))

    var1 = sigmas ** 2
    var0 = prior_sigmas ** 2

    aux1 = torch.log(torch.div(var0, var1))
    aux2 = torch.div(
        torch.pow(means - prior_means, 2), var0)
    aux3 = torch.div(var1, var0)
    kl_div = torch.mul(aux1 + aux2 + aux3 - 1, 0.5).sum()

    return kl_div, 0.5 * mu_term, 0.5 * sigma_tr_term, 0.5 * log_det_term, -0.5 * k


def differentiable_bound(empirical_risk,
                         means, sigmas, prior_means, prior_sigmas,
                         delta=(1-0.95), n=60000):

    kl, _, _, _, _ = kl_to_prior(means, sigmas, prior_means, prior_sigmas)
    kl_ratio = (kl + np.log(2 * np.sqrt(n) / delta)) / (2 * n)
    return ((empirical_risk + kl_ratio)**0.5 + (kl_ratio)**0.5)**2


def dziugaite_bound(empirical_risk, means, sigmas, prior_means, prior_sigmas,
                    delta=(1-0.95), n=60000):
    kl, _, _, _, _ = kl_to_prior(means, sigmas, prior_means, prior_sigmas)
    gap = ((kl + math.log(n / delta)) / (2 * (n - 1))) ** 0.5
    return empirical_risk + gap


"""# Data"""

# @title MNIST loader
# Stolen from torchvision and made faster.


class MNIST(VisionDataset):
    """`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.

    Args:
        root (string): Root directory of dataset where ``MNIST/processed/training.pt``
            and  ``MNIST/processed/test.pt`` exist.
        train (bool, optional): If True, creates dataset from ``training.pt``,
            otherwise from ``test.pt``.
        download (bool, optional): If true, downloads the dataset from the internet and
            puts it in root directory. If dataset is already downloaded, it is not
            downloaded again.
        transform (callable, optional): A function/transform that  takes in an PIL image
            and returns a transformed version. E.g, ``transforms.RandomCrop``
        target_transform (callable, optional): A function/transform that takes in the
            target and transforms it.
    """

    resources = [
        ("http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz",
         "f68b3c2dcbeaaa9fbdd348bbdeb94873"),
        ("http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz",
         "d53e105ee54ea40749a09fcbcd1e9432"),
        ("http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz",
         "9fb629c4189551a2d022fa330f9573f3"),
        ("http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz",
         "ec29112dd5afa0611ce80d1b7f02629c")
    ]

    training_file = 'training.pt'
    test_file = 'test.pt'
    classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four',
               '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']

    @property
    def train_labels(self):
        warnings.warn("train_labels has been renamed targets")
        return self.targets

    @property
    def test_labels(self):
        warnings.warn("test_labels has been renamed targets")
        return self.targets

    @property
    def train_data(self):
        warnings.warn("train_data has been renamed data")
        return self.data

    @property
    def test_data(self):
        warnings.warn("test_data has been renamed data")
        return self.data

    def __init__(self, root, train=True, transform=None, target_transform=None,
                 download=False):
        super(MNIST, self).__init__(root, transform=transform,
                                    target_transform=target_transform)
        self.train = train  # training set or test set

        if download:
            self.download()

        if not self._check_exists():
            raise RuntimeError('Dataset not found.' +
                               ' You can use download=True to download it')

        if self.train:
            data_file = self.training_file
        else:
            data_file = self.test_file
        self.data, self.targets = torch.load(
            os.path.join(self.processed_folder, data_file))

        self.cache = {}

    def __getitem__(self, index):
        """
        Args:
            index (int): Index

        Returns:
            tuple: (image, target) where target is index of the target class.
        """
        if index in self.cache:
            return self.cache[index]
        img, target = self.data[index], int(self.targets[index])

        img = img.unsqueeze_(0).float().div_(255)
        if self.transform is not None:
            img = self.transform(img)

        if self.target_transform is not None:
            target = self.target_transform(target)

        self.cache[index] = (img, target)
        return self.cache[index]

    def __len__(self):
        return len(self.data)

    @property
    def raw_folder(self):
        return os.path.join(self.root, self.__class__.__name__, 'raw')

    @property
    def processed_folder(self):
        return os.path.join(self.root, self.__class__.__name__, 'processed')

    @property
    def class_to_idx(self):
        return {_class: i for i, _class in enumerate(self.classes)}

    def _check_exists(self):
        return (os.path.exists(os.path.join(self.processed_folder,
                                            self.training_file)) and
                os.path.exists(os.path.join(self.processed_folder,
                                            self.test_file)))

    def download(self):
        """Download the MNIST data if it doesn't exist in processed_folder already."""

        if self._check_exists():
            return

        makedir_exist_ok(self.raw_folder)
        makedir_exist_ok(self.processed_folder)

        # download files
        for url, md5 in self.resources:
            filename = url.rpartition('/')[2]
            download_and_extract_archive(
                url, download_root=self.raw_folder, filename=filename, md5=md5)

        # process and save as torch files
        print('Processing...')

        training_set = (
            read_image_file(os.path.join(
                self.raw_folder, 'train-images-idx3-ubyte')),
            read_label_file(os.path.join(
                self.raw_folder, 'train-labels-idx1-ubyte'))
        )
        test_set = (
            read_image_file(os.path.join(
                self.raw_folder, 't10k-images-idx3-ubyte')),
            read_label_file(os.path.join(
                self.raw_folder, 't10k-labels-idx1-ubyte'))
        )
        with open(os.path.join(self.processed_folder, self.training_file), 'wb') as f:
            torch.save(training_set, f)
        with open(os.path.join(self.processed_folder, self.test_file), 'wb') as f:
            torch.save(test_set, f)

        print('Done!')

    def extra_repr(self):
        return "Split: {}".format("Train" if self.train is True else "Test")


def get_int(b):
    return int(codecs.encode(b, 'hex'), 16)


def open_maybe_compressed_file(path):
    """Return a file object that possibly decompresses 'path' on the fly.
       Decompression occurs when argument `path` is a string and ends with '.gz' or '.xz'.
    """
    if not isinstance(path, torch._six.string_classes):
        return path
    if path.endswith('.gz'):
        import gzip
        return gzip.open(path, 'rb')
    if path.endswith('.xz'):
        import lzma
        return lzma.open(path, 'rb')
    return open(path, 'rb')


def read_sn3_pascalvincent_tensor(path, strict=True):
    """Read a SN3 file in "Pascal Vincent" format (Lush file 'libidx/idx-io.lsh').
       Argument may be a filename, compressed filename, or file object.
    """
    # typemap
    if not hasattr(read_sn3_pascalvincent_tensor, 'typemap'):
        read_sn3_pascalvincent_tensor.typemap = {
            8: (torch.uint8, np.uint8, np.uint8),
            9: (torch.int8, np.int8, np.int8),
            11: (torch.int16, np.dtype('>i2'), 'i2'),
            12: (torch.int32, np.dtype('>i4'), 'i4'),
            13: (torch.float32, np.dtype('>f4'), 'f4'),
            14: (torch.float64, np.dtype('>f8'), 'f8')}
    # read
    with open_maybe_compressed_file(path) as f:
        data = f.read()
    # parse
    magic = get_int(data[0:4])
    nd = magic % 256
    ty = magic // 256
    assert nd >= 1 and nd <= 3
    assert ty >= 8 and ty <= 14
    m = read_sn3_pascalvincent_tensor.typemap[ty]
    s = [get_int(data[4 * (i + 1): 4 * (i + 2)]) for i in range(nd)]
    parsed = np.frombuffer(data, dtype=m[1], offset=(4 * (nd + 1)))
    assert parsed.shape[0] == np.prod(s) or not strict
    return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)


def read_label_file(path):
    with open(path, 'rb') as f:
        x = read_sn3_pascalvincent_tensor(f, strict=False)
    assert(x.dtype == torch.uint8)
    assert(x.ndimension() == 1)
    return x.long()


def read_image_file(path):
    with open(path, 'rb') as f:
        x = read_sn3_pascalvincent_tensor(f, strict=False)
    assert(x.dtype == torch.uint8)
    assert(x.ndimension() == 3)
    return x


kwargs = {'num_workers': 0, 'pin_memory': True} if torch.cuda.is_available() else {
}
batch_size = 256

train_loader = torch.utils.data.DataLoader(
    MNIST('../data', train=True, download=True,
          transform=transforms.Compose([
              transforms.Normalize((0.1307,), (0.3081,))
          ])),
    batch_size=batch_size, shuffle=True, **kwargs)
test_loader = torch.utils.data.DataLoader(
    MNIST('../data', train=False, transform=transforms.Compose([
        transforms.Normalize((0.1307,), (0.3081,))
    ])),
    batch_size=batch_size, shuffle=True, **kwargs)

"""# Training loop"""


def train_noisy(model, train_loader, optimizer, epoch, prior_means, prior_sigmas,
                objective='pbb', penalty=1):
    model.train()

    def compute_bound(pred_err, means, sigmas):
        if objective == 'dziugaite':
            bound, kl = dziugaite_bound(
                pred_err, means, sigmas, prior_means, prior_sigmas)
            return bound
        elif objective == 'kl':
            kl, _, _, _, _ = kl_to_prior(
                means, sigmas, prior_means, prior_sigmas)
            return pred_err + penalty * kl
        elif objective == 'pbb':
            return differentiable_bound(pred_err, means, sigmas, prior_means, prior_sigmas)
        else:
            assert False

    mean_pred_err = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        # get data
        data, target = data.to(DEVICE), target.to(DEVICE)

        # calculate loss
        output = model(data)
        pred_err = F.nll_loss(output, target)
        mean_pred_err += pred_err.item()

        means = model.get_means()
        sigmas = model.get_sigmas()

        bound = compute_bound(pred_err, means, sigmas)

        # take step
        optimizer.zero_grad()
        bound.backward()
        optimizer.step()

    mean_pred_err /= batch_idx
    means = model.get_means()
    sigmas = model.get_sigmas()
    avg_surr_bound = compute_bound(mean_pred_err, means, sigmas)
    print('Train Epoch: {} Batch: {} \t LR: {:.3e} \t Log loss: {:.6f}\t Surrogate bound: {:.6f}'.format(
        epoch, batch_idx, optimizer.param_groups[0]['lr'], mean_pred_err, avg_surr_bound))


"""# Evaluation"""


def eval_noisy(model, eval_loader, prior_means, prior_sigmas, name, length):
    model.eval()
    test_loss = torch.zeros(1).to(DEVICE)
    correct = 0
    samples = 0
    with torch.no_grad():
        for data, target in eval_loader:
            data, target = data.to(DEVICE), target.to(DEVICE)

            output = model(data)

            # sum up batch loss
            test_loss += F.nll_loss(output, target, reduction='sum')
            # get the index of the max log-probability
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            samples += len(data)

            if samples >= length:
                break

    test_loss /= samples
    incorrect = samples - correct

    means = model.get_means()
    sigmas = model.get_sigmas()

    test_risk = incorrect / samples
    dz_bound = dziugaite_bound(
        test_risk, means, sigmas, prior_means, prior_sigmas)
    pbb_bound = differentiable_bound(
        test_risk, means, sigmas, prior_means, prior_sigmas)
    kl, mean_term, sigma_tr_term, log_det_term, k_term = kl_to_prior(
        means, sigmas, prior_means, prior_sigmas)

    flat_sigmas = torch.cat([p.flatten() for p in model.get_sigmas()], 0)
    flat_prior_sigmas = torch.cat([p.flatten() for p in prior_sigmas], 0)
    print(('{} set: Error: {}/{} ({:.0f}%), Log loss: {:.6f}, Dziugaite bound: {:.6f}, PBB bound: {:.6f}, '
           'KL: {:.6f}, Mean term: {:.6f}, Sigma_tr term: {:.6f}, Log-det term: {:.6f}, k term: {:.6f}').format(
        name,
        incorrect, samples, 100. * incorrect /
        samples, test_loss.item(), dz_bound.item(), pbb_bound.item(),
        kl, mean_term, sigma_tr_term, log_det_term, k_term))


"""# Main"""

# Optimizing f_{quad} using the hyperparameters from Vikram

prior_sigma = 3e-2
model = Net(prior_sigma, clipping='hard').to(DEVICE)

prior_means = [p.clone().detach() for p in model.get_means()]
prior_sigmas = [p.clone().detach() for p in model.get_sigmas()]

optimizer = optim.SGD(model.parameters(), lr=5e-3, momentum=0.95)

for epoch in range(0, 3500):
    print("Epoch:", epoch)

    if epoch % 10 == 0:
        eval_noisy(model, train_loader, prior_means,
                   prior_sigmas, "Train", 60000)
        eval_noisy(model, test_loader, prior_means,
                   prior_sigmas, "Test", 10000)
    train_noisy(model, train_loader, optimizer,
                epoch, prior_means, prior_sigmas)
    print()
