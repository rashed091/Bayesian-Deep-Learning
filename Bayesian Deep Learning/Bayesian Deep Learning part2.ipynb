{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLSS2019: Bayesian Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will uncertainty estimation can be\n",
    "used in active learning or expert-in-the-loop pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan of the tutorial\n",
    "1. [Imports and definitions](#Imports-and-definitions)\n",
    "2. [Bayesian Active Learning with images](#Bayesian-Active-Learning-with-images)\n",
    "   1. [The model](#The-model)\n",
    "   2. [the Acquisition Function](#the-Acquisition-Function)\n",
    "   3. [Data and the Oracle](#Data-and-the-Oracle)\n",
    "   4. [the Active Learning loop](#the-Active-Learning-loop)\n",
    "   5. [The baseline](#The-baseline)\n",
    "3. [Bayesian Active Learning by Disagreement](#Bayesian-Active-Learning-by-Disagreement)\n",
    "   1. [Points of improvement: batch-vs-single](#Points-of-improvement:-batch-vs-single)\n",
    "   2. [Points of improvement: bias](#Points-of-improvement:-bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)**\n",
    "* to view documentation on something  type in `something?` (with one question mark)\n",
    "* to view code of something type in `something??` (with two question marks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we import necessary modules and functions and\n",
    "define the computational device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install some boilerplate service code for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade git+https://github.com/ivannz/mlss2019-bayesian-deep-learning.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, numpy for computing, matplotlib for plotting and tqdm for progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deep learning stuff will be using [pytorch](https://pytorch.org/).\n",
    "\n",
    "If you are unfamiliar with it, it is basically like `numpy` with autograd,\n",
    "native GPU support, and tools for building training and serializing models.\n",
    "<!-- (and with `axis` argument replaced with `dim` :) -->\n",
    "\n",
    "There are good introductory tutorials on `pytorch`, like this\n",
    "[one](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the boilerplate code.\n",
    "\n",
    "* a procedure that implements a minibatch SGD **fit** loop\n",
    "* a function, that **evaluates** the model on the provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl import fit, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm to sample a random function is:\n",
    "* for $b = 1... B$ do:\n",
    "\n",
    "  1. draw an independent realization $f_b\\colon \\mathcal{X} \\to \\mathcal{Y}$\n",
    "  with from the process $\\{f_\\omega\\}_{\\omega \\sim q(\\omega)}$\n",
    "  2. get $\\hat{y}_{bi} = f_b(\\tilde{x}_i)$ for $i=1 .. m$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl.bdl import freeze, unfreeze\n",
    "\n",
    "def sample_function(model, dataset, n_draws=1, verbose=False):\n",
    "    \"\"\"Draw a realization of a random function.\"\"\"\n",
    "    outputs = []\n",
    "    for _ in tqdm.tqdm(range(n_draws), disable=not verbose):\n",
    "        freeze(model)\n",
    "\n",
    "        outputs.append(predict(model, dataset))\n",
    "\n",
    "    unfreeze(model)\n",
    "\n",
    "    return torch.stack(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample the class probabilities $p(y_x = k \\mid x, \\omega, m)$\n",
    "with $\\omega \\sim q(\\omega)$ by a model that **outputs raw class\n",
    "logit scores**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_proba(model, dataset, n_draws=1):\n",
    "    logits = sample_function(model, dataset, n_draws=n_draws)\n",
    "\n",
    "    return F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictive posterior class probabilities\n",
    "$$\n",
    "p(y_x = k \\mid x, m)\n",
    "%     = \\mathbb{E}_{\\omega \\sim q(\\omega)}\n",
    "%         p(y_x = k \\mid x, \\omega, m)\n",
    "    \\approx \\frac1{\\lvert \\mathcal{W} \\rvert}\n",
    "        \\sum_{\\omega \\in \\mathcal{W}}\n",
    "            p(y_x = k \\mid x, \\omega, m)\n",
    "    \\,, $$\n",
    "with $\\mathcal{W}$ -- iid draws from $q(\\omega)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model, dataset, n_draws=1):\n",
    "    proba = sample_proba(model, dataset, n_draws=n_draws)\n",
    "\n",
    "    return proba.mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gat the maximum a posteriori class label **(MAP)**: $\n",
    "\\hat{y}_x\n",
    "    = \\arg \\max_k \\mathbb{E}_{\\omega \\sim q(\\omega)}\n",
    "        p(y_x = k \\mid x, \\omega, m)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(model, dataset, n_draws=1):\n",
    "    proba = predict_proba(model, dataset, n_draws=n_draws)\n",
    "\n",
    "    return proba.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need some functionality from scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def evaluate(model, dataset, n_draws=1):\n",
    "    assert isinstance(dataset, TensorDataset)\n",
    "\n",
    "    predicted = predict_label(model, dataset, n_draws=n_draws)\n",
    "\n",
    "    target = dataset.tensors[1].cpu().numpy()\n",
    "    return confusion_matrix(target, predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to plot images in a small dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl.flex import plot\n",
    "from torch.utils.data import TensorDataset\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def display(images, n_col=None, title=None, figsize=None, refresh=False):\n",
    "    if isinstance(images, TensorDataset):\n",
    "        images, targets = images.tensors\n",
    "    \n",
    "    if refresh:\n",
    "        clear_output(True)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    plot(ax, images, n_col=n_col, cmap=plt.cm.bone)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Active Learning with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data labelling is costly and time consuming\n",
    "* unlabeled instances are essentially free\n",
    "\n",
    "**Goal** Achieve high performance with fewer labels by\n",
    "identifying the best instances to learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essential blocks of active learning:\n",
    "\n",
    "* a **model** $m$ capable of quantifying uncertainty (preferably a Bayesian model)\n",
    "* an **acquisition function** $a\\colon \\mathcal{M} \\times \\mathcal{X}^* \\to \\mathbb{R}$\n",
    "  that for any finite set of inputs $S\\subset \\mathcal{X}$ quantifies their usefulness\n",
    "  to the model $m\\in \\mathcal{M}$\n",
    "* a labelling **oracle**, e.g. a human expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reuse the `DropoutLinear` from the first part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Sequential\n",
    "from torch.nn import AvgPool2d, LeakyReLU\n",
    "from torch.nn import Linear, Conv2d\n",
    "\n",
    "from mlss2019bdl.bdl import DropoutLinear, DropoutConv2d\n",
    "\n",
    "class MNISTModel(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = Sequential(\n",
    "            Conv2d(1, 32, 3, 1),\n",
    "            LeakyReLU(),\n",
    "            DropoutConv2d(32, 64, 3, 1, p=p),\n",
    "            LeakyReLU(),\n",
    "            AvgPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.tail = Sequential(\n",
    "            DropoutLinear(12 * 12 * 64, 128, p=p),\n",
    "            LeakyReLU(),\n",
    "            DropoutLinear(128, 10, p=p),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Take images and compute their class logits.\"\"\"\n",
    "        x = self.head(input)\n",
    "        return self.tail(x.flatten(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the Acquisition Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many acquisition criteria (borrowed from [Gal17a](http://proceedings.mlr.press/v70/gal17a.html)):\n",
    "* Classification\n",
    "  * Posterior predictive entropy\n",
    "  * Posterior Mutual Information\n",
    "  * Variance ratios\n",
    "  * BALD\n",
    "\n",
    "* Regression\n",
    "  * predictive variance\n",
    "\n",
    "... and there is always the baseline **random acquisition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(812_760_351)\n",
    "\n",
    "def random_acquisition(dataset, model, n_request=1, n_draws=1):\n",
    "    indices = random_state.choice(len(dataset), size=n_request)\n",
    "\n",
    "    return torch.from_numpy(indices).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and the Oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the datasets from the `train` part of\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/)\n",
    "(or [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist)):\n",
    "* ($\\mathcal{S}_\\mathrm{train}$) initial **training**: $30$ images\n",
    "* ($\\mathcal{S}_\\mathrm{valid}$) our **validation**:\n",
    "  $5000$ images, stratified\n",
    "* ($\\mathcal{S}_\\mathrm{pool}$) acquisition **pool**:\n",
    "  $5000$ of the unused images, skewed to class $0$\n",
    "\n",
    "The true test sample of MNIST is in $\\mathcal{S}_\\mathrm{test}$ -- we\n",
    "will use it to evaluate the final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl.dataset import get_dataset\n",
    "\n",
    "S_train, S_pool, S_valid, S_test = get_dataset(\n",
    "    n_train=30,\n",
    "    n_valid=5000,\n",
    "    n_pool=5000,\n",
    "    name=\"MNIST\",  # \"KMNIST\"\n",
    "    path=\"./data\",\n",
    "    random_state=722_257_201)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `query_oracle(ix, D)` **request** the instances in `D` at the specified\n",
    "  indices `ix` into a dataset and **remove** from them from `D`\n",
    "\n",
    "* `merge(*datasets, [out=])` merge the datasets, creting a new one, or replacing `out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl.dataset import collect as query_oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the Active Learning loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. fit $m$ on $\\mathcal{S}_{\\mathrm{labelled}}$\n",
    "\n",
    "\n",
    "2. get exact (or approximate) $$\n",
    "    \\mathcal{S}^* \\in \\arg \\max\\limits_{S \\subseteq \\mathcal{S}_\\mathrm{unlabelled}}\n",
    "        a(m, S)\n",
    "$$ satisfying **budget constraints** and **without** access to targets\n",
    "(constraints, like $\\lvert S \\rvert \\leq \\ell$ or other economically motivated ones).\n",
    "\n",
    "\n",
    "3. request the **oracle** to provide labels for each $x\\in \\mathcal{S}^*$\n",
    "\n",
    "\n",
    "4. update $\n",
    "\\mathcal{S}_{\\mathrm{labelled}}\n",
    "    \\leftarrow \\mathcal{S}^*\n",
    "        \\cup \\mathcal{S}_{\\mathrm{labelled}}\n",
    "$ and goto 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from mlss2019bdl.dataset import merge\n",
    "\n",
    "def active_learn(S_train,\n",
    "                 S_pool,\n",
    "                 S_valid,\n",
    "                 acquire_fn,\n",
    "                 n_budget=150,\n",
    "                 n_max_request=3,\n",
    "                 n_draws=11,\n",
    "                 n_epochs=200,\n",
    "                 p=0.5,\n",
    "                 weight_decay=1e-2):\n",
    "\n",
    "    model = MNISTModel(p=p).to(device)\n",
    "\n",
    "    scores, balances = [], []\n",
    "    S_train, S_pool = copy.deepcopy(S_train), copy.deepcopy(S_pool)\n",
    "    while True:\n",
    "        # 1. fit on train\n",
    "        l2_reg = weight_decay * (1 - p) / max(len(S_train), 1)\n",
    "\n",
    "        model = fit(model, S_train, batch_size=32, criterion=\"cross_entropy\",\n",
    "                    weight_decay=l2_reg, n_epochs=n_epochs)\n",
    "\n",
    "\n",
    "        # (optional) keep track of scores and plot the train dataset\n",
    "        scores.append(evaluate(model, S_valid, n_draws))\n",
    "        balances.append(np.bincount(S_train.tensors[1], minlength=10))\n",
    "\n",
    "        accuracy = scores[-1].diagonal().sum() / scores[-1].sum()\n",
    "        title = f\"(n_train) {len(S_train)} (Acc.) {accuracy:.1%}\"\n",
    "        display(S_train, n_col=30, figsize=(15, 5), title=title, refresh=True)\n",
    "\n",
    "\n",
    "        # 2-3. request new data from pool, if within budget\n",
    "        n_request = min(n_budget - len(S_train), n_max_request)\n",
    "        if n_request <= 0:\n",
    "            break\n",
    "\n",
    "        indices = acquire_fn(S_pool, model, n_request=n_request, n_draws=n_draws)\n",
    "\n",
    "        # 4. update the train dataset\n",
    "        S_requested = query_oracle(indices, S_pool)\n",
    "        S_train = merge(S_train, S_requested)\n",
    "\n",
    "    return model, S_train, np.stack(scores, axis=0), np.stack(balances, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `collect(ix, D)` **collect** the instances in `D` at the specified\n",
    "  indices `ix` into a dataset and **remove** from them from `D`\n",
    "\n",
    "* `merge(*datasets, [out=])` merge the datasets, creting a new one, or replacing `out`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How powerful will our model with random acquisition get under a total budget of $150$ images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = active_learn(\n",
    "    S_train,\n",
    "    S_pool,\n",
    "    S_valid,\n",
    "    random_acquisition,\n",
    "    n_draws=21,\n",
    "    n_budget=150,\n",
    "    n_max_request=3,\n",
    "    n_epochs=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the dynamics of the accuracy ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(scores):\n",
    "    tp = scores.diagonal(axis1=-2, axis2=-1)\n",
    "    return tp.sum(-1) / scores.sum((-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rand, train_rand, scores_rand, balances_rand = baseline\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "ax.plot(accuracy(scores_rand), label='Accuracy (random)', lw=2)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..., and the frequency of each class in $\\mathcal{S}_\\mathrm{train}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "lines = ax.plot(balances_rand, lw=2)\n",
    "plt.legend(lines, list(range(10)), ncol=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Active Learning by Disagreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Active Learning by Disagreement, or **BALD** criterion, is\n",
    "based on the posterior mutual information between model's predictions\n",
    "$y_x$ at some point $x$ and its parameters $\\omega$:\n",
    "\n",
    "$$\\begin{align}\n",
    "    a(m, S)\n",
    "        &= \\sum_{x\\in S} a(m, \\{x\\})\n",
    "        \\\\\n",
    "    a(m, \\{x\\})\n",
    "        &= \\mathbb{I}(y_x; \\omega \\mid x, m, D)\n",
    "\\end{align}\n",
    "    \\,, \\tag{bald} $$\n",
    "\n",
    "with the [**Mutual Information**](https://en.wikipedia.org/wiki/Mutual_information#Relation_to_Kullback%E2%80%93Leibler_divergence)\n",
    "(**MI**)\n",
    "$$\n",
    "    \\mathbb{I}(y_x; \\omega \\mid x, m, D)\n",
    "        = \\mathbb{H}\\bigl(\n",
    "            \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m, D)}\n",
    "                p(y_x \\,\\mid\\, x, \\omega, m, D)\n",
    "        \\bigr)\n",
    "        - \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m, D)}\n",
    "            \\mathbb{H}\\bigl(\n",
    "                p(y_x \\,\\mid\\, x, \\omega, m, D)\n",
    "            \\bigr)\n",
    "    \\,, \\tag{mi} $$\n",
    "\n",
    "and the [(differential) **entropy**](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)\n",
    "(all densities and/or probability mass functions can be conditional):\n",
    "\n",
    "$$\n",
    "    \\mathbb{H}(p(y))\n",
    "        = - \\mathbb{E}_{y\\sim p} \\log p(y)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) Implementing the acquisition function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $a(m, S)$ is additively separable in $S$, i.e.\n",
    "equals $\\sum_{x\\in S} a(m, \\{x\\})$. This implies\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\max_{S \\subseteq \\mathcal{S}_\\mathrm{unlabelled}} a(m, S)\n",
    "        &= \\max_{z \\in \\mathcal{S}_\\mathrm{unlabelled}}\n",
    "            \\max_{F \\in \\mathcal{S}_\\mathrm{unlabelled} \\setminus \\{z\\}}\n",
    "            \\sum_{x\\in F \\cup \\{x\\}} a(m, \\{x\\})\n",
    "        \\\\\n",
    "        &= \\max_{z \\in \\mathcal{S}_\\mathrm{unlabelled}}\n",
    "            a(m, \\{z\\})\n",
    "            + \\max_{F \\in \\mathcal{S}_\\mathrm{unlabelled} \\setminus \\{z\\}}\n",
    "                \\sum_{x\\in F} a(m, \\{x\\})\n",
    "\\end{align}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore selecting the $\\ell$ `most interesting` points from\n",
    "$\\mathcal{S}_\\mathrm{unlabelled}$ is trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acquisition function that we implement has interface\n",
    "identical to `random_acquisition` but uses BALD to choose\n",
    "instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BALD_acquisition(dataset, model, n_request=1, n_draws=1):\n",
    "    ## Exercise: implement BALD\n",
    "\n",
    "    proba = sample_proba(model, dataset, n_draws=n_draws)\n",
    "\n",
    "    mi = mutual_information(proba)\n",
    "\n",
    "    return mi.argsort(descending=True)[:n_request]\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) implementing entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical (discrete) random variables $y \\sim \\mathcal{Cat}(\\mathbf{p})$,\n",
    "$\\mathbf{p} \\in \\{ \\mu \\in [0, 1]^d \\colon \\sum_k \\mu_k = 1\\}$, the entropy is\n",
    "\n",
    "$$\n",
    "    \\mathbb{H}(p(y))\n",
    "        = - \\mathbb{E}_{y\\sim p(y)} \\log p(y)\n",
    "        = - \\sum_k p_k \\log p_k\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** although in calculus $0 \\cdot \\log 0 = 0$ (because\n",
    "$\\lim_{p\\downarrow 0} p \\cdot \\log p = 0$), in floating point\n",
    "arithmetic $0 \\cdot \\log 0 = \\mathrm{NaN}$. So you need to add\n",
    "some **really tiny float number** to the argument of $\\log$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_entropy(proba):\n",
    "    \"\"\"Compute the entropy along the last dimension.\"\"\"\n",
    "\n",
    "    ## Exercise: the probabilities sum to one along the last axis.\n",
    "    #  Please, compute their entropy.\n",
    "\n",
    "    return - torch.kl_div(torch.tensor(0.).to(proba), proba).sum(dim=-1)\n",
    "\n",
    "    return - torch.sum(proba * torch.log(proba + 1e-20), dim=-1)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) implementing mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a tensor $p_{bik}$ of probabilities $p(y_{x_i}=k \\mid x_i, \\omega_b, m, D)$\n",
    "with $\\omega_b \\sim q(\\omega \\mid m, D)$ with $\\mathcal{W} = (\\omega_b)_{b=1}^B$\n",
    "being iid draws from $q(\\omega \\mid m, D)$.\n",
    "\n",
    "Let's implement a procedure that computes the Monte Carlo estimate of the\n",
    "posterior predictive distribution, its **entropy** and **mutual information**\n",
    "\n",
    "$$\n",
    "    \\mathbb{I}_\\mathrm{MC}(y_x; \\omega \\mid x, m, D)\n",
    "        = \\mathbb{H}\\bigl(\n",
    "            \\hat{p}(y_x\\mid x, m, D)\n",
    "        \\bigr)\n",
    "        - \\frac1{\\lvert \\mathcal{W} \\rvert} \\sum_{\\omega\\in \\mathcal{W}}\n",
    "            \\mathbb{H}\\bigl(\n",
    "                p(y_x \\,\\mid\\, x, \\omega, m, D)\n",
    "            \\bigr)\n",
    "    \\,, \\tag{mi-mc} $$\n",
    "where\n",
    "$$\n",
    "\\hat{p}(y_x\\mid x, m, D)\n",
    "    = \\frac1{\\lvert \\mathcal{W} \\rvert} \\sum_{\\omega\\in \\mathcal{W}}\n",
    "        \\,p(y_x \\mid x, \\omega, m, D)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(proba):\n",
    "    ## Exercise: compute a Monte Carlo estimator of the predictive\n",
    "    ##   distribution, its entropy and MI `H E_w p(., w) - E_w H p(., w)`\n",
    "\n",
    "    entropy_expected = categorical_entropy(proba.mean(dim=0))\n",
    "    expected_entropy = categorical_entropy(proba).mean(dim=0)\n",
    "\n",
    "    return entropy_expected - expected_entropy\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How powerful will our model with **BALD** acquisition, if we can afford no more than $150$ images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bald_results = active_learn(\n",
    "    S_train,\n",
    "    S_pool,\n",
    "    S_valid,\n",
    "    BALD_acquisition,\n",
    "    n_draws=21,\n",
    "    n_budget=150,\n",
    "    n_max_request=3,\n",
    "    n_epochs=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the dynamics of the accuracy ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bald, train_bald, scores_bald, balances_bald = bald_results\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "ax.plot(accuracy(scores_rand), label='Accuracy (random)', lw=2)\n",
    "ax.plot(accuracy(scores_bald), label='Accuracy (BALD)', lw=2)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..., and the frequency of each class in $\\mathcal{S}_\\mathrm{train}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "lines = ax.plot(balances_bald, lw=2)\n",
    "plt.legend(lines, list(range(10)), ncol=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *one-versus-rest* precision / recall scores on\n",
    "$\\mathcal{S}_\\mathrm{valid}$. For binary classification:\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\mathrm{Precision}\n",
    "    &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}\n",
    "        \\approx \\mathbb{P}(y = 1 \\mid \\hat{y} = 1)\n",
    "    \\,, \\\\\n",
    "\\mathrm{Recall}\n",
    "    &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}\n",
    "        \\approx \\mathbb{P}(\\hat{y} = 1 \\mid y = 1)\n",
    "    \\,.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def pr_scores(score_matrix):\n",
    "    tp = score_matrix.diagonal(axis1=-2, axis2=-1)\n",
    "    fp, fn = score_matrix.sum(axis=-2) - tp, score_matrix.sum(axis=-1) - tp\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"precision\": {l: f\"{p:.2%}\" for l, p in enumerate(tp / (tp + fp))},\n",
    "        \"recall\": {l: f\"{p:.2%}\" for l, p in enumerate(tp / (tp + fn))},\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "scores[\"rand\"] = evaluate(model_rand, S_test, n_draws=21)\n",
    "scores[\"bald\"] = evaluate(model_bald, S_test, n_draws=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat({\n",
    "    name: pr_scores(score)\n",
    "    for name, score in scores.items()\n",
    "}, axis=1).T\n",
    "\n",
    "df.swaplevel().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question(s) (to work on in your spare time)\n",
    "\n",
    "* Run the experiments on the `KMNIST` dataset\n",
    "\n",
    "* Replicate figure 1 from [Gat et al. (2017): p. 4](http://proceedings.mlr.press/v70/gal17a.html).\n",
    "  You will need to re-run each experiment several times $11$, recording\n",
    "  the accuracy dynamics of each, then compare the mean and $25\\%$-$75\\%$\n",
    "  quantiles as they evolve with the size of the training sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Points of improvement: batch-vs-single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A drawback of the `pointwise` top-$\\ell$ procedure above is that, although\n",
    "it acquires individually informative instances, altogether they might end\n",
    "up **being** `jointly poorly informative`. This can be corrected, if we\n",
    "would seek the highest mutual information among finite sets $\n",
    "S \\subseteq \\mathcal{S}_\\mathrm{unlabelled}\n",
    "$ of size $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such acquisition function is called **batch-BALD**\n",
    "([Kirsch et al.; 2019](https://arxiv.org/abs/1906.08158.pdf)):\n",
    "\n",
    "$$\\begin{align}\n",
    "    a(m, S)\n",
    "        &= \\mathbb{I}\\bigl((y_x)_{x\\in S}; \\omega \\mid S, m \\bigr)\n",
    "        = \\mathbb{H} \\bigl(\n",
    "            \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m)} p\\bigl((y_x)_{x\\in S}\\mid S, \\omega, m \\bigr)\n",
    "        \\bigr)\n",
    "        - \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m)} H\\bigl(\n",
    "            p\\bigl((y_x)_{x\\in S}\\mid S, \\omega, m \\bigr)\n",
    "        \\bigr)\n",
    "\\end{align}\n",
    "    \\,. \\tag{batch-bald} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This criterion requires combinatorially growing number of computations and\n",
    "memory, however there are working solutions like random sampling of subsets\n",
    "$\\mathcal{S}$ from $\\mathcal{S}_\\mathrm{unlabelled}$ or greedily maximizing\n",
    "of this **submodular** criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Points of improvement: bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term in the **MC** estimate of the mutual information is the\n",
    "so-called **plug-in** estimator of the entropy:\n",
    "\n",
    "$$\n",
    "    \\hat{H}\n",
    "        = \\mathbb{H}(\\hat{p}) = - \\sum_k \\hat{p}_k \\log \\hat{p}_k\n",
    "    \\,, $$\n",
    "\n",
    "where $\\hat{p}_k = \\tfrac1B \\sum_b p_{bk}$ is the full sample estimator\n",
    "of the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known that this plug-in estimate is biased\n",
    "(see [blog: Nowozin, 2015](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-1.html)\n",
    "and references therein, also this [notebook](https://colab.research.google.com/drive/1z9ZDNM6NFmuFnU28d8UO0Qymbd2LiNJW)). <!--($\\log$ + Jensen)-->\n",
    "In order to correct for small-sample bias we can use\n",
    "[jackknife resampling](https://en.wikipedia.org/wiki/Jackknife_resampling).\n",
    "It derives an estimate of the finite sample bias from the leave-one-out\n",
    "estimators of the entropy and is relatively computationally cheap\n",
    "(see [blog: Nowozin, 2015](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-2.html),\n",
    "[Miller, R. G. (1974)](http://www.math.ntu.edu.tw/~hchen/teaching/LargeSample/references/Miller74jackknife.pdf) and these [notes](http://people.bu.edu/aimcinto/jackknife.pdf)).\n",
    "\n",
    "The jackknife correction of a plug-in estimator $\\mathbb{H}(\\cdot)$\n",
    "is computed thus: given a sample $(p_b)_{b=1}^B$ with $p_b$ -- discrete distribution on $1..K$\n",
    "* for each $b=1.. B$\n",
    "  * get the leave-one-out estimator: $\\hat{p}_k^{-b} = \\tfrac1{B-1} \\sum_{j\\neq b} p_{jk}$\n",
    "  * compute the plug-in entropy estimator: $\\hat{H}_{-b} = \\mathbb{H}(\\hat{p}^{-b})$\n",
    "* then compute the bias-corrected entropy estimator $\n",
    "\\hat{H}_J\n",
    "    = \\hat{H} + (B - 1) \\bigl\\{\n",
    "        \\hat{H} - \\tfrac1B \\sum_b \\hat{H}^{-b}\n",
    "    \\bigr\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** when we knock the $i$-th data point out of the sample mean\n",
    "$\\mu = \\tfrac1n \\sum_i x_i$ and recompute the mean $\\mu_{-i}$ we get\n",
    "the following relation\n",
    "$$ \\mu_{-i}\n",
    "    = \\frac1{n-1} \\sum_{j\\neq i} x_j\n",
    "    = \\frac{n}{n-1} \\mu - \\tfrac1{n-1} x_i\n",
    "    = \\mu + \\frac{\\mu - x_i}{n-1}\n",
    "    \\,. $$\n",
    "This makes it possible to quickly compute leave-one-out estimators of\n",
    "discrete probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task*) Unbiased estimator of entropy and mutual information\n",
    "\n",
    "Try to efficiently implement a bias-corrected acquisition\n",
    "function, and see it is worth the effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BALD_jknf_acquisition(dataset, model, n_request=1, n_draws=1):\n",
    "    proba = sample_proba(model, dataset, n_draws=n_draws)\n",
    "\n",
    "    ## Exercise: MC estimate of the predictive distribution, entropy and MI\n",
    "    ##  mutual information `H E_w p(., w) - E_w H p(., w)` with jackknife\n",
    "    ##  correction.\n",
    "\n",
    "    # plug-in estimate of entropy    \n",
    "    proba_avg = proba.mean(dim=0)\n",
    "    entropy_expected = categorical_entropy(proba_avg)\n",
    "\n",
    "    # jackknife correction\n",
    "    proba_loo = proba_avg + (proba_avg - proba) / (len(proba) - 1)\n",
    "    expected_entropy_loo = categorical_entropy(proba_loo).mean(dim=0)\n",
    "    entropy_expected += (len(proba) - 1) * (entropy_expected - expected_entropy_loo)\n",
    "\n",
    "    mi = entropy_expected - categorical_entropy(proba).mean(dim=0)\n",
    "\n",
    "    return mi.argsort(descending=True)[:n_request]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jknf_results = active_learn(\n",
    "    S_train,\n",
    "    S_pool,\n",
    "    S_valid,\n",
    "    BALD_jknf_acquisition,\n",
    "    n_draws=21,\n",
    "    n_budget=150,\n",
    "    n_max_request=3,\n",
    "    n_epochs=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "model_jknf, train_jknf, scores_jknf, balances_jknf = jknf_results\n",
    "ax.plot(accuracy(scores_rand), label='Accuracy (random)', lw=2)\n",
    "ax.plot(accuracy(scores_bald), label='Accuracy (BALD)', lw=2)\n",
    "ax.plot(accuracy(scores_jknf), label='Accuracy (BALD-jknf)', lw=2)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "lines = ax.plot(balances_jknf, lw=2)\n",
    "plt.legend(lines, list(range(10)), ncol=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
