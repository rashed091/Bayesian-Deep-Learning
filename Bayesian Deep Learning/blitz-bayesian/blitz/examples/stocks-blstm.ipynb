{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from blitz.modules import BayesianLSTM\n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "REMEMBER TO DOWNLOAD THE DATASET AT KAGGLE AT THIS LINK:\n",
    "https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231\n",
    "\"\"\"\n",
    "amazon=\"data/AMZN_2006-01-01_to_2018-01-01.csv\"\n",
    "ibm=\"data/IBM_2006-01-01_to_2018-01-01.csv\"\n",
    "df = pd.read_csv(ibm)\n",
    "\n",
    "window_size = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_prices = df[\"Close\"]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "close_prices_arr = np.array(close_prices).reshape(-1, 1)\n",
    "close_prices = scaler.fit_transform(close_prices_arr)\n",
    "\n",
    "close_prices_unscaled = df[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timestamps_ds(series, \n",
    "                         timestep_size=window_size):\n",
    "    time_stamps = []\n",
    "    labels = []\n",
    "    aux_deque = deque(maxlen=timestep_size)\n",
    "    \n",
    "    #starting the timestep deque\n",
    "    for i in range(timestep_size):\n",
    "        aux_deque.append(0)\n",
    "    \n",
    "    #feed the timestamps list\n",
    "    for i in range(len(series)-1):\n",
    "        aux_deque.append(series[i])\n",
    "        time_stamps.append(list(aux_deque))\n",
    "    \n",
    "    #feed the labels lsit\n",
    "    for i in range(len(series)-1):\n",
    "        labels.append(series[i + 1])\n",
    "    \n",
    "    assert len(time_stamps) == len(labels), \"Something went wrong\"\n",
    "    \n",
    "    #torch-tensoring it\n",
    "    features = torch.tensor(time_stamps[timestep_size:]).float()\n",
    "    labels = torch.tensor(labels[timestep_size:]).float()\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@variational_estimator\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.lstm_1 = BayesianLSTM(1, 10)\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x_, _ = self.lstm_1(x)\n",
    "        \n",
    "        #gathering only the latent end-of-sequence for the linear layer\n",
    "        x_ = x_[:, -1, :]\n",
    "        x_ = self.linear(x_)\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, ys = create_timestamps_ds(close_prices)\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs,\n",
    "                                                    ys,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "ds = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "dataloader_train = torch.utils.data.DataLoader(ds, batch_size=8, shuffle=True)\n",
    "\n",
    "net = NN()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "for epoch in range(10):\n",
    "    for i, (datapoints, labels) in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = net.sample_elbo(inputs=datapoints,\n",
    "                               labels=labels,\n",
    "                               criterion=criterion,\n",
    "                               sample_nbr=3)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration%250==0:\n",
    "            preds_test = net(X_test)[:,0].unsqueeze(1)\n",
    "            loss_test = criterion(preds_test, y_test)\n",
    "            print(\"Iteration: {} Val-loss: {:.4f}\".format(str(iteration), loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_unscaled\n",
    "original = close_prices_unscaled[1:][window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(original)\n",
    "df_pred[\"Date\"] = df.Date\n",
    "df[\"Date\"] = pd.to_datetime(df_pred[\"Date\"])\n",
    "df_pred = df_pred.reset_index()\n",
    "#df_pred = df_pred.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_stock_future(X_test,\n",
    "                                           future_length,\n",
    "                                           sample_nbr=10):\n",
    "    \n",
    "    #sorry for that, window_size is a global variable, and so are X_train and Xs\n",
    "    global window_size\n",
    "    global X_train\n",
    "    global Xs\n",
    "    global scaler\n",
    "    \n",
    "    #creating auxiliar variables for future prediction\n",
    "    preds_test = []\n",
    "    test_begin = X_test[0:1, :, :]\n",
    "    test_deque = deque(test_begin[0,:,0].tolist(), maxlen=window_size)\n",
    "\n",
    "    idx_pred = np.arange(len(X_train), len(Xs))\n",
    "    \n",
    "    #predict it and append to list\n",
    "    for i in range(len(X_test)):\n",
    "        #print(i)\n",
    "        as_net_input = torch.tensor(test_deque).unsqueeze(0).unsqueeze(2)\n",
    "        pred = [net(as_net_input).cpu().item() for i in range(sample_nbr)]\n",
    "        \n",
    "        \n",
    "        test_deque.append(torch.tensor(pred).mean().cpu().item())\n",
    "        preds_test.append(pred)\n",
    "        \n",
    "        if i % future_length == 0:\n",
    "            #our inptus become the i index of our X_test\n",
    "            #That tweak just helps us with shape issues\n",
    "            test_begin = X_test[i:i+1, :, :]\n",
    "            test_deque = deque(test_begin[0,:,0].tolist(), maxlen=window_size)\n",
    "\n",
    "    #preds_test = np.array(preds_test).reshape(-1, 1)\n",
    "    #preds_test_unscaled = scaler.inverse_transform(preds_test)\n",
    "    \n",
    "    return idx_pred, preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_intervals(preds_test, ci_multiplier):\n",
    "    global scaler\n",
    "    \n",
    "    preds_test = torch.tensor(preds_test)\n",
    "    \n",
    "    pred_mean = preds_test.mean(1)\n",
    "    pred_std = preds_test.std(1).detach().cpu().numpy()\n",
    "\n",
    "    pred_std = torch.tensor((pred_std))\n",
    "    \n",
    "    upper_bound = pred_mean + (pred_std * ci_multiplier)\n",
    "    lower_bound = pred_mean - (pred_std * ci_multiplier)\n",
    "    #gather unscaled confidence intervals\n",
    "\n",
    "    pred_mean_final = pred_mean.unsqueeze(1).detach().cpu().numpy()\n",
    "    pred_mean_unscaled = scaler.inverse_transform(pred_mean_final)\n",
    "\n",
    "    upper_bound_unscaled = upper_bound.unsqueeze(1).detach().cpu().numpy()\n",
    "    upper_bound_unscaled = scaler.inverse_transform(upper_bound_unscaled)\n",
    "    \n",
    "    lower_bound_unscaled = lower_bound.unsqueeze(1).detach().cpu().numpy()\n",
    "    lower_bound_unscaled = scaler.inverse_transform(lower_bound_unscaled)\n",
    "    \n",
    "    return pred_mean_unscaled, upper_bound_unscaled, lower_bound_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_length=7\n",
    "sample_nbr=4\n",
    "ci_multiplier=10\n",
    "idx_pred, preds_test = pred_stock_future(X_test, future_length, sample_nbr)\n",
    "pred_mean_unscaled, upper_bound_unscaled, lower_bound_unscaled = get_confidence_intervals(preds_test,\n",
    "                                                                                          ci_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df.Close[-750:]).reshape(-1, 1)\n",
    "under_upper = upper_bound_unscaled > y\n",
    "over_lower = lower_bound_unscaled < y\n",
    "total = (under_upper == over_lower)\n",
    "\n",
    "print(\"{} our predictions are in our confidence interval\".format(np.mean(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"ytick.color\" : \"w\",\n",
    "          \"xtick.color\" : \"w\",\n",
    "          \"axes.labelcolor\" : \"w\",\n",
    "          \"axes.edgecolor\" : \"w\"}\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "plt.title(\"IBM Stock prices\", color=\"white\")\n",
    "\n",
    "plt.plot(df_pred.index,\n",
    "         df_pred.Close,\n",
    "         color='black',\n",
    "         label=\"Real\")\n",
    "\n",
    "plt.plot(idx_pred,\n",
    "         pred_mean_unscaled,\n",
    "         label=\"Prediction for {} days, than consult\".format(future_length),\n",
    "         color=\"red\")\n",
    "\n",
    "plt.fill_between(x=idx_pred,\n",
    "                 y1=upper_bound_unscaled[:,0],\n",
    "                 y2=lower_bound_unscaled[:,0],\n",
    "                 facecolor='green',\n",
    "                 label=\"Confidence interval\",\n",
    "                 alpha=0.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"ytick.color\" : \"w\",\n",
    "          \"xtick.color\" : \"w\",\n",
    "          \"axes.labelcolor\" : \"w\",\n",
    "          \"axes.edgecolor\" : \"w\"}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "plt.title(\"IBM Stock prices\", color=\"white\")\n",
    "\n",
    "\n",
    "plt.fill_between(x=idx_pred,\n",
    "                 y1=upper_bound_unscaled[:,0],\n",
    "                 y2=lower_bound_unscaled[:,0],\n",
    "                 facecolor='green',\n",
    "                 label=\"Confidence interval\",\n",
    "                 alpha=0.75)\n",
    "\n",
    "plt.plot(idx_pred,\n",
    "         df_pred.Close[-len(pred_mean_unscaled):],\n",
    "         label=\"Real\",\n",
    "         alpha=1,\n",
    "         color='black',\n",
    "         linewidth=0.5)\n",
    "\n",
    "plt.plot(idx_pred,\n",
    "         pred_mean_unscaled,\n",
    "         label=\"Prediction for {} days, than consult\".format(future_length),\n",
    "         color=\"red\",\n",
    "         alpha=0.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blitz",
   "language": "python",
   "name": "blitz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
