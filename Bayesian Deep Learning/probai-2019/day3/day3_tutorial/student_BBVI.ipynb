{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying BBVI for a simple Gaussian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"BBVI_exercise.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from a simple model: Normal(10, 1)\n",
    "data = np.random.normal(loc = 10, scale = 1, size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function: ELBO\n",
    "\n",
    "Calculate the exact value of the ELBO. Generally one would have to estimate this using sampling, but for this simple model we can evaluate it exactly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lower_bound(tau, q_mu):\n",
    "    \"\"\"\n",
    "    Helper routine: Calculate ELBO. Data is the sampled x-values, anything without a star relates to the prior,\n",
    "    everything _with_ a star relates to the variational posterior.\n",
    "    Note that we have no nu without a star; I am simplifying by forcing this to be zero a priori\n",
    "\n",
    "    Note: This function obviously only works when the model is as in this code challenge,\n",
    "    and is not a general solution.\n",
    "\n",
    "    :param data: The sampled data\n",
    "    :param tau: prior precision for mu, the mean for the data generation\n",
    "    :param alpha: prior shape of dist for gamma, the precision  of the data generation\n",
    "    :param beta: prior rate of dist for gamma, the precision  of the data generation\n",
    "    :param nu_star: VB posterior mean for the distribution of mu - the mean of the data generation\n",
    "    :param tau_star: VB posterior precision for the distribution of mu - the mean of the data generation\n",
    "    :param alpha_star: VB posterior shape of dist for gamma, the precision  of the data generation\n",
    "    :param beta_star: VB posterior shape of dist for gamma, the precision  of the data generation\n",
    "    :return: the ELBO\n",
    "    \"\"\"\n",
    "\n",
    "    # We calculate ELBO as E_q log p(x,mu) - E_q log q(mu)\n",
    "    # log p(x,z) here is log p(mu) + \\sum_i log p(x_i | mu, 1)\n",
    "\n",
    "    # E_q log p(mu)\n",
    "    log_p = -.5 * np.log(2 * np.pi) - .5 * (1/tau) * (1 + q_mu**2)\n",
    "\n",
    "\n",
    "    # E_q log p(x_i|mu, 1)\n",
    "    for xi in data:\n",
    "        log_p += -.5 * np.log(2 * np.pi) - .5 * (xi * xi - 2 * xi * q_mu + 1 + q_mu**2)\n",
    "\n",
    "    # Entropy of mu (Gaussian)\n",
    "    entropy = .5 * np.log(2 * np.pi * np.exp(1))\n",
    "\n",
    "    return log_p + entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual estimation of the gradient of the ELBO for the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient estimator using sampling -- vanilla BBVI\n",
    "# We here assume the model X ~ Normal(mu, 1)\n",
    "# with unknown mu, that in itself is Normal, mean 0 and standard deviation 1000, \n",
    "# so effectively an uniformed prior. \n",
    "# The variational dstribution for mu is also Normal, with parameter q_mu_lambda\n",
    "# -- taking the role of lambda in the calculations -- and variance 1.\n",
    "#\n",
    "# Note:\n",
    "# We can sample from a normal using:\n",
    "# * np.random.normal(loc=mu, scale=1, size=1)\n",
    "# We can evaluate the the normal density using\n",
    "# * norm.logpdf(sample, loc = mu, scale = std. dev.)\n",
    "\n",
    "def grad_estimate(q_mu_lambda, samples = 1):\n",
    "    # sum_grad_estimate will hold the sum as we move along over the <samples> samples. \n",
    "    sum_grad_estimate = 0\n",
    "    for i in range(samples):\n",
    "        # Sample one example from current best guess for the variational distribution\n",
    "        mu_sample = np.random.normal(loc=q_mu_lambda, scale=1, size=1)\n",
    "        \n",
    "        # Now we want to calculate the contribution from this sample, namely \n",
    "        # [log p(x, mu_sample) - log q(mu|lambda) ] * grad( log q(mu_sample|lambda) )\n",
    "        #\n",
    "        value = ?\n",
    "\n",
    "        # Next grad (log q(mu_sample|lambda))\n",
    "        # The Normal distribution gives the score function with known variance as <value> - <mean>\n",
    "        grad_q = ?\n",
    "\n",
    "        \n",
    "        # grad ELBO for this sample is therefore in total given by\n",
    "        sum_grad_estimate = sum_grad_estimate + grad_q * value\n",
    "       \n",
    "    # Divide by number of samples to get average value -- the estimated expectation  \n",
    "    return sum_grad_estimate/samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform BBVI using the estimated gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100 sample(s) -- Estimate:   9.91943; error   0.8%  --  Calc.time: 17.40 sec.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "no_loops = 500\n",
    "sample_count = 100\n",
    "##### Starting point\n",
    "q_mu = -10\n",
    "start = time.time()\n",
    "elbos = []\n",
    "lr = 1E-4 \n",
    "\n",
    "\n",
    "#loop a couple of times\n",
    "for t in range(no_loops):\n",
    "    elbos.append(calculate_lower_bound(1000, q_mu))\n",
    "    q_grad = grad_estimate(q_mu, samples=sample_count)\n",
    "    q_mu = q_mu + lr * q_grad\n",
    "\n",
    "print(\"{:4d} sample(s) -- Estimate: {:9.5f}; error {:5.1f}%  --  Calc.time: {:5.2f} sec.\".format(\n",
    "    sample_count, float(q_mu), float(10*np.abs(q_mu-10)), time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "* Try varying the number of samples used for estimating the gradient. What effect does it have on the results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TFEnv conda)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
