{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model description\n",
    "In this code-task we work with a fairly simple model, where we have observations $x_i$, $i=1,\\ldots N$, that we assume follow a Gaussian distribution. The mean and precision (inverse variance) are unknown, so we model them in Bayesian way: The mean denoted by the random variable $\\mu$ is a Gaussian with a priori mean $0$ and precision $\\tau$. The precision of the data generating process is modelled using the random variable $\\gamma$. $\\gamma$ is a priori Gamma distributed with parameters $\\alpha$ (shape) and $\\beta$ (rate).\n",
    "\n",
    "$$\n",
    "\\mu \\sim Normal(0,\\tau^{-1})\\\\\n",
    "\\gamma \\sim Gamma(\\alpha,\\beta)\\\\\n",
    "x_i \\sim Normal(\\mu, \\gamma)\n",
    "$$\n",
    "\n",
    "In total, the model is thus like this: $\\mu \\rightarrow X_i \\leftarrow \\gamma$ (hyper-parameters not shown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special, stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Startup: Define priors, and sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define priors\n",
    "alpha_prior, beta_prior = 1E-2, 1E-2   # Parameters for the prior for the precision of x\n",
    "tau_prior = 1E-6  # A priori precision for the precision of mu\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(123)\n",
    "N = 4\n",
    "correct_mean = 5\n",
    "correct_precision = 1\n",
    "x = np.random.normal(loc=correct_mean, scale=1./np.sqrt(correct_precision), size=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-routine: Make plot of posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior(posterior_mean_mu, posterior_prec_mu,\n",
    "                   posterior_alpha_gamma, posterior_beta_gamma,\n",
    "                   correct_mean, correct_precision):\n",
    "    mu_range = np.linspace(posterior_mean_mu - 5./np.sqrt(posterior_prec_mu),\n",
    "                           posterior_mean_mu + 5. / np.sqrt(posterior_prec_mu), 500).astype(np.float32)\n",
    "    precision_range = np.linspace(1E-2, 3, 500).astype(np.float32)\n",
    "    mu_mesh, precision_mesh = np.meshgrid(mu_range, precision_range)\n",
    "    variational_log_pdf = \\\n",
    "        stats.norm.logpdf(mu_mesh, loc=posterior_mean_mu, scale=1. / np.sqrt(posterior_prec_mu)) + \\\n",
    "        stats.gamma.logpdf(x=precision_mesh,\n",
    "                           a=posterior_alpha_gamma,\n",
    "                           scale=1. / posterior_beta_gamma)\n",
    "    plt.figure()\n",
    "    plt.contour(mu_mesh, precision_mesh, variational_log_pdf, 25)\n",
    "    plt.plot(correct_mean, correct_precision, \"bo\")\n",
    "    plt.title('Posterior over $(\\mu, \\\\tau)$. Blue dot: True parameters')\n",
    "    plt.xlabel(\"Mean $\\mu$\")\n",
    "    plt.ylabel(\"Precision $\\\\tau$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-routine: Calculate ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lower_bound(data, tau, alpha, beta, nu_star, tau_star, alpha_star, beta_star):\n",
    "    \"\"\"\n",
    "    Helper routine: Calculate ELBO. Data is the sampled x-values, anything without a star relates to the prior,\n",
    "    everything _with_ a star relates to the variational posterior.\n",
    "    Note that we have no nu without a star; I am simplifying by forcing this to be zero a priori\n",
    "\n",
    "    Note: This function obviously only works when the model is as in this code challenge,\n",
    "    and is not a general solution.\n",
    "\n",
    "    :param data: The sampled data\n",
    "    :param tau: prior precision for mu, the mean for the data generation\n",
    "    :param alpha: prior shape of dist for gamma, the precision  of the data generation\n",
    "    :param beta: prior rate of dist for gamma, the precision  of the data generation\n",
    "    :param nu_star: VB posterior mean for the distribution of mu - the mean of the data generation\n",
    "    :param tau_star: VB posterior precision for the distribution of mu - the mean of the data generation\n",
    "    :param alpha_star: VB posterior shape of dist for gamma, the precision  of the data generation\n",
    "    :param beta_star: VB posterior shape of dist for gamma, the precision  of the data generation\n",
    "    :return: the ELBO\n",
    "    \"\"\"\n",
    "\n",
    "    # We calculate ELBO as E_q log p(x,z) - E_q log q(z)\n",
    "    # log p(x,z) here is log p(mu) + log p(gamma) + \\sum_i log p(x_i | mu, gamma)\n",
    "\n",
    "    # E_q log p(mu)\n",
    "    log_p = -.5 * np.log(2 * np.pi) + .5 * np.log(tau) - .5 * tau * (1 / tau_star + nu_star * nu_star)\n",
    "\n",
    "    # E_q log p(gamma)\n",
    "    log_p = log_p + alpha * np.log(beta) + \\\n",
    "            (alpha - 1) * (special.digamma(alpha_star) - np.log(beta_star)) - beta * alpha_star / beta_star\n",
    "\n",
    "    # E_q log p(x_i|mu, gamma)\n",
    "    for xi in data:\n",
    "        log_p += -.5 * np.log(2 * np.pi) \\\n",
    "                 + .5 * (special.digamma(alpha_star) - np.log(beta_star)) \\\n",
    "                 - .5 * alpha_star / beta_star * (xi * xi - 2 * xi * nu_star + 1 / tau_star + nu_star * nu_star)\n",
    "\n",
    "    # Entropy of mu (Gaussian)\n",
    "    entropy = .5 * np.log(2 * np.pi * np.exp(1) / tau_star)\n",
    "    entropy += alpha_star - np.log(beta_star) + special.gammaln(alpha_star) \\\n",
    "               + (1 - alpha_star) * special.digamma(alpha_star)\n",
    "\n",
    "    return log_p + entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the VB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to compute a variational approxmation of the posterior over the unknown paramters $\\mu$ and $\\gamma$, \n",
    "\n",
    "$$\n",
    "p(\\mu,\\gamma|x_1,\\ldots,x_n) \\approx q(\\mu)q(\\gamma)\n",
    "$$\n",
    "\n",
    "\n",
    "We are looking for VB posteriors over $\\mu$ and $\\gamma$. It turns out after some pencil pushing that the posteriors are in the same distributional families as the priors were, so $\\mu$ remains Gaussian, $\\gamma$ remains Gamma distributed. What we need is the updated parameters for these two distributions. We have two parameters to update $q(\\mu)$, which are denoted as `q_mu` and `q_tau`, and another two parameters to update $q(\\gamma)$, which are denoted as `q_alpha` and `q_beta`.\n",
    "The parameters of the (prior) distribution $p(\\cdot)$ are called something ending with `_prior`, like `alpha_prior` for $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "q_alpha = alpha_prior\n",
    "q_beta = beta_prior\n",
    "q_mu = 0\n",
    "q_tau = tau_prior\n",
    "previous_lb = -np.inf\n",
    "\n",
    "# Start iterating\n",
    "print(\"\\n\" + 100 * \"=\" + \"\\n   VB iterations:\\n\" + 100 * \"=\")\n",
    "for iteration in range(1000):\n",
    "    # Update gamma distribution\n",
    "    q_alpha = ?????\n",
    "    q_beta = ?????\n",
    "    expected_gamma = ?????\n",
    "    \n",
    "    # Update Gaussian distribution\n",
    "    q_tau = ?????\n",
    "    q_mu = ?????\n",
    "    \n",
    "    # Calculate Lower-bound\n",
    "    this_lb = calculate_lower_bound(data=x, tau=tau_prior, alpha=alpha_prior, beta=beta_prior,\n",
    "                                    nu_star=q_mu, tau_star=q_tau, alpha_star=q_alpha, beta_star=q_beta)\n",
    "\n",
    "    print(\"{:2d}.  alpha: {:6.3f}, beta: {:12.3f}, nu: {:6.3f}, tau: {:6.3f}, ELBO: {:12.7f}\".format(\n",
    "        iteration + 1, q_alpha, q_beta, q_mu, q_tau, this_lb))\n",
    "    \n",
    "    if this_lb < previous_lb:\n",
    "        raise ValueError(\"ELBO is decreasing. Something is wrong! Goodbye...\")\n",
    "    \n",
    "    if iteration > 0 and np.abs((this_lb - previous_lb) / previous_lb) < 1E-8:\n",
    "        # Very little improvement. We are done.\n",
    "        break\n",
    "    \n",
    "    # If we didn't break we need to run again. Update the value for \"previous\"\n",
    "    previous_lb = this_lb\n",
    "    \n",
    "\n",
    "print(\"\\n\" + 100 * \"=\" + \"\\n   Result:\\n\" + 100 * \"=\")\n",
    "print(\"E[mu] = {:5.3f} with data average {:5.3f} and prior mean {:5.3f}.\".format(q_mu, np.mean(x), 0.))\n",
    "print(\"E[gamma] = {:5.3f} with inverse of data covariance {:5.3f} and prior {:5.3f}.\".format(\n",
    "    q_alpha / q_beta, 1. / np.cov(x), alpha_prior / beta_prior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make plot of Variational Bayes posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(q_mu, q_tau, q_alpha, q_beta, correct_mean, correct_precision)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probabilistic.ai",
   "language": "python",
   "name": "probabilistic.ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
