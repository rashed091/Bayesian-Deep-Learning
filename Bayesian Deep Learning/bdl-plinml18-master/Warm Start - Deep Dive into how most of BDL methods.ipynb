{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Start - Deep Dive into how most of BDL methods\n",
    "\n",
    "In this short notebook we will briefly describe different methods of obtaining Bayesian posterior and examine them with respect to applicability to neural networks models:\n",
    "\n",
    "### **Analytic solutions**\n",
    "\n",
    "Intractable / do not exist in most of cases.\n",
    "\n",
    "### **Analytical approximations** \n",
    "\n",
    "Let's have a look why this might not work by examining snippets from famous **Pattern Recognition and Machine Learning** book (by Christopher Bishop):\n",
    "![Citation-1](example_images/variational_dropout/Bayes - Bishop - 2.png)\n",
    "Laplace estimation assumes that posterior is locally Gaussian and matches appropriate Gaussian distribution by the mode matching.\n",
    "![Citation-1](example_images/variational_dropout/Bayes - Bishop - 3.png)\n",
    "In order to find the mode - you need to run separate Backpropagation process.\n",
    "![Citation-1](example_images/variational_dropout/Bayes - Bishop - 4.png)\n",
    "And still - even after that you need to assume that your network is locally linear. Well - this might explain why Bayesian methods were not popular in case of neural networks models.\n",
    "\n",
    "### **Variational methods**\n",
    "\n",
    "This is one of the most popular methods used nowadays for Bayesian treatment of Neural Networks. Most of the methods uses the so-called *Variational Free Energy*. To understand that methods let's introduce:\n",
    "- $q(w|\\theta)$ - distribution over weights parametrized by $\\theta$. It should be easy to sample from this distribution (given $\\theta$) and it should make the *parametrization* trick possible.\n",
    "- $P(w)$ - prior distribution over weights,\n",
    "- $P(D|w)$ - posterior distribution over weights (given data).\n",
    "\n",
    "Then - in order to find the parameters $\\theta$ for which $q(w|\\theta)$ approximates $P(w|D)$ one should minimize:\n",
    "\n",
    "$$\\theta^* = \\arg \\min _ \\theta = KL\\left(q(w|\\theta)||P(w)\\right) - \\mathbb{E}_{q(w|\\theta)}\\log P(D|w).$$\n",
    "\n",
    "The reasons why this method is so popular are following:\n",
    "- $KL\\left(q(w|\\theta)||P(w)\\right)$ - is usually easy to compute. In some methods *prior* is introduced by defining this term,\n",
    "- if $q(w|\\theta)$ is easy to sample - one may approximate the second term using values obtained on models samples and optimize the loss using *reparametrization trick*.\n",
    "\n",
    "### **Mixed methods**\n",
    "\n",
    "In some methods - one may use the neural network as a feature extractor and use a Bayesian treatment for the last layer - as if it was the input to the model. Examples of such methods are [pseudo-count](https://arxiv.org/abs/1606.01868) methods or a [core-set](https://arxiv.org/abs/1708.00489) approach to active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
