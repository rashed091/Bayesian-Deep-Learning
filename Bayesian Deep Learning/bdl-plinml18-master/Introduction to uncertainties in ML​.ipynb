{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import decode_predictions\n",
    "from keras.applications.xception import preprocess_input\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overconfidence of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.xception.Xception(\n",
    "    include_top=True,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classes=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CVPR 2015 - Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The deep neural network is the pre-trained network modeled on AlexNet provided by Caffe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 45))\n",
    "\n",
    "files = sorted(glob.glob(\"./example_images/out-of-distribution/*\"))\n",
    "\n",
    "for j, image_path in enumerate(files):\n",
    "    image_ = image.load_img(image_path)\n",
    "    \n",
    "    image_preprocessed = preprocess_input(image.img_to_array(image_)[np.newaxis])\n",
    "    \n",
    "    preds = model.predict(image_preprocessed)\n",
    "    \n",
    "    y = j % 2\n",
    "    x = j // 2\n",
    "    axes[x, y].imshow(image_)\n",
    "    img_, class_, prob_ = decode_predictions(preds)[0][0]\n",
    "    axes[x, y].set_title(f'{class_} - probability {prob_}')\n",
    "\n",
    "    j += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple methods for discovering uncertainties\n",
    "\n",
    "Now we will focus on a basic technique that can be used to:\n",
    "+ Find out-if-distribution examples\n",
    "+ Model aleatoric uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbours\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Gaussian divided into three quantiles\", fontsize='large')\n",
    "X1, Y1 = make_blobs(n_features=2, centers=3, cluster_std=2, random_state=9)\n",
    "_ = plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "                s=50, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = classifier.fit(X1, Y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1). Find out-of-distribution samples\n",
    "\n",
    "For a point we compute the mean distance to k nearest neighbours.\n",
    "\n",
    "Then, the distance serves as an out-of-distribution detection metric\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_forgrid = np.linspace(-15, 2.5, 100)\n",
    "y_forgrid = np.linspace(-10, 4, 100)\n",
    "\n",
    "X_mesh, Y_mesh = np.meshgrid(x_forgrid, y_forgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = np.stack((X_mesh.flatten(), Y_mesh.flatten()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_to_plot = classifier.kneighbors(coordinates)[0].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Out-of-distribution uncertainty\", fontsize='large')\n",
    "X1, Y1 = make_blobs(n_features=2, centers=3, cluster_std=2, random_state=9)\n",
    "\n",
    "plt.scatter(\n",
    "    coordinates[:, 0],\n",
    "    coordinates[:, 1],\n",
    "    s=80,\n",
    "    alpha=0.9,\n",
    "    c=-grid_to_plot,\n",
    "    cmap='gray',\n",
    "    marker='s'\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "_ = plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "                s=50, edgecolor='k')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can compare an example from a test set with the training set under this metric, In case of a point from a training set, we will ignore the point itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_distances = classifier.kneighbors(X1)[0][:, 1:].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(training_set_distances, bins=20, normed=True, alpha=0.5, label='Training set')\n",
    "_ = plt.hist(grid_to_plot, bins=100, normed=True, alpha=0.5, label='All the gridpoints')\n",
    "plt.legend()\n",
    "_ = plt.xlabel('Average distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find approximation of alleatoric uncertainty\n",
    "\n",
    "Knowing that probabilities can give us basic measure of uncertainty, we will now create a simple measure of alleatoric uncertainty:\n",
    "\n",
    "$\\frac{\\sum_{i}{d_{i}}}{k} - \\lambda f(x)$\n",
    "\n",
    "where \n",
    "+ $k$ - number of nearest neighbours\n",
    "+ $d_{i}$ - distances of k nearest neighbours\n",
    "+ $f$ - model - outputs the probability of the most likely class\n",
    "+ $x$ - sample\n",
    "+ $\\lambda$ - hyperparameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = classifier.predict_proba(coordinates).max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Three blobs - all uncertainties\", fontsize='large')\n",
    "X1, Y1 = make_blobs(n_features=2, centers=3, cluster_std=2, random_state=9)\n",
    "\n",
    "plt.scatter(\n",
    "    coordinates[:, 0],\n",
    "    coordinates[:, 1],\n",
    "    s=80,\n",
    "    alpha=0.9,\n",
    "    c=np.clip(-grid_to_plot + 5 * grid_predictions, 0, None),\n",
    "    cmap='gray',\n",
    "    marker='s'\n",
    "\n",
    ")\n",
    "\n",
    "_ = plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "                s=50, edgecolor='k')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "We will work on Reuters dataset.\n",
    "\n",
    "I extract two out of all classes. The task is to create a method that maximizes ROC AUC in the out-of-distribution detection task. Out-of-distribution detection task is a binary classification problem where the classes are: out-of-distribution examples and in-distribution examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains and evaluate a simple MLP\n",
    "on the Reuters newswire topic classification task.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 1000\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will  use only 2 classes in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before dataset cleaning: {y_train.max() + 1} classes\")\n",
    "x_train = x_train[y_train < 2]\n",
    "y_train = y_train[y_train < 2]\n",
    "print(f\"After dataset cleaning: {y_train.max() + 1} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_groundtruth = y_test > 1 # 1 is OOD example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here you should write your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_groundtruth, scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
