{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Background</h4>\n",
    "<p>We consider a general feed-forward BNN with <span class=\"math\">\\(L\\)</span> total layers (including the output layer and <span class=\"math\">\\(L-1\\)</span> hidden layers), indexed <span class=\"math\">\\(1, 2, \\ldots \\ell, \\ldots L\\)</span>. It takes as input a vector <span class=\"math\">\\(x\\)</span> and produces a real-valued scalar output <span class=\"math\">\\(f(x) \\in \\mathbb{R}\\)</span>.</p>\n",
    "<p>The 1st layer takes as input a <span class=\"math\">\\(D\\)</span>-dimensional data vector: <span class=\"math\">\\(x = [x_1, x_2, \\ldots x_D]\\)</span>. Each of the <span class=\"math\">\\(J^{(1)}\\)</span> hidden units, indexed by <span class=\"math\">\\(j\\)</span>, produces a scalar value by multiplying the input vector <span class=\"math\">\\(x\\)</span> by a weight vector, adding a bias, and feeding the resulting scalar through an activation function:</p>\n",
    "<div class=\"math\">$$\n",
    "h^{(1)}_{j}(x, w, b) = \\text{activation}(b^{(1)}_{j} + \\sum_{d=1}^D w^{(1)}_{j,d} x_{d} )\n",
    "$$</div>\n",
    "<p>If there are 2 or more hidden layers, we'll write that layer <span class=\"math\">\\(\\ell\\)</span> has <span class=\"math\">\\(J^{\\ell}\\)</span> units. Each unit produces a scalar in the same fashion: taking as input the <span class=\"math\">\\(J^{(\\ell-1)}\\)</span>-length vector <span class=\"math\">\\(h^{\\ell-1}\\)</span> produced by the previous layer, multiplying by unit-specific weights, adding unit-specific bias, and applying an activation function:\n",
    "</p>\n",
    "<div class=\"math\">$$\n",
    "h^{(\\ell)}_{j}(x, w, b) = \\text{activation}(b^{(\\ell)}_{j} + \\sum_{k=1}^{J^{(\\ell-1)}} w^{(\\ell)}_{j,k} h^{(\\ell-1)}_{k}(x,w,b) )\n",
    "$$</div>\n",
    "<p>At the final layer <span class=\"math\">\\(L\\)</span>, we produce a scalar value <span class=\"math\">\\(f(x, w, b)\\)</span> via:\n",
    "</p>\n",
    "<div class=\"math\">$$\n",
    "f(x, w, b) = b^{(L)}_{1} + \\sum_{k=1}^{J^{(L-1)}} w^{(L)}_{k} h^{(L-1)}_{k}(x,w,b)\n",
    "$$</div>\n",
    "<h4>Possible Activation Functions</h4>\n",
    "<ul>\n",
    "<li>Tanh: Hyperbolic tangent function -- see <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html\">numpy's np.tanh</a></li>\n",
    "<li>ReLU: <span class=\"math\">\\(\\text{relu}(z) = \\text{max}(0, z)\\)</span></li>\n",
    "<li>SquaredExponential (aka 'RBF'): <span class=\"math\">\\(\\text{sqexp}(z) = \\exp(-z^2)\\)</span></li>\n",
    "</ul>\n",
    "<h4>Possible Architectures</h4>\n",
    "<p>A general neural network for regression with L total layers will have L-1 hidden layers, each one with different numbers of hidden units. We can specify the size of the hidden network as a list of integers, like this:</p>\n",
    "<ul>\n",
    "<li><code>[]</code> means there are no hidden layers and no hidden units (equivalent to linear regression)</li>\n",
    "<li><code>[5]</code> means there is one hidden layer with 5 units</li>\n",
    "<li><code>[5, 3]</code> means there are two hidden layers, the first has 5 units and second has 3 units.</li>\n",
    "</ul>\n",
    "<p>In general, the number of hidden layers (L-1) is equal to the length of the list, and the size of the <span class=\"math\">\\(\\ell\\)</span>-th layer is given by the integer at position <span class=\"math\">\\(\\ell\\)</span> in the list.</p>\n",
    "<h4>Likelihood model</h4>\n",
    "<p>We can use the scalar value produced by the feed-foward neural network as the mean of a Gaussian <em>likelihood</em> distribution that explains observed input/output training data pairs: <span class=\"math\">\\(y_n, x_n\\)</span>:</p>\n",
    "<div class=\"math\">\\begin{align}\n",
    "p(y|x, w, b) &amp;= \\prod_{n=1}^N p(y_n | x_n, w, b)\n",
    "\\\\\n",
    "    &amp;= \\prod_{n=1}^N \\mathcal{N}(y_n \\mid f(x_n, w, b), \\sigma^2)\n",
    "\\end{align}</div>\n",
    "<p>Where <span class=\"math\">\\(\\sigma\\)</span> is a given standard deviation hyperparameter.</p>\n",
    "<h2><a name=\"problem-1\">Problem 1: Sampling from a BNN Prior</a></h2>\n",
    "<p>Write Python code to sample function values produced by the output of a general BNN regression architecture, where the weight parameters and biases at every layer each have an independent Gaussian prior -- Normal(mean=0, variance=1).</p>\n",
    "<p>You should sample the function values that correspond to a set of at least 200 evenly-spaced test points <span class=\"math\">\\(\\{x_i\\}\\)</span> between -20 and 20. One way to generate a 1D array of <span class=\"math\">\\(G\\)</span> points would be: <code>x_grid_G = np.linspace(-20, 20, G)</code>. </p>\n",
    "<p>To demonstrate your implementation, you'll make plots of sampled function values (just like in HW1). Each individual plot should show a line plot of the test grid points <span class=\"math\">\\(x_i\\)</span> and the corresponding sampled function values <span class=\"math\">\\(f_i = f(x_i)\\)</span>. Use a matplotlib line style '-' to emphasize the connecting lines between the specific <span class=\"math\">\\(\\{x_i, f_i\\}\\)</span> pair values (showing the specific dots themselves can make it tough to observe qualitative patterns).</p>\n",
    "<p>For Problem 1, your report PDF should include:</p>\n",
    "<p>a. 4 row x 3 column grid of plots, where each panel shows 5 samples from the prior </p>\n",
    "<ul>\n",
    "<li>For the rows, try 4 different architectures: [2], [10], [2,2], [10, 10]</li>\n",
    "<li>For the columns, try 3 different activation functions: ReLu, tanh, and squared exponential (aka 'RBF')</li>\n",
    "</ul>\n",
    "<p>b. Short text description of the qualitative trends you observe. How does a deeper network impact the function shape? How does the activation function impact function shape? A few short but complete sentences.</p>\n",
    "<h2><a name=\"problem-2\">Problem 2: Sample from Posterior using your own HMC implementation </a></h2>\n",
    "<p>Consider the following training data with <span class=\"math\">\\(N=6\\)</span> example pairs of <span class=\"math\">\\(x\\)</span> and <span class=\"math\">\\(y\\)</span> values (as in HW1):</p>\n",
    "<div class=\"highlight\"><pre><span></span>x_train_N = np.asarray([-2.,    -1.8,   -1.,  1.,  1.8,     2.])\n",
    "y_train_N = np.asarray([-3.,  0.2224,    3.,  3.,  0.2224, -3.])\n",
    "</pre></div>\n",
    "\n",
    "\n",
    "<p>Your goal is to write an implementation of HMC that can sample from the posterior given this data. <strong>Hint:</strong> the pseudocode algorithm on Page 14 of Neal's Handbook of MCMC Chapter on HMC is a helpful resource: <a href=\"https://arxiv.org/pdf/1206.1901.pdf#page=14\">https://arxiv.org/pdf/1206.1901.pdf#page=14</a></p>\n",
    "<p>You may use automatic differentiation tools (like autograd, PyTorch, or Tensorflow) as you wish. We recommend autograd as a simple option that has been demonstrated in class. See 'Part 5' of the Jupyter Notebook we worked on in class for examples of useful NN data structures and gradient descent training of NNs with autograd: <a href=\"https://github.com/tufts-ml/comp150_bdl_2018f_public/blob/master/notebooks/intro_to_autograd_and_neural_net_training.ipynb\">intro_to_autograd_and_neural_net_training.ipynb</a>.</p>\n",
    "<p>You should think carefully about how you set the step_size and the number of leapfrog steps for an HMC proposal. You may need to try several values and find the one that performs best. </p>\n",
    "<p>You may refer to existing HMC implementations you find online for high-level understanding, but you <em>must</em> write your own code and be able to defend any code you submit as your original work. Working with a small set of fellow students from this class (at most 2 partners) is encouraged, provided you abide by the collaboration policy.</p>\n",
    "<p><em>Implementation Details</em>:</p>\n",
    "<ul>\n",
    "<li>Fix the BNN architecture to 1 layer of 10 hidden units, with <code>tanh</code> as the activation function.</li>\n",
    "<li>Use a Normal(0, 1) prior for all weights and biases (as in Part 1).</li>\n",
    "<li>Use <code>sigma=0.1</code> for the likelihood's Gaussian noise level.</li>\n",
    "<li>Run at least 3 chains each for at least 2000 total iterations (remember to worry about burnin).</li>\n",
    "</ul>\n",
    "<p><em>Instructions:</em> For Problem 2, your report PDF should include:</p>\n",
    "<p>a. Plot of the \"potential energy\" (aka negative log joint probability) vs. iteration, for each of your chains. This should be one plot with multiple lines.</p>\n",
    "<p>b. Plot of sampled function values from the \"posterior\", for multiple chains. Show 10 samples per plot. Avoid showing samples in the transient burn-in phase of the sampler.</p>\n",
    "<p>c. Plot of the empirical mean of many samples from the \"posterior\". Also show +/- 2 standard deviations (see matplotlib's <a href=\"https://matplotlib.org/gallery/recipes/fill_between_alpha.html\">fill_between</a> function). Avoid showing samples in the transient burn-in phase of the sampler.</p>\n",
    "<h3><a name=\"template-code\">Template Python Code</a></h3>\n",
    "<p>The following two function templates might help you solve Problem 2. You are not required to use either of these, but it just might help.</p>\n",
    "<p>See also the pseudocode found on Page 14 of Neal's paper: <a href=\"https://arxiv.org/pdf/1206.1901.pdf#page=14\">https://arxiv.org/pdf/1206.1901.pdf#page=14</a>.</p>\n",
    "<p>Brief template code intro: we assume we have defined Python functions that can</p>\n",
    "<ul>\n",
    "<li>Calculate the kinetic energy, given momentum values</li>\n",
    "<li>Calculate the potential energy of BNN regression, given some bnn parameter values (aka 'position')</li>\n",
    "<li>Calculate the <em>gradient</em> of the potential energy (perhaps via autograd)</li>\n",
    "</ul>\n",
    "<p>(You'll need to write each of these functions).</p>\n",
    "<p>Given these functions, we can build an HMC sampler using the <code>run_HMC_sampler</code> and <code>make_proposal_via_leapfrog_steps</code> functions defined below.</p>\n",
    "<h4>Template: Run HMC Sampler for many iterations</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-00eabd3f900f>, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-00eabd3f900f>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    cur_momentum_vec = # TODO draw momentum using prng\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def run_HMC_sampler(\n",
    "        init_bnn_params=None,\n",
    "        n_hmc_iters=100,\n",
    "        n_leapfrog_steps=1,\n",
    "        step_size=1.0,\n",
    "        random_seed=42,\n",
    "        calc_potential_energy=None,\n",
    "        calc_kinetic_energy=None,\n",
    "        calc_grad_potential_energy=None,\n",
    "        ):\n",
    "    \"\"\" Run HMC sampler for many iterations (many proposals)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bnn_samples : list\n",
    "        List of samples of NN parameters produced by HMC\n",
    "        Can be viewed as 'approximate' posterior samples if chain runs to convergence.\n",
    "    info : dict\n",
    "        Tracks energy values at each iteration and other diagnostics.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    See Neal's pseudocode algorithm for a single HMC proposal + acceptance:\n",
    "    https://arxiv.org/pdf/1206.1901.pdf#page=14\n",
    "\n",
    "    This function repeats many HMC proposal steps.\n",
    "    \"\"\"\n",
    "    # Create random-number-generator with specific seed for reproducibility\n",
    "    prng = np.random.RandomState(int(random_seed))\n",
    "\n",
    "    # Set initial bnn params\n",
    "    cur_bnn_params = init_bnn_params\n",
    "    cur_potential_energy = calc_potential_energy(cur_bnn_params)\n",
    "\n",
    "    bnn_samples = list()\n",
    "    # TODO make lists to track energies over iterations\n",
    "\n",
    "    n_accept = 0\n",
    "    for t in range(n_hmc_iters):\n",
    "        # Draw momentum for CURRENT configuration\n",
    "        cur_momentum_vec = # TODO draw momentum using prng\n",
    "\n",
    "        # Create PROPOSED configuration\n",
    "        prop_bnn_params, prop_momentum_vec = make_proposal_via_leapfrog_steps(\n",
    "            cur_bnn_params, cur_momentum_vec,\n",
    "            n_leapfrog_steps=n_leapfrog_steps,\n",
    "            step_size=step_size,\n",
    "            calc_grad_potential_energy=calc_grad_potential_energy)\n",
    "\n",
    "        # TODO Compute probability of accept/reject for proposal\n",
    "        # TODO You'll use need to use kinetic and potential energy functions   \n",
    "        accept_proba = 0.0 # (Placeholder)\n",
    "\n",
    "        # Draw random value from (0,1) to determine if we accept or not\n",
    "        if prng.rand() < accept_proba:\n",
    "            # If here, we accepted the proposal\n",
    "            n_accept += 1\n",
    "\n",
    "            # TODO what current state needs to be updated?\n",
    "\n",
    "        # Update list of samples from \"posterior\"\n",
    "        bnn_samples.append(cur_bnn_params)\n",
    "        # TODO update energy tracking lists\n",
    "\n",
    "        # Print some diagnostics every 50 iters\n",
    "        if t < 5 or ((t+1) % 50 == 0) or (t+1) == n_hmc_iters:\n",
    "            accept_rate = float(n_accept) / float(t+1)\n",
    "            print(\"iter %6d/%d after %7.1f sec | accept_rate %.3f\" % (\n",
    "                t+1, n_hmc_iters, time.time() - start_time_sec, accept_rate))\n",
    "\n",
    "    return (\n",
    "        bnn_samples,\n",
    "        dict(\n",
    "            n_accept=n_accept,\n",
    "            n_hmc_iters=n_hmc_iters,\n",
    "            accept_rate=accept_rate),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template: Construct a Single HMC Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-3a283f268fda>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-3a283f268fda>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def make_proposal_via_leapfrog_steps(\n",
    "        cur_bnn_params, cur_momentum_vec,\n",
    "        n_leapfrog_steps=1,\n",
    "        step_size=1.0,\n",
    "        calc_grad_potential_energy=None):\n",
    "    \"\"\" Construct one HMC proposal via leapfrog integration\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prop_bnn_params : same type/size as cur_bnn_params\n",
    "    prop_momentum_vec : same type/size as cur_momentum_vec\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize proposed variables as copies of current values\n",
    "    prop_bnn_params = copy.deepcopy(cur_bnn_params)\n",
    "    prop_momentum_vec = copy.deepcopy(cur_momentum_vec)\n",
    "\n",
    "    # TODO: half step update of momentum\n",
    "    # This will use the grad of potential energy (use provided function)\n",
    "\n",
    "    for step_id in range(n_leapfrog_steps):\n",
    "        # TODO: full step update of 'position' (aka bnn_params)\n",
    "        # This will use the grad of kinetic energy (has simple closed form)\n",
    "\n",
    "        if step_id < (n_leapfrog_steps - 1):\n",
    "            # TODO: full step update of momentum\n",
    "\n",
    "        else:\n",
    "            # Special case for final step\n",
    "            # TODO: half step update of momentum\n",
    "\n",
    "\n",
    "    # TODO: don't forget to flip sign of momentum (ensure symmetry)\n",
    "\n",
    "    return prop_bnn_params, prop_momentum_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name=\"debugging-tips\">Debugging Tips</a></h2>\n",
    "<p>Here are some tricks to simplify your life if you are having trouble:</p>\n",
    "<ul>\n",
    "<li>\n",
    "<p>Before you try HMC, see if you can get a simpler random walk Metropolis-Hastings sampler to work. This should be quite easy (just a few lines of code), but would let you verify that you understand how to evaluate acceptance probabilities for the BNN model.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p>Before you try HMC on a network with hidden units, try it with 0 hidden layers (equivalent to Bayesian linear regression). This should be a much simpler model with just two scalar random variables (weight and bias). </p>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
