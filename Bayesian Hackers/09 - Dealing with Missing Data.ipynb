{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Missing Data\n",
    "\n",
    "Missing data is a common problem in most real-world scientific datasets. While the best way for dealing with missing data will always be preventing their occurrence in the first place, it usually can't be helped, particularly when data are collected passively or voluntarily, or when data collection and recording is distributed among several people. \n",
    "\n",
    "For example, consider a survey dataset for some wildlife species:\n",
    "\n",
    "| Count | Site | Observer | Temperature |\n",
    "| ----- | ---- | -------- | ----------- |\n",
    "| 15    | 1    | 1        | 15          |\n",
    "| 10    | 1    | 2        | NA          |\n",
    "| 6     | 1    | 1        | 11          |\n",
    "\n",
    "Each row contains the number of individuals seen during the survey, along with three covariates: the site on which the survey was conducted, the observer that collected the data, and the temperature during the survey. If we are interested in modelling, say, population size as a function of the count and the associated covariates, it is difficult to accommodate the second observation because the temperature is missing (perhaps the thermometer was broken that day). Ignoring this observation will allow us to fit the model, but it wastes information that is contained in the other covariates.\n",
    "\n",
    "There are a variety of ways for dealing with missing data, from the very naïve to the very sophisticated, and unfortunately the more common approaches tend to be *ad hoc* and will usually do more harm than good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Learning outcomes for hearing-impaired children\n",
    "\n",
    "As a motivating example, we will use a dataset of educational outcomes for children with hearing impairment. Here, we are interested in determining factors that are associated with better or poorer learning outcomes. \n",
    "\n",
    "![hearing aid](images/hearing_aid.jpg)\n",
    "\n",
    "There is a suite of available predictors, including: \n",
    "\n",
    "* gender (`male`)\n",
    "* number of siblings in the household (`siblings`)\n",
    "* index of family involvement (`family_inv`)\n",
    "* whether the primary household language is not English (`non_english`)\n",
    "* presence of a previous disability (`prev_disab`)\n",
    "* non-white race (`non_white`)\n",
    "* age at the time of testing (in months, `age_test`)\n",
    "* whether hearing loss is not severe (`non_severe_hl`)\n",
    "* whether the subject's mother obtained a high school diploma or better (`mother_hs`)\n",
    "* whether the hearing impairment was identified by 3 months of age (`early_ident`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>male</th>\n",
       "      <th>siblings</th>\n",
       "      <th>family_inv</th>\n",
       "      <th>non_english</th>\n",
       "      <th>prev_disab</th>\n",
       "      <th>age_test</th>\n",
       "      <th>non_severe_hl</th>\n",
       "      <th>mother_hs</th>\n",
       "      <th>early_ident</th>\n",
       "      <th>non_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  male  siblings  family_inv non_english  prev_disab  age_test  \\\n",
       "0     40     0       2.0         2.0       False         NaN        55   \n",
       "1     31     1       0.0         NaN       False         0.0        53   \n",
       "2     83     1       1.0         1.0        True         0.0        52   \n",
       "3     75     0       3.0         NaN       False         0.0        55   \n",
       "5     62     0       0.0         4.0       False         1.0        50   \n",
       "\n",
       "   non_severe_hl  mother_hs early_ident non_white  \n",
       "0            1.0        NaN       False     False  \n",
       "1            0.0        0.0       False     False  \n",
       "2            1.0        NaN       False      True  \n",
       "3            0.0        1.0       False     False  \n",
       "5            0.0        NaN       False     False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores = pd.read_csv('../data/test_scores.csv', index_col=0)\n",
    "test_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For three variables in the dataset, there are incomplete records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score             0\n",
       "male              0\n",
       "siblings          0\n",
       "family_inv       33\n",
       "non_english       0\n",
       "prev_disab       18\n",
       "age_test          0\n",
       "non_severe_hl     0\n",
       "mother_hs        73\n",
       "early_ident       0\n",
       "non_white         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores.isnull().sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for dealing with missing data\n",
    "\n",
    "The easiest (and worst) way to deal with missing data is to **ignore it**. That is, simply run the analysis, missing values and all, hoping for the best. If your software is any good, this approach will simply not work; the algorithm will try to operate on data that includes missing values, and propagate them, resulting in statistics and estimates that cannot be calculated, which will typically raise errors. If your software is poor, it will make some assumption or decision about the missing values, and proceed to generate  results conditional on the assumption, which creates problems that may never be detected because no indication was given to any potential problem. \n",
    "\n",
    "The next easiest (worst) approach to analyzing data with missing values is to conduct list-wise deletion, by deleting the records that have missing values. This is called **complete case analysis**, because only records that are complete get retained for the analysis. The degree to which complete case analysis is undesirable depends on the mechanism by which data have become missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Missingness\n",
    "\n",
    "- **Missing completely at random (MCAR)**: When data are MCAR, missing cases are, on average, identical to non-missing cases, with respect to the model. Ignoring the missingness will reduce the power of the analysis, but will not bias inference.\n",
    "- **Missing at random (MAR)**: Missing data depends (usually probabilistically) on measured values, and hence can be modeled by variables observed in the data set. Accounting for the values which “cause” the missing data will produce unbiased results in an analysis.\n",
    "- **Missing not at random (MNAR)**: Missing data depend on unmeasured or unknown variables. There is no information available to account for the missingness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Consider a medical dataset collected that records both age and number of current medications. In this dataset, age is recorded for all individuals, but number of meds is missing for some individuals.\n",
    "> - if the probability of missingness is independent of both age and meds count, the data are MCAR (*missing at random, observed at random*)\n",
    "> - if the probability of missingness depends on age but not on meds count, the data are MAR (*missing at random, not observed at random*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very best-case scenario for using complete case analysis, which corresponds to MCAR missingness, results in a **loss of power** due to the reduction in sample size. The analysis will lose the information contained in the non-missing elements of a partially-missing record. *Deletion of records due to partially-missing variables will always result in estimates that are inefficient.*\n",
    "\n",
    "When data are not missing completely at random, inferences from complete case analysis may be **biased** due to systematic differences between missing and non-missing records that affects the estimates of key parameters. Here, we say that the missing data mechanism is **non-ignorable**.\n",
    "\n",
    "**It is usually better to make up values for missing data than to throw real data away!**\n",
    "\n",
    "When can record deletion be used?\n",
    "\n",
    "- Data are missing completely at random\n",
    "- Fraction of observations with missing elements is very small relative to overall sample size\n",
    "- predictor and response variables are both absent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the probability of missingnewss is related to the response variable, estimates can be badly biased. Its important to characterize the missingness in terms of the outcome of interest before deleting observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAEyCAYAAAAWQX0ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUxwDAED3/wEvvO4gv6LoSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ed0b5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12,5\n",
    "                                       ))\n",
    "for i,variable in enumerate(['mother_hs', 'prev_disab', 'family_inv']):\n",
    "    (test_scores.assign(missing=test_scores[variable].isnull())\n",
    "         .groupby('missing')\n",
    "         .score.hist(ax=axes[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "To make our estimates as efficient as possible, **we want to preserve the information and interpretation of the non-missing data**.\n",
    "\n",
    "One alternative to complete case analysis is to simply fill (*impute*) the missing values with a reasonable guess a the true value, such as the mean, median or modal value of the fully-observed records. This imputation, while not recovering any information regarding the missing value itself for use in the analysis, does provide a mechanism for including the non-missing values in the analysis, thereby making use of all available information.\n",
    "\n",
    "Performing mean imputation is easy in Pandas, via the DataFrame/Series `fillna` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1256038647342994"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores.siblings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "siblings_imputed = test_scores.siblings.fillna(test_scores.siblings.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach may be reasonable under the MCAR assumption, but may induce bias under a MAR scenario, whereby missing values may **differ systematically** relative to non-missing values, making the particular summary statistic used for imputation *biased* as a mean/median/modal value for the missing values.\n",
    "\n",
    "Beyond this, the use of a single imputed value to stand in place of the actual missing value glosses over the **uncertainty** associated with this guess at the true value. Any subsequent analysis procedure (*e.g.* regression analysis) will behave as if the imputed value were observed, despite the fact that we are actually unsure of the actual value for the missing variable. The practical consequence of this is that the variance of any estimates resulting from the imputed dataset will be **artificially reduced**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Imputation\n",
    "\n",
    "One robust alternative to addressing missing data is **multiple imputation** (Schaffer 1999, van Buuren 2012). It produces unbiased parameter estimates, while simultaneously accounting for the uncertainty associated with imputing missing values. It is conceptually and mechanistically straightforward, and produces complete datasets that may be analyzed using any statistical methodology or software one chooses, as if the data had no missing values to begin with.\n",
    "\n",
    "Multiple imputation generates imputed values based on a **regression model**. This regression model will help us generate reasonable values, particularly if data are MAR, since it uses information in the dataset that may be informative in predicting what the true value may be. Ideally, we want predictor variables that are **correlated** with the missing variable, and with the mechanism of missingness, if any. For example, one might be able to use test scores from one subject to predict missing test scores from another; or, the probability of income reporting to be missing may vary systematically according to the education level of the individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if there is any potential information among the variables in our dataset to use for imputation, it is helpful to calculate the pairwise correlation between all the variables. Since we have discrete variables in our data, the [Spearman rank correlation coefficient](http://www.wikiwand.com/en/Spearman%27s_rank_correlation_coefficient) is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>male</th>\n",
       "      <th>siblings</th>\n",
       "      <th>family_inv</th>\n",
       "      <th>non_english</th>\n",
       "      <th>prev_disab</th>\n",
       "      <th>age_test</th>\n",
       "      <th>non_severe_hl</th>\n",
       "      <th>mother_hs</th>\n",
       "      <th>early_ident</th>\n",
       "      <th>non_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073063</td>\n",
       "      <td>-0.085044</td>\n",
       "      <td>-0.539019</td>\n",
       "      <td>-0.278798</td>\n",
       "      <td>-0.184426</td>\n",
       "      <td>0.024057</td>\n",
       "      <td>0.140305</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.222711</td>\n",
       "      <td>-0.345061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.073063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.072006</td>\n",
       "      <td>-0.008714</td>\n",
       "      <td>0.053338</td>\n",
       "      <td>-0.052054</td>\n",
       "      <td>-0.081165</td>\n",
       "      <td>0.031825</td>\n",
       "      <td>0.050372</td>\n",
       "      <td>-0.007690</td>\n",
       "      <td>-0.048638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>siblings</th>\n",
       "      <td>-0.085044</td>\n",
       "      <td>-0.072006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078471</td>\n",
       "      <td>-0.049989</td>\n",
       "      <td>-0.038020</td>\n",
       "      <td>0.104905</td>\n",
       "      <td>-0.003689</td>\n",
       "      <td>0.096268</td>\n",
       "      <td>0.077318</td>\n",
       "      <td>0.006234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_inv</th>\n",
       "      <td>-0.539019</td>\n",
       "      <td>-0.008714</td>\n",
       "      <td>0.078471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221696</td>\n",
       "      <td>0.082314</td>\n",
       "      <td>-0.029120</td>\n",
       "      <td>-0.092815</td>\n",
       "      <td>-0.358898</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>0.401617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_english</th>\n",
       "      <td>-0.278798</td>\n",
       "      <td>0.053338</td>\n",
       "      <td>-0.049989</td>\n",
       "      <td>0.221696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021996</td>\n",
       "      <td>0.068095</td>\n",
       "      <td>-0.047775</td>\n",
       "      <td>-0.199639</td>\n",
       "      <td>-0.015812</td>\n",
       "      <td>0.225428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_disab</th>\n",
       "      <td>-0.184426</td>\n",
       "      <td>-0.052054</td>\n",
       "      <td>-0.038020</td>\n",
       "      <td>0.082314</td>\n",
       "      <td>-0.021996</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136604</td>\n",
       "      <td>0.048132</td>\n",
       "      <td>0.137893</td>\n",
       "      <td>0.046592</td>\n",
       "      <td>-0.021367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_test</th>\n",
       "      <td>0.024057</td>\n",
       "      <td>-0.081165</td>\n",
       "      <td>0.104905</td>\n",
       "      <td>-0.029120</td>\n",
       "      <td>0.068095</td>\n",
       "      <td>0.136604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.122811</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.068430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_severe_hl</th>\n",
       "      <td>0.140305</td>\n",
       "      <td>0.031825</td>\n",
       "      <td>-0.003689</td>\n",
       "      <td>-0.092815</td>\n",
       "      <td>-0.047775</td>\n",
       "      <td>0.048132</td>\n",
       "      <td>-0.122811</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015996</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0.028480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_hs</th>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.050372</td>\n",
       "      <td>0.096268</td>\n",
       "      <td>-0.358898</td>\n",
       "      <td>-0.199639</td>\n",
       "      <td>0.137893</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>-0.015996</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024411</td>\n",
       "      <td>-0.214209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_ident</th>\n",
       "      <td>0.222711</td>\n",
       "      <td>-0.007690</td>\n",
       "      <td>0.077318</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>-0.015812</td>\n",
       "      <td>0.046592</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0.024411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_white</th>\n",
       "      <td>-0.345061</td>\n",
       "      <td>-0.048638</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.401617</td>\n",
       "      <td>0.225428</td>\n",
       "      <td>-0.021367</td>\n",
       "      <td>0.068430</td>\n",
       "      <td>0.028480</td>\n",
       "      <td>-0.214209</td>\n",
       "      <td>-0.022854</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  score      male  siblings  family_inv  non_english  \\\n",
       "score          1.000000  0.073063 -0.085044   -0.539019    -0.278798   \n",
       "male           0.073063  1.000000 -0.072006   -0.008714     0.053338   \n",
       "siblings      -0.085044 -0.072006  1.000000    0.078471    -0.049989   \n",
       "family_inv    -0.539019 -0.008714  0.078471    1.000000     0.221696   \n",
       "non_english   -0.278798  0.053338 -0.049989    0.221696     1.000000   \n",
       "prev_disab    -0.184426 -0.052054 -0.038020    0.082314    -0.021996   \n",
       "age_test       0.024057 -0.081165  0.104905   -0.029120     0.068095   \n",
       "non_severe_hl  0.140305  0.031825 -0.003689   -0.092815    -0.047775   \n",
       "mother_hs      0.228500  0.050372  0.096268   -0.358898    -0.199639   \n",
       "early_ident    0.222711 -0.007690  0.077318    0.006370    -0.015812   \n",
       "non_white     -0.345061 -0.048638  0.006234    0.401617     0.225428   \n",
       "\n",
       "               prev_disab  age_test  non_severe_hl  mother_hs  early_ident  \\\n",
       "score           -0.184426  0.024057       0.140305   0.228500     0.222711   \n",
       "male            -0.052054 -0.081165       0.031825   0.050372    -0.007690   \n",
       "siblings        -0.038020  0.104905      -0.003689   0.096268     0.077318   \n",
       "family_inv       0.082314 -0.029120      -0.092815  -0.358898     0.006370   \n",
       "non_english     -0.021996  0.068095      -0.047775  -0.199639    -0.015812   \n",
       "prev_disab       1.000000  0.136604       0.048132   0.137893     0.046592   \n",
       "age_test         0.136604  1.000000      -0.122811   0.016760     0.033789   \n",
       "non_severe_hl    0.048132 -0.122811       1.000000  -0.015996     0.008211   \n",
       "mother_hs        0.137893  0.016760      -0.015996   1.000000     0.024411   \n",
       "early_ident      0.046592  0.033789       0.008211   0.024411     1.000000   \n",
       "non_white       -0.021367  0.068430       0.028480  -0.214209    -0.022854   \n",
       "\n",
       "               non_white  \n",
       "score          -0.345061  \n",
       "male           -0.048638  \n",
       "siblings        0.006234  \n",
       "family_inv      0.401617  \n",
       "non_english     0.225428  \n",
       "prev_disab     -0.021367  \n",
       "age_test        0.068430  \n",
       "non_severe_hl   0.028480  \n",
       "mother_hs      -0.214209  \n",
       "early_ident    -0.022854  \n",
       "non_white       1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores.dropna().corr(method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to impute missing values the mother's high school education indicator variable, which takes values of 0 for no high school diploma, or 1 for high school diploma or greater. The appropriate model to predict binary variables is a **logistic regression**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, we will only use variables that are themselves complete to build the predictive model, hence our subset of predictors will exclude family involvement score (`family_inv`) and previous disability (`prev_disab`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impute_subset = test_scores.drop(labels=['family_inv','prev_disab','score'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = impute_subset.pop('mother_hs').values\n",
    "X = impute_subset.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *training* and *test* sets in this case will be the non-missing and missing values, respectively, since we want to use supervised learning to build our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing = np.isnan(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a logistic regression model, and fit it using the non-missing observations.\n",
    "\n",
    "As a performance hack, we will normalize the input variables (center them and subtract by their respective standard deviations). This will make the models run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize = lambda x: (x - x.mean()) / x.std()\n",
    "X = X.apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can set up our logistic regression model in PyMC3, as we have done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, Bernoulli, invlogit\n",
    "\n",
    "with Model() as model:\n",
    "\n",
    "    μ = Normal('μ', 0, sd=10)\n",
    "    β = Normal('β', 0, sd=10, shape=X.shape[1])\n",
    "\n",
    "    p = invlogit(μ + β.dot(X[~missing].T))\n",
    "\n",
    "    mother_hs = Bernoulli('mother_hs', p, observed=y[~missing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have indexed out the non-missing values from `X` and `y`. \n",
    "\n",
    "We need to add one additional component to our model: **predictions for the unobserved values**. We can do this by creating a `Deterministic` node that calculates the expected probability at the missing values for high school education:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Deterministic\n",
    "\n",
    "with model:\n",
    "    \n",
    "    p_pred = Deterministic('p_pred', \n",
    "                    invlogit(μ + β.dot(X.loc[missing].T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates samples from the **posterior predictive distribution** for `p`, the same mechanism we used when peforming model checking.\n",
    "\n",
    "We can now run our model and extract the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 109.18: 100%|██████████| 20000/20000 [00:02<00:00, 9685.47it/s] \n",
      "Finished [100%]: Average Loss = 109.18\n"
     ]
    }
   ],
   "source": [
    "from pymc3 import fit\n",
    "\n",
    "with model:\n",
    "    \n",
    "    tr = fit(20000, random_seed=RANDOM_SEED).sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.77185554,  0.46366082,  0.69227549,  0.79256812,  0.42083697,\n",
       "        0.53936998,  0.80524387,  0.49734257,  0.81899292,  0.58292826,\n",
       "        0.71277124,  0.55940473,  0.76080566,  0.65288601,  0.79873058,\n",
       "        0.50568591,  0.82814364,  0.4712178 ,  0.71263727,  0.73398766,\n",
       "        0.74553918,  0.67907498,  0.77919538,  0.69435048,  0.50649043,\n",
       "        0.7456188 ,  0.40491748,  0.83531538,  0.85419456,  0.77355884,\n",
       "        0.76357147,  0.69907599,  0.50168539,  0.63739584,  0.4262249 ,\n",
       "        0.52163027,  0.73131633,  0.75121597,  0.71835811,  0.69227549,\n",
       "        0.42865549,  0.67278392,  0.7005504 ,  0.79288964,  0.59280548,\n",
       "        0.77779468,  0.63592453,  0.82996551,  0.68868687,  0.71950342,\n",
       "        0.41896307,  0.58117537,  0.84465594,  0.74923002,  0.84542377,\n",
       "        0.69553836,  0.84106226,  0.43771244,  0.68513284,  0.54805074,\n",
       "        0.54001904,  0.78759293,  0.78740536,  0.7612674 ,  0.84072848,\n",
       "        0.64402648,  0.73337787,  0.79108273,  0.49285189,  0.82301248,\n",
       "        0.70863529,  0.80985417,  0.86418764])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr['p_pred'].mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are *probabilities* of having a high school education; we need to use these in conjunction with a classification rule in order to create imputed values. We will assume `mother_hs=1` if the probability exceeds 0.5, or `mother_hs=0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tr['p_pred'].mean(0) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values can then be inserted in place of the missing values, and an analysis can be performed on the entire dataset.\n",
    "\n",
    "However, this is still just a single imputation for each missing value, and hence glosses over the uncertainty associated with the derivation of the imputes. Multiple imputation proceeds by **imputing several values**, to generate several complete datasets and performing the same analysis on all of them. With a set of estimates in hand, an *average* estimate of model parameters can be obtained that more adequately accounts for the uncertainty, hopefully providing more robust inference than from a single impute.\n",
    "\n",
    "There are a variety of ways to generate multiple imputations. Here, we will define a *set* of predictive models. For example, we can pick arbitrary subsets of the predictor variables and use each subset to generate a vector of imputed values.\n",
    "\n",
    "For efficiency, we will wrap a general model specification in a function, which can be instantiated with any subset of covariatest that we specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imputation_model(predictors):\n",
    "\n",
    "    with Model() as model:\n",
    "\n",
    "        μ = Normal('μ', 0, sd=10)\n",
    "        β = Normal('β', 0, sd=10, shape=len(predictors))\n",
    "\n",
    "        p = invlogit(μ + β.dot(X.loc[~missing, predictors].T))\n",
    "\n",
    "        mother_hs = Bernoulli('mother_hs', p, observed=y[~missing])\n",
    "\n",
    "        p_pred = Deterministic('p_pred', \n",
    "                        invlogit(μ + β.dot(X.loc[missing, predictors].T)))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly few imputations are required to acheive reasonable estimates, with 3-10 usually sufficient. We will use 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = (['non_english', 'age_test', 'non_severe_hl'],\n",
    "              ['male', 'siblings', 'non_english'],\n",
    "              X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run each of these models in turn and save the imputed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 94.925: 100%|██████████| 20000/20000 [00:01<00:00, 11133.77it/s]\n",
      "Finished [100%]: Average Loss = 94.925\n",
      "Average Loss = 94.783: 100%|██████████| 20000/20000 [00:01<00:00, 12270.39it/s]\n",
      "Finished [100%]: Average Loss = 94.785\n",
      "Average Loss = 109.18: 100%|██████████| 20000/20000 [00:01<00:00, 11348.17it/s]\n",
      "Finished [100%]: Average Loss = 109.18\n"
     ]
    }
   ],
   "source": [
    "mother_hs_imp = []\n",
    "\n",
    "for P in predictors:\n",
    "    \n",
    "    mod = imputation_model(P)\n",
    "    \n",
    "    with mod:\n",
    "    \n",
    "        tr = fit(20000, random_seed=RANDOM_SEED).sample(500)\n",
    "\n",
    "    imputed = (tr['p_pred'].mean(0) > 0.5).astype(int)\n",
    "    \n",
    "    mother_hs_imp.append(imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1]),\n",
       " array([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mother_hs_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform 3 separate analyses, using the method of our choice, each based upon a different set of imputed values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 1,296.2: 100%|██████████| 20000/20000 [00:02<00:00, 9186.46it/s]  \n",
      "Finished [100%]: Average Loss = 1,295.6\n",
      "Average Loss = 1,296.2: 100%|██████████| 20000/20000 [00:01<00:00, 11414.65it/s]  \n",
      "Finished [100%]: Average Loss = 1,295.6\n",
      "Average Loss = 1,296.2: 100%|██████████| 20000/20000 [00:01<00:00, 10907.66it/s]  \n",
      "Finished [100%]: Average Loss = 1,295.6\n"
     ]
    }
   ],
   "source": [
    "from pymc3 import HalfCauchy\n",
    "\n",
    "coefficients = []\n",
    "\n",
    "for imputes in mother_hs_imp:\n",
    "        \n",
    "    X = test_scores.drop(labels=['family_inv','prev_disab'], axis=1).astype(float)\n",
    "    X.loc[missing, 'mother_hs'] = imputes\n",
    "    y = X.pop('score')\n",
    "    \n",
    "    with Model():\n",
    "\n",
    "        μ = Normal('μ', 0, sd=10)\n",
    "        β = Normal('β', 0, sd=10, shape=X.shape[1])\n",
    "        \n",
    "        σ = HalfCauchy('σ', 1)\n",
    "\n",
    "        score = Normal('score', μ + β.dot(X.apply(normalize).T), sd=σ, observed=y)\n",
    "        \n",
    "        samples = fit(20000, random_seed=RANDOM_SEED).sample(1000)\n",
    "\n",
    "    coefficients.append(samples['β'].mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>siblings</th>\n",
       "      <th>non_english</th>\n",
       "      <th>age_test</th>\n",
       "      <th>non_severe_hl</th>\n",
       "      <th>mother_hs</th>\n",
       "      <th>early_ident</th>\n",
       "      <th>non_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.356000</td>\n",
       "      <td>-1.445651</td>\n",
       "      <td>-2.857459</td>\n",
       "      <td>1.215705</td>\n",
       "      <td>2.349807</td>\n",
       "      <td>2.940681</td>\n",
       "      <td>2.910947</td>\n",
       "      <td>-3.127525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.576308</td>\n",
       "      <td>-1.718688</td>\n",
       "      <td>-3.040058</td>\n",
       "      <td>1.486784</td>\n",
       "      <td>2.391821</td>\n",
       "      <td>2.740079</td>\n",
       "      <td>2.824085</td>\n",
       "      <td>-3.169633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.572083</td>\n",
       "      <td>-1.659527</td>\n",
       "      <td>-2.997017</td>\n",
       "      <td>1.410336</td>\n",
       "      <td>2.437533</td>\n",
       "      <td>2.772642</td>\n",
       "      <td>2.830254</td>\n",
       "      <td>-3.148109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       male  siblings  non_english  age_test  non_severe_hl  mother_hs  \\\n",
       "0 -0.356000 -1.445651    -2.857459  1.215705       2.349807   2.940681   \n",
       "1 -0.576308 -1.718688    -3.040058  1.486784       2.391821   2.740079   \n",
       "2 -0.572083 -1.659527    -2.997017  1.410336       2.437533   2.772642   \n",
       "\n",
       "   early_ident  non_white  \n",
       "0     2.910947  -3.127525  \n",
       "1     2.824085  -3.169633  \n",
       "2     2.830254  -3.148109  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_labels = ['male',\n",
    "                'siblings',\n",
    "                'non_english',\n",
    "                'age_test',\n",
    "                'non_severe_hl',\n",
    "                'mother_hs',\n",
    "                'early_ident',\n",
    "                'non_white']\n",
    "\n",
    "coef_df = pd.DataFrame(coefficients, columns=coeff_labels)\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for each coefficient is then based on the average of these three estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male            -0.501464\n",
       "siblings        -1.607955\n",
       "non_english     -2.964845\n",
       "age_test         1.370942\n",
       "non_severe_hl    2.393053\n",
       "mother_hs        2.817801\n",
       "early_ident      2.855095\n",
       "non_white       -3.148422\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we obtain somewhat different estimates when we use multiple imputation versus complete case analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 849.95: 100%|██████████| 20000/20000 [00:01<00:00, 10927.23it/s]   \n",
      "Finished [100%]: Average Loss = 849.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "male             0.944586\n",
       "siblings        -2.162773\n",
       "non_english     -2.704296\n",
       "age_test         1.384706\n",
       "non_severe_hl    2.326195\n",
       "mother_hs        3.168774\n",
       "early_ident      3.165545\n",
       "non_white       -3.155507\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = test_scores.drop(labels=['family_inv','prev_disab'], axis=1).astype(float).dropna(0)\n",
    "y = X.pop('score')\n",
    "\n",
    "with Model():\n",
    "\n",
    "    μ = Normal('μ', 0, sd=10)\n",
    "    β = Normal('β', 0, sd=10, shape=X.shape[1])\n",
    "\n",
    "    σ = HalfCauchy('σ', 1)\n",
    "\n",
    "    score = Normal('score', μ + β.dot(X.apply(normalize).T), sd=σ, observed=y)\n",
    "\n",
    "    samples = fit(20000, random_seed=RANDOM_SEED).sample(1000)\n",
    "    \n",
    "pd.Series(samples['β'].mean(0), index=coeff_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates from mean imputation are more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 1,296.2: 100%|██████████| 20000/20000 [00:01<00:00, 10128.39it/s]  \n",
      "Finished [100%]: Average Loss = 1,295.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "male            -0.387961\n",
       "siblings        -1.504720\n",
       "non_english     -3.135256\n",
       "age_test         1.492877\n",
       "non_severe_hl    2.347635\n",
       "mother_hs        2.925255\n",
       "early_ident      2.862539\n",
       "non_white       -3.202634\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = test_scores.drop(labels=['family_inv','prev_disab'], axis=1).astype(float)\n",
    "X['mother_hs'] = X.mother_hs.fillna(X.mother_hs.mean())\n",
    "y = X.pop('score')\n",
    "\n",
    "with Model():\n",
    "\n",
    "    μ = Normal('μ', 0, sd=10)\n",
    "    β = Normal('β', 0, sd=10, shape=X.shape[1])\n",
    "\n",
    "    σ = HalfCauchy('σ', 1)\n",
    "\n",
    "    score = Normal('score', μ + β.dot(X.apply(normalize).T), sd=σ, observed=y)\n",
    "\n",
    "    samples = fit(20000, random_seed=RANDOM_SEED).sample(1000)\n",
    "    \n",
    "pd.Series(samples['β'].mean(0), index=coeff_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated missing data imputation\n",
    "\n",
    "A more \"fully Bayesian\" approach for multiple imputation is to treat the missing values as latent variables, and estimate them just as one would with any other model unknown. This allows imputation to occur within the same modeling framework as the regession analysis itself. However, it involves constructing a sub-model for the missing data, which requires a little additional work.\n",
    "\n",
    "Let's first extract `mother_hs` from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = test_scores.drop(labels=['family_inv','prev_disab'], axis=1).astype(float)\n",
    "y = X.pop('score')\n",
    "mother_hs_ = X.pop('mother_hs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we provide a model for `mother_hs`, PyMC will automatically impute values as it draws posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     NaN\n",
       "1     0.0\n",
       "2     NaN\n",
       "3     1.0\n",
       "5     NaN\n",
       "6     NaN\n",
       "7     1.0\n",
       "9     0.0\n",
       "10    NaN\n",
       "12    1.0\n",
       "Name: mother_hs, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mother_hs_.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, here we will assume `mother_hs` is MCAR. So, all that is required is a sampling distribution for the variable, which is binary. So, an appropriate model is the Bernoulli distribution, just as we used for the likelihood in the logistic regression model.\n",
    "\n",
    "We will assign a uniform prior to the probability of the mother having at least a high school education. Here is the complete model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigned NUTS to π_interval__\n",
      "Assigned BinaryGibbsMetropolis to mother_hs_missing\n",
      "Assigned NUTS to μ\n",
      "Assigned NUTS to β\n",
      "Assigned NUTS to γ\n",
      "Assigned NUTS to σ_log__\n",
      "100%|██████████| 2000/2000 [00:42<00:00, 46.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from pymc3 import Uniform, sample\n",
    "\n",
    "with Model() as bayes_impute:\n",
    "    \n",
    "    π = Uniform('π', 0, 1)\n",
    "    mother_hs = Bernoulli('mother_hs', π, observed=mother_hs_)\n",
    "    \n",
    "    μ = Normal('μ', 0, sd=10)\n",
    "    β = Normal('β', 0, sd=10, shape=X.shape[1])\n",
    "    γ = Normal('γ', 0, sd=10)\n",
    "\n",
    "    σ = HalfCauchy('σ', 1)\n",
    "\n",
    "    score = Normal('score', μ + β.dot(X.apply(normalize).T) + γ*mother_hs, sd=σ, observed=y)\n",
    "\n",
    "    samples = sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.34 ,  0.799,  0.527,  0.72 ,  0.682,  0.872,  0.606,  0.829,\n",
       "        0.79 ,  0.636,  0.694,  0.772,  0.49 ,  0.594,  0.388,  0.333,\n",
       "        0.612,  0.17 ,  0.831,  0.507,  0.853,  0.792,  0.85 ,  0.268,\n",
       "        0.285,  0.801,  0.583,  0.638,  0.196,  0.817,  0.925,  0.782,\n",
       "        0.845,  0.685,  0.612,  0.59 ,  0.559,  0.705,  0.746,  0.579,\n",
       "        0.788,  0.751,  0.835,  0.678,  0.834,  0.836,  0.837,  0.759,\n",
       "        0.805,  0.783,  0.833,  0.801,  0.914,  0.919,  0.553,  0.653,\n",
       "        0.792,  0.663,  0.54 ,  0.806,  0.805,  0.918,  0.75 ,  0.608,\n",
       "        0.405,  0.538,  0.497,  0.915,  0.732,  0.686,  0.626,  0.844,\n",
       "        0.777])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['mother_hs_missing'].mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MCAR assumption can be relaxed and replaced by a more sophisticated model than the simpl Bernoulli sampling distribtion used here. In fact, we could implement the full multivariate model that we used for multiple imputation relatively easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "\n",
    "To consolidaate what we've learned here, let's consider the problem of predicting survival in the Titanic disaster, based on our available information. Here, we want to predict survival as a function of several variables that are available from the ship's record.\n",
    "\n",
    "![titanic](images/titanic_cube.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Allen, Miss. Elisabeth Walton\n",
       "1                          Allison, Master. Hudson Trevor\n",
       "2                            Allison, Miss. Helen Loraine\n",
       "3                    Allison, Mr. Hudson Joshua Creighton\n",
       "4         Allison, Mrs. Hudson J C (Bessie Waldo Daniels)\n",
       "5                                     Anderson, Mr. Harry\n",
       "6                       Andrews, Miss. Kornelia Theodosia\n",
       "7                                  Andrews, Mr. Thomas Jr\n",
       "8           Appleton, Mrs. Edward Dale (Charlotte Lamson)\n",
       "9                                 Artagaveytia, Mr. Ramon\n",
       "10                                 Astor, Col. John Jacob\n",
       "11      Astor, Mrs. John Jacob (Madeleine Talmadge Force)\n",
       "12                          Aubart, Mme. Leontine Pauline\n",
       "13                           Barber, Miss. Ellen \"Nellie\"\n",
       "14                   Barkworth, Mr. Algernon Henry Wilson\n",
       "15                                    Baumann, Mr. John D\n",
       "16                               Baxter, Mr. Quigg Edmond\n",
       "17        Baxter, Mrs. James (Helene DeLaudeniere Chaput)\n",
       "18                                  Bazzani, Miss. Albina\n",
       "19                                   Beattie, Mr. Thomson\n",
       "20                          Beckwith, Mr. Richard Leonard\n",
       "21       Beckwith, Mrs. Richard Leonard (Sallie Monypeny)\n",
       "22                                  Behr, Mr. Karl Howell\n",
       "23                                  Bidois, Miss. Rosalie\n",
       "24                                      Bird, Miss. Ellen\n",
       "25                                    Birnbaum, Mr. Jakob\n",
       "26                                Bishop, Mr. Dickinson H\n",
       "27                Bishop, Mrs. Dickinson H (Helen Walton)\n",
       "28                                 Bissette, Miss. Amelia\n",
       "29              Bjornstrom-Steffansson, Mr. Mauritz Hakan\n",
       "                              ...                        \n",
       "1279                 Vestrom, Miss. Hulda Amanda Adolfina\n",
       "1280                                      Vovk, Mr. Janko\n",
       "1281                                 Waelens, Mr. Achille\n",
       "1282                                  Ware, Mr. Frederick\n",
       "1283                          Warren, Mr. Charles William\n",
       "1284                                    Webber, Mr. James\n",
       "1285                                  Wenzel, Mr. Linhart\n",
       "1286      Whabee, Mrs. George Joseph (Shawneene Abi-Saab)\n",
       "1287                     Widegren, Mr. Carl/Charles Peter\n",
       "1288                            Wiklund, Mr. Jakob Alfred\n",
       "1289                              Wiklund, Mr. Karl Johan\n",
       "1290                     Wilkes, Mrs. James (Ellen Needs)\n",
       "1291                     Willer, Mr. Aaron (\"Abi Weller\")\n",
       "1292                                   Willey, Mr. Edward\n",
       "1293                    Williams, Mr. Howard Hugh \"Harry\"\n",
       "1294                                 Williams, Mr. Leslie\n",
       "1295                                  Windelov, Mr. Einar\n",
       "1296                                     Wirz, Mr. Albert\n",
       "1297                               Wiseman, Mr. Phillippe\n",
       "1298                            Wittevrongel, Mr. Camille\n",
       "1299                                  Yasbeck, Mr. Antoni\n",
       "1300              Yasbeck, Mrs. Antoni (Selini Alexander)\n",
       "1301                                 Youseff, Mr. Gerious\n",
       "1302                                    Yousif, Mr. Wazli\n",
       "1303                                Yousseff, Mr. Gerious\n",
       "1304                                 Zabour, Miss. Hileni\n",
       "1305                                Zabour, Miss. Thamine\n",
       "1306                            Zakarian, Mr. Mapriededer\n",
       "1307                                  Zakarian, Mr. Ortin\n",
       "1308                                   Zimmerman, Mr. Leo\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_excel(\"../data/titanic.xls\", \"titanic\")\n",
    "titanic.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a reasonable Bayesian multivariate generalized linear model with this data, using several variables such as sex, age and fare to predict survival. Make sure you involve the following steps:\n",
    "\n",
    "- data imputation\n",
    "- model checking\n",
    "- model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- R. J. A. Little and D. B. Rubin. Statistical Analysis with Missing Data. Wiley, New York, second edition, 2002.\n",
    "- M. J. Knol, K. J. M. Janssen, R. T. Donders, A. C. G. Egberts, E. R. Heerding, D. E. Grobbee, K. G. M. Moons, and M. I. Geerlings. [Unpredictable bias when using the missing indicator method or complete case analysis for missing confounder values: an empirical example](https://www.ncbi.nlm.nih.gov/pubmed/20346625). J Clin Epi, 63:728–736, 2010.\n",
    "- van Buuren, S. (2012). Flexible Imputation of Missing Data (pp. 1–326).\n",
    "- Schafer, J. L. (1999). Multiple imputation: a primer. Statistical Methods in Medical Research, 8(1), 3–15. http://doi.org/10.1177/096228029900800102"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
