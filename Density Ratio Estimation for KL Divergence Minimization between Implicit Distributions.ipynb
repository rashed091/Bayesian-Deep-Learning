{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The Kullback-Leibler (KL) divergence between distributions $p$ and $q$ is\n",
    "defined as</p>\n",
    "\n",
    "<p>$$\n",
    "\\mathcal{D}_{\\mathrm{KL}}[p(x) || q(x)] :=\n",
    "\\mathbb{E}_{p(x)} \\left [ \\log \\left ( \\frac{p(x)}{q(x)} \\right ) \\right ].\n",
    "$$</p>\n",
    "\n",
    "<p>It can be expressed more succinctly as</p>\n",
    "\n",
    "<p>$$\n",
    "\\mathcal{D}_{\\mathrm{KL}}[p(x) || q(x)] = \\mathbb{E}_{p(x)} [ \\log r^{*}(x) ],\n",
    "$$</p>\n",
    "\n",
    "<p>where $r^{*}(x)$ is defined to be the ratio of between the densities $p(x)$ and\n",
    "$q(x)$,</p>\n",
    "\n",
    "<p>$$\n",
    "r^{*}(x) := \\frac{p(x)}{q(x)}.\n",
    "$$</p>\n",
    "\n",
    "<p>This density ratio is crucial for computing not only the KL divergence but for\n",
    "all $f$-divergences, defined as<sup class=\"footnote-ref\" id=\"fnref:1\"><a href=\"#fn:1\">1</a></sup></p>\n",
    "\n",
    "<p>$$\n",
    "\\mathcal{D}_f[p(x) || q(x)] :=\n",
    "\\mathbb{E}_{q(x)} \\left [ f \\left ( \\frac{p(x)}{q(x)} \\right ) \\right ].\n",
    "$$</p>\n",
    "\n",
    "<p>Rarely can this expectation (i.e. integral) can be calculated analytically&mdash;in\n",
    "most cases, we must resort to Monte Carlo approximation methods, which\n",
    "explicitly requires the density ratio.\n",
    "In the more severe case where this density ratio is unavailable, because either\n",
    "or both $p(x)$ and $q(x)$ are not calculable, we must resort to methods for\n",
    "<em>density ratio estimation</em>.\n",
    "In this post, we illustrate how to perform density ratio estimation by\n",
    "exploiting its tight correspondence to <em>probabilistic classification</em>.</p>\n",
    "\n",
    "<h3 id=\"example-univariate-gaussians\">Example: Univariate Gaussians</h3>\n",
    "\n",
    "<p>Let us consider the following univariate Gaussian distributions as the running\n",
    "example for this post,</p>\n",
    "\n",
    "<p>$$\n",
    "p(x) = \\mathcal{N}(x \\mid 1, 1^2),\n",
    "\\qquad\n",
    "\\text{and}\n",
    "\\qquad\n",
    "q(x) = \\mathcal{N}(x \\mid 0, 2^2).\n",
    "$$</p>\n",
    "\n",
    "<p>We will be using <em>TensorFlow</em>, <em>TensorFlow Probability</em>, and <em>Keras</em> in the\n",
    "code snippets throughout this post.</p>\n",
    "\n",
    "<pre><code class=\"language-python\">import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "</code></pre>\n",
    "\n",
    "<p>We first instantiate the distributions:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">p = tfp.distributions.Normal(loc=1., scale=1.)\n",
    "q = tfp.distributions.Normal(loc=0., scale=2.)\n",
    "</code></pre>\n",
    "\n",
    "<p>Their densities are shown below:</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"gaussian_1d_densities.svg\" alt=\"Univariate Gaussian densities.\" />\n",
    "\n",
    "\n",
    "\n",
    "<figcaption data-pre=\"Figure \" data-post=\":\" >\n",
    "  \n",
    "  <p>\n",
    "    Univariate Gaussian densities.\n",
    "    \n",
    "    \n",
    "    \n",
    "  </p> \n",
    "</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "<p>For any pair of distributions, we can implement their density ratio function $r$\n",
    "as follows:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">def log_density_ratio(p, q):\n",
    "\n",
    "    def log_ratio(x):\n",
    "\n",
    "        return p.log_prob(x) - q.log_prob(x)\n",
    "\n",
    "    return log_ratio\n",
    "</code></pre>\n",
    "\n",
    "<pre><code class=\"language-python\">def density_ratio(p, q):\n",
    "\n",
    "    log_ratio = log_density_ratio(p, q)\n",
    "\n",
    "    def ratio(x):\n",
    "\n",
    "        return tf.exp(log_ratio(x))\n",
    "\n",
    "    return ratio\n",
    "</code></pre>\n",
    "\n",
    "<p>Let&rsquo;s create the density ratio function for the Gaussian distributions we just\n",
    "instantiated:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; r = density_ratio(p, q)\n",
    "</code></pre>\n",
    "\n",
    "<p>This density ratio function is plotted as the orange dotted line below,\n",
    "alongside the individual densities shown in the previous plot:</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"gaussian_1d_density_ratios.svg\" alt=\"Ratio of Gaussian densities.\" />\n",
    "\n",
    "\n",
    "\n",
    "<figcaption data-pre=\"Figure \" data-post=\":\" >\n",
    "  \n",
    "  <p>\n",
    "    Ratio of Gaussian densities.\n",
    "    \n",
    "    \n",
    "    \n",
    "  </p> \n",
    "</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "<h2 id=\"analytical-form\">Analytical Form</h2>\n",
    "\n",
    "<p>For our running example, we picked $p(x)$ and $q(x)$ to be Gaussians so that\n",
    "it is possible to integrate out $x$ and compute the KL divergence <em>analytically</em>.\n",
    "When we introduce the approximate methods later, this will provide us a &ldquo;gold\n",
    "standard&rdquo; to benchmark against.</p>\n",
    "\n",
    "<p>In general, for Gaussian distributions</p>\n",
    "\n",
    "<p>$$\n",
    "p(x) = \\mathcal{N}(x \\mid \\mu_p, \\sigma_p^2),\n",
    "\\qquad\n",
    "\\text{and}\n",
    "\\qquad\n",
    "q(x) = \\mathcal{N}(x \\mid \\mu_q, \\sigma_q^2),\n",
    "$$\n",
    "it is easy to verify that\n",
    "$$\n",
    "\\mathrm{KL}[ p(x) || q(x) ]\n",
    "= \\log \\sigma_q - \\log \\sigma_p - \\frac{1}{2}\n",
    "  \\left [\n",
    "    1 - \\left ( \\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{\\sigma_q^2} \\right )\n",
    "  \\right ].\n",
    "$$</p>\n",
    "\n",
    "<p>This is implemented below:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">def _kl_divergence_gaussians(p, q):\n",
    "\n",
    "    r = p.loc - q.loc\n",
    "\n",
    "    return (tf.log(q.scale) - tf.log(p.scale) -\n",
    "            .5 * (1. - (p.scale**2 + r**2) / q.scale**2))\n",
    "</code></pre>\n",
    "\n",
    "<p>We can use this to compute the KL divergence between $p(x)$ and $q(x)$\n",
    "<em>exactly</em>:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; _kl_divergence_gaussians(p, q).eval()\n",
    "0.44314718\n",
    "</code></pre>\n",
    "\n",
    "<p>Equivalently, we could also use <code>kl_divergence</code> from <em>TensorFlow\n",
    "Probability&ndash;Distributions</em> (<code>tfp.distributions</code>), which implements the\n",
    "analytical closed-form expression of the KL divergence between distributions\n",
    "when such exists.</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; tfp.distributions.kl_divergence(p, q).eval()\n",
    "0.44314718\n",
    "</code></pre>\n",
    "\n",
    "<h2 id=\"monte-carlo-estimation-prescribed-distributions\">Monte Carlo Estimation &mdash; prescribed distributions</h2>\n",
    "\n",
    "<p>For distributions where their KL divergence is not analytically tractable, we\n",
    "may appeal to Monte Carlo (MC) estimation:</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  \\mathcal{D}_{\\mathrm{KL}}[p(x) || q(x)]\n",
    "  &amp; = \\mathbb{E}_{p(x)} [ \\log r^{*}(x) ] \\newline\n",
    "  &amp; \\approx \\frac{1}{M} \\sum_{i=1}^{M} \\log r^{*}(x_p^{(i)}),\n",
    "  \\quad x_p^{(i)} \\sim p(x).\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>Clearly, this requires the density ratio $r^{*}(x)$ and, in turn, the densities\n",
    "$p(x)$ and $q(x)$ to be analytically tractable. Distributions for which the\n",
    "density function can be readily evaluated are sometimes referred to as\n",
    "<strong>prescribed distributions</strong>. As before, we <em>prescribed</em> Gaussians distributions\n",
    "in our running example so the Monte Carlo estimate can be later compared against.\n",
    "We approximate their KL divergence using $M = 5000$ Monte Carlo samples as\n",
    "follows:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; p_samples = p.sample(5000)\n",
    "&gt;&gt;&gt; true_log_ratio = log_density_ratio(p, q)\n",
    "&gt;&gt;&gt; tf.reduce_mean(true_log_ratio(p_samples)).eval()\n",
    "0.44670376\n",
    "</code></pre>\n",
    "\n",
    "<p>Or equivalently, using the <code>expectation</code> function from <em>TensorFlow\n",
    "Probability&ndash;Monte Carlo</em> (<code>tfp.monte_carlo</code>):</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; tfp.monte_carlo.expectation(f=true_log_ratio, samples=p_samples).eval()\n",
    "0.4581419\n",
    "</code></pre>\n",
    "\n",
    "<p>More generally, we can approximate any $f$-divergence with MC estimation:</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  \\mathcal{D}_f[p(x) || q(x)]\n",
    "  &amp; = \\mathbb{E}_{q(x)} [ f(r^{*}(x)) ] \\newline\n",
    "  &amp; \\approx \\frac{1}{M} \\sum_{i=1}^{M} f(r^{*}(x_q^{(i)})),\n",
    "  \\quad x_q^{(i)} \\sim q(x).\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>This can be done using the  <code>monte_carlo_csiszar_f_divergence</code> function from\n",
    "<em>TensorFlow Probability&ndash;Variational Inference</em> (<code>tfp.vi</code>).\n",
    "One simply needs to specify the appropriate convex function $f$.\n",
    "The convex function that instantiates the (forward) KL divergence is provided\n",
    "in <code>tfp.vi</code> as <code>kl_forward</code>, alongside many other common $f$-divergences.</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; tfp.vi.monte_carlo_csiszar_f_divergence(f=tfp.vi.kl_forward,\n",
    "...                                         p_log_prob=p.log_prob, q=q,\n",
    "...                                         num_draws=5000).eval()\n",
    "0.4430853\n",
    "</code></pre>\n",
    "\n",
    "<h2 id=\"density-ratio-estimation-implicit-distributions\">Density Ratio Estimation &mdash; implicit distributions</h2>\n",
    "\n",
    "<p>When either density $p(x)$ or $q(x)$ is unavailable, things become more tricky.\n",
    "Which brings us to the topic of this post. Suppose we only have samples from\n",
    "$p(x)$ and $q(x)$&mdash;these could be natural images, outputs from a neural\n",
    "network with stochastic inputs, or in the case of our running example, i.i.d.\n",
    "samples drawn from Gaussians, etc.\n",
    "Distributions for which we are only able to observe their samples are known as\n",
    "<strong>implicit distributions</strong>, since their samples <em>imply</em> some underlying true\n",
    "density which we may not have direct access to.</p>\n",
    "\n",
    "<p>Density ratio estimation is concerned with estimating the ratio of densities\n",
    "$r^{*}(x) = p(x) / q(x)$ given access only to samples from $p(x)$ and $q(x)$.\n",
    "Moreover, density ratio estimation usually encompass methods that achieve this\n",
    "without resorting to direct <em>density estimation</em> of the individual densities\n",
    "$p(x)$ or $q(x)$, since any error in the estimation of the denominator $q(x)$\n",
    "is magnified exponentially.</p>\n",
    "\n",
    "<p>Of the many density ratio estimation methods that now\n",
    "flourish<sup class=\"footnote-ref\" id=\"fnref:sugiyama2012density\"><a href=\"#fn:sugiyama2012density\">2</a></sup>, the classical approach of <em>probabilistic\n",
    "classification</em> remains dominant, due in no small part to its simplicity.</p>\n",
    "\n",
    "<h3 id=\"reducing-density-ratio-estimation-to-probabilistic-classification\">Reducing Density Ratio Estimation to Probabilistic Classification</h3>\n",
    "\n",
    "<p>We now demonstrate that density ratio estimation can be reduced to probabilistic\n",
    "classification. We shall do this by highlighting the one-to-one correspondence\n",
    "between the density ratio of $p(x)$ and $q(x)$ and the optimal probabilistic\n",
    "classifier that discriminates between their samples.\n",
    "Specifically, suppose we have a collection of samples from both $p(x)$ and $q(x)$,\n",
    "where each sample is assigned a class label indicating which distribution it was\n",
    "drawn from. Then, from an estimator of the class-membership probabilities, it is\n",
    "straightforward to recover an estimator of the density ratio.</p>\n",
    "\n",
    "<p>Suppose we have $N_p$ and $N_q$ samples drawn from $p(x)$ and $q(x)$,\n",
    "respectively,</p>\n",
    "\n",
    "<p>$$\n",
    "x_p^{(1)}, \\dotsc, x_p^{(N_p)} \\sim p(x),\n",
    "\\qquad \\text{and} \\qquad\n",
    "x_q^{(1)}, \\dotsc, x_q^{(N_q)} \\sim q(x).\n",
    "$$</p>\n",
    "\n",
    "<p>Then, we form the dataset ${ (x_n, y_n) }_{n=1}^N$, where $N = N_p + N_q$\n",
    "and</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  (x_1, \\dotsc, x_N) &amp; = (x_p^{(1)}, \\dotsc, x_p^{(N_p)},\n",
    "                            x_q^{(1)}, \\dotsc, x_q^{(N_q)}), \\newline\n",
    "  (y_1, \\dotsc, y_N) &amp; = (\\underbrace{1, \\dotsc, 1}_{N_p},\n",
    "                            \\underbrace{0, \\dotsc, 0}_{N_q}).\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>In other words, we label samples drawn from $p(x)$ as 1 and those drawn from\n",
    "$q(x)$ as 0. In code, this looks like:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; p_samples = p.sample(sample_shape=(n_p, 1))\n",
    "&gt;&gt;&gt; q_samples = q.sample(sample_shape=(n_q, 1))\n",
    "&gt;&gt;&gt; X = tf.concat([p_samples, q_samples], axis=0)\n",
    "&gt;&gt;&gt; y = tf.concat([tf.ones_like(p_samples), tf.zeros_like(q_samples)], axis=0)\n",
    "</code></pre>\n",
    "\n",
    "<p>This dataset is visualized below. The blue squares in the top row are samples\n",
    "$x_p^{(i)} \\sim p(x)$ with label 1; red squares in the bottom row are samples\n",
    "$x_q^{(j)} \\sim q(x)$ with label 0.</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"dataset.svg\" alt=\"Classification dataset.\" />\n",
    "\n",
    "\n",
    "\n",
    "<figcaption data-pre=\"Figure \" data-post=\":\" >\n",
    "  \n",
    "  <p>\n",
    "    Classification dataset.\n",
    "    \n",
    "    \n",
    "    \n",
    "  </p> \n",
    "</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "<p>Now, by construction, we have</p>\n",
    "\n",
    "<p>$$\n",
    "p(x) = \\mathcal{P}(x \\mid y = 1),\n",
    "\\qquad\n",
    "\\text{and}\n",
    "\\qquad\n",
    "q(x) = \\mathcal{P}(x \\mid y = 0).\n",
    "$$</p>\n",
    "\n",
    "<p>Using Baye&rsquo;s rule, we can write</p>\n",
    "\n",
    "<p>$$\n",
    "\\mathcal{P}(x \\mid y) =\n",
    "\\frac{\\mathcal{P}(y \\mid x) \\mathcal{P}(x)}\n",
    "     {\\mathcal{P}(y)}.\n",
    "$$</p>\n",
    "\n",
    "<p>Hence, we can express the density ratio $r^{*}(x)$ as</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  r^{*}(x) &amp; = \\frac{p(x)}{q(x)}\n",
    "       = \\frac{\\mathcal{P}(x \\mid y = 1)}\n",
    "              {\\mathcal{P}(x \\mid y = 0)} \\newline\n",
    "       &amp; = \\left ( \\frac{\\mathcal{P}(y = 1 \\mid x) \\mathcal{P}(x)}\n",
    "                        {\\mathcal{P}(y = 1)} \\right )\n",
    "           \\left ( \\frac{\\mathcal{P}(y = 0 \\mid x) \\mathcal{P}(x)}\n",
    "                        {\\mathcal{P}(y = 0)} \\right ) ^ {-1} \\newline\n",
    "       &amp; = \\frac{\\mathcal{P}(y = 0)}{\\mathcal{P}(y = 1)}\n",
    "           \\frac{\\mathcal{P}(y = 1 \\mid x)}\n",
    "                {\\mathcal{P}(y = 0 \\mid x)}.\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>Let us approximate the ratio of marginal densities by the ratio of sample sizes,</p>\n",
    "\n",
    "<p>$$\n",
    "\\frac{\\mathcal{P}(y = 0)}\n",
    "     {\\mathcal{P}(y = 1)}\n",
    "\\approx\n",
    "\\frac{N_q}{N_p + N_q}\n",
    "\\left ( \\frac{N_p}{N_p + N_q} \\right )^{-1}\n",
    "= \\frac{N_q}{N_p}.\n",
    "$$</p>\n",
    "\n",
    "<p>To avoid notational clutter, let us assume from now on that $N_q = N_p$.\n",
    "We can then write $r^{*}(x)$ in terms of class-posterior probabilities,</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  r^{*}(x) = \\frac{\\mathcal{P}(y = 1 \\mid x)}\n",
    "              {\\mathcal{P}(y = 0 \\mid x)}.\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<h4 id=\"recovering-the-density-ratio-from-the-class-probability\">Recovering the Density Ratio from the Class Probability</h4>\n",
    "\n",
    "<p>This yields a one-to-one correspondence between the density ratio $r^{*}(x)$\n",
    "and the class-posterior probability $\\mathcal{P}(y = 1 \\mid x)$.\n",
    "Namely,</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  r^{*}(x) = \\frac{\\mathcal{P}(y = 1 \\mid x)}\n",
    "              {\\mathcal{P}(y = 0 \\mid x)}\n",
    "       &amp; = \\frac{\\mathcal{P}(y = 1 \\mid x)}\n",
    "                {1 - \\mathcal{P}(y = 1 \\mid x)} \\newline\n",
    "       &amp; = \\exp\n",
    "           \\left [\n",
    "             \\log \\frac{\\mathcal{P}(y = 1 \\mid x)}\n",
    "                       {1 - \\mathcal{P}(y = 1 \\mid x)} \\right ] \\newline\n",
    "       &amp; = \\exp[ \\sigma^{-1}(\\mathcal{P}(y = 1 \\mid x)) ],\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>where $\\sigma^{-1}$ is the <em>logit</em> function, or inverse sigmoid function, given\n",
    "by $\\sigma^{-1}(\\rho) = \\log \\left ( \\frac{\\rho}{1-\\rho} \\right )$</p>\n",
    "\n",
    "<h4 id=\"recovering-the-class-probability-from-the-density-ratio\">Recovering the Class Probability from the Density Ratio</h4>\n",
    "\n",
    "<p>By simultaneously manipulating both sides of this equation, we can also recover\n",
    "the exact class-posterior probability as a function of the density ratio,</p>\n",
    "\n",
    "<p>$$\n",
    "\\mathcal{P}(y=1 \\mid x) = \\sigma(\\log r^{*}(x)) = \\frac{p(x)}{p(x) + q(x)}.\n",
    "$$</p>\n",
    "\n",
    "<p>This is implemented below:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">def optimal_classifier(p, q):\n",
    "\n",
    "    def classifier(x):\n",
    "\n",
    "        return tf.truediv(p.prob(x), p.prob(x) + q.prob(x))\n",
    "\n",
    "    return classifier\n",
    "</code></pre>\n",
    "\n",
    "<p>In the figure below, The class-posterior probability $\\mathcal{P}(y=1 \\mid x)$\n",
    "is plotted against the dataset visualized earlier.</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"optimal_classifier.svg\" alt=\"Optimal classifier---class-posterior probabilities.\" />\n",
    "\n",
    "\n",
    "\n",
    "<figcaption data-pre=\"Figure \" data-post=\":\" >\n",
    "  \n",
    "  <p>\n",
    "    Optimal classifier&mdash;class-posterior probabilities.\n",
    "    \n",
    "    \n",
    "    \n",
    "  </p> \n",
    "</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "<!-- <div class=\"alert alert-note\">\n",
    "  <div>\n",
    "    <p>If there is just one thing you take away from this post, let it be this:</p>\n",
    "\n",
    "<p>$$\n",
    "r^{*}(x) = \\exp[ \\sigma^{-1}(\\mathcal{P}(y = 1 \\mid x)) ],\n",
    "$$</p>\n",
    "\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    " -->\n",
    "\n",
    "<h3 id=\"probabilistic-classification-with-logistic-regression\">Probabilistic Classification with Logistic Regression</h3>\n",
    "\n",
    "<p>The class-posterior probability $\\mathcal{P}(y = 1 \\mid x)$ can be approximated\n",
    "using a parameterized function $D_{\\theta}(x)$ with parameters $\\theta$. This\n",
    "functions takes as input samples from $p(x)$ and $q(x)$ and outputs a <em>score</em>,\n",
    "or probability, in the range $[0, 1]$ that it was drawn from $p(x)$.\n",
    "Hence, we refer to $D_{\\theta}(x)$ as the probabilistic classifier.</p>\n",
    "\n",
    "<p>From before, it is clear to see how an estimator of the density ratio\n",
    "$r_{\\theta}(x)$ might be constructed as a function of probabilistic classifier\n",
    "$D_{\\theta}(x)$. Namely,</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  r_{\\theta}(x) &amp; = \\exp[ \\sigma^{-1}(D_{\\theta}(x)) ] \\newline\n",
    "  &amp; \\approx \\exp[ \\sigma^{-1}(\\mathcal{P}(y = 1 \\mid x)) ] = r^{*}(x),\n",
    "\\end{align}\n",
    "$$\n",
    "and <em>vice versa</em>,\n",
    "$$\n",
    "\\begin{align}\n",
    "  D_{\\theta}(x) &amp; = \\sigma(\\log r_{\\theta}(x)) \\newline\n",
    "  &amp; \\approx \\sigma(\\log r^{*}(x)) = \\mathcal{P}(y = 1 \\mid x).\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>Instead of $D_{\\theta}(x)$, we usually specify the parameterized function\n",
    "$\\log r_{\\theta}(x)$. This is also referred to as the <em>log-odds</em>, or <em>logits</em>,\n",
    "since it is equivalent to the unnormalized output of the classifier before being\n",
    "fed through the logistic sigmoid function.</p>\n",
    "\n",
    "<p>We define a small fully-connected neural network with two hidden layers and ReLU\n",
    "activations:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">log_ratio = Sequential([\n",
    "    Dense(16, input_dim=1, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1),\n",
    "])\n",
    "</code></pre>\n",
    "\n",
    "<p>This simple architecture is visualized in the diagram below:</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"log_ratio_architecture.svg\" alt=\"Log Density Ratio Architecture.\" />\n",
    "\n",
    "\n",
    "\n",
    "<figcaption data-pre=\"Figure \" data-post=\":\" >\n",
    "  \n",
    "  <p>\n",
    "    Log Density Ratio Architecture.\n",
    "    \n",
    "    \n",
    "    \n",
    "  </p> \n",
    "</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "<p>We learn the optimal class probability estimator by optimizing it with respect\n",
    "to a <em>proper scoring rule</em><sup class=\"footnote-ref\" id=\"fnref:gneiting2007strictly\"><a href=\"#fn:gneiting2007strictly\">3</a></sup> that yields well-calibrated probabilistic predictions, such as the <em>binary cross-entropy loss</em>,</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  \\mathcal{L}(\\theta) &amp; :=\n",
    "  - \\mathbb{E}_{p(x)} [ \\log D_{\\theta} (x) ]\n",
    "  - \\mathbb{E}_{q(x)} [ \\log(1-D_{\\theta} (x)) ] \\newline\n",
    "  &amp; =\n",
    "  - \\mathbb{E}_{p(x)} [ \\log \\sigma ( \\log r_{\\theta} (x) ) ]\n",
    "  - \\mathbb{E}_{q(x)} [ \\log(1 - \\sigma ( \\log r_{\\theta} (x) )) ].\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>An implementation optimized for numerical stability is given below:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">def _binary_crossentropy(log_ratio_p, log_ratio_q):\n",
    "\n",
    "    loss_p = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=log_ratio_p,\n",
    "        labels=tf.ones_like(log_ratio_p)\n",
    "    )\n",
    "\n",
    "    loss_q = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=log_ratio_q,\n",
    "        labels=tf.zeros_like(log_ratio_q)\n",
    "    )\n",
    "\n",
    "    return tf.reduce_mean(loss_p + loss_q)\n",
    "</code></pre>\n",
    "\n",
    "<p>Now we can build a <a href=\"https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models\" target=\"_blank\">multi-input/multi-output model</a>, where the <a href=\"http://louistiao.me/notes/keras-constant-input-layers-with-fixed-source-of-stochasticity/\" target=\"_blank\">inputs\n",
    "are fixed with stochastic tensors</a>&mdash;samples from\n",
    "$p(x)$ and $q(x)$, respectively.</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; x_p = Input(tensor=p_samples)\n",
    "&gt;&gt;&gt; x_q = Input(tensor=q_samples)\n",
    "&gt;&gt;&gt; log_ratio_p = log_ratio(x_p)\n",
    "&gt;&gt;&gt; log_ratio_q = log_ratio(x_q)\n",
    "</code></pre>\n",
    "\n",
    "<p>The model can now be compiled and finalized. Since we&rsquo;re using a custom loss\n",
    "that take the two sets of log-ratios as input, we specify <code>loss=None</code> and\n",
    "define it instead through the <code>add_loss</code> method.</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; m = Model(inputs=[x_p, x_q], outputs=[log_ratio_p, log_ratio_q])\n",
    "&gt;&gt;&gt; m.add_loss(_binary_crossentropy(log_ratio_p, log_ratio_q))\n",
    "&gt;&gt;&gt; m.compile(optimizer='rmsprop', loss=None)\n",
    "</code></pre>\n",
    "\n",
    "<p>As a sanity-check, the loss evaluated on a random batch can be obtained like so:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; m.evaluate(x=None, steps=1)\n",
    "1.3765026330947876\n",
    "</code></pre>\n",
    "\n",
    "<p>We can now fit our estimator, recording the loss at the end of each epoch:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; hist = m.fit(x=None, y=None, steps_per_epoch=1, epochs=500)\n",
    "</code></pre>\n",
    "\n",
    "<p>The following animation shows how the predictions for the probabilistic\n",
    "classifier, density ratio, log density ratio, evolve after every epoch:</p>\n",
    "\n",
    "<video controls autoplay src=\"https://giant.gfycat.com/FrighteningThunderousFlicker.webm\"></video>\n",
    "\n",
    "<p>It is overlaid on top of their exact, analytical counterparts, which are only\n",
    "available since we prescribed them to be Gaussian distribution.\n",
    "For implicit distributions, these won&rsquo;t be accessible at all.</p>\n",
    "\n",
    "<p>Below is the final plot of how the binary cross-entropy loss converges:</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"binary_crossentropy.svg\" alt=\"Binary Cross-entropy Loss.\" />\n",
    "\n",
    "\n",
    "\n",
    "<figcaption data-pre=\"Figure \" data-post=\":\" >\n",
    "  \n",
    "  <p>\n",
    "    Binary Cross-entropy Loss.\n",
    "    \n",
    "    \n",
    "    \n",
    "  </p> \n",
    "</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "<p>Below is a plot of the probabilistic classifier $D_{\\theta}(x)$ (<em>dotted green</em>),\n",
    "plotted against the optimal classifier, which is the class-posterior probability\n",
    "$\\mathcal{P}(y=1 \\mid x) = \\frac{p(x)}{p(x) + q(x)}$ (<em>solid blue</em>):</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"class_probability_estimation.svg\" alt=\"Class Probability Estimator.\" />\n",
    "\n",
    "\n",
    "\n",
    "<figcaption data-pre=\"Figure \" data-post=\":\" >\n",
    "  \n",
    "  <p>\n",
    "    Class Probability Estimator.\n",
    "    \n",
    "    \n",
    "    \n",
    "  </p> \n",
    "</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "<p>Below is a plot of the density ratio estimator $r_{\\theta}(x)$\n",
    "(<em>dotted green</em>), plotted against the exact density ratio function\n",
    "$r^{*}(x) = \\frac{p(x)}{q(x)}$ (<em>solid blue</em>):</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"density_ratio_estimation.svg\" alt=\"Density Ratio Estimator.\" />\n",
    "\n",
    "\n",
    "\n",
    "<figcaption data-pre=\"Figure \" data-post=\":\" >\n",
    "  \n",
    "  <p>\n",
    "    Density Ratio Estimator.\n",
    "    \n",
    "    \n",
    "    \n",
    "  </p> \n",
    "</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "<p>And finally, the previous plot in logarithmic scale:</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"log_density_ratio_estimation.svg\" alt=\"Log Density Ratio Estimator.\" />\n",
    "\n",
    "\n",
    "\n",
    "<figcaption data-pre=\"Figure \" data-post=\":\" >\n",
    "  \n",
    "  <p>\n",
    "    Log Density Ratio Estimator.\n",
    "    \n",
    "    \n",
    "    \n",
    "  </p> \n",
    "</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "<p>While it may appear that we are simply performing regression on the latent\n",
    "function $r^{*}(x)$ (which is not wrong&mdash;we are), it is important to emphasize that\n",
    "we do this without ever having observed values of $r^{*}(x)$.\n",
    "Instead, we only ever observed samples from $p(x)$ and $q(x)$\n",
    "This has profound implications and potential for a great number of applications\n",
    "that we shall explore later on.</p>\n",
    "\n",
    "<h3 id=\"back-to-monte-carlo-estimation\">Back to Monte Carlo estimation</h3>\n",
    "\n",
    "<p>Having an obtained an estimate of the log density ratio, it is now feasible to\n",
    "perform Monte Carlo estimation:</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  \\mathcal{D}_{\\mathrm{KL}}[p(x) || q(x)]\n",
    "  &amp; = \\mathbb{E}_{p(x)} [ \\log r^{*}(x) ] \\newline\n",
    "  &amp; \\approx \\frac{1}{M} \\sum_{i=1}^{M} \\log r^{*}(x_p^{(i)}),\n",
    "  \\quad x_p^{(i)} \\sim p(x) \\newline\n",
    "  &amp; \\approx \\frac{1}{M} \\sum_{i=1}^{M} \\log r_{\\theta}(x_p^{(i)}),\n",
    "  \\quad x_p^{(i)} \\sim p(x).\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; tf.squeeze(tfp.monte_carlo.expectation(f=log_ratio, samples=p_samples)).eval()\n",
    "0.4570999\n",
    "</code></pre>\n",
    "\n",
    "<p>In other words, we draw MC samples from $p(x)$ as before. But instead of taking\n",
    "the mean of the function $\\log r^{*}(x)$ evaluated on these samples (which is\n",
    "unavailable for implicit distributions), we do so on a proxy function\n",
    "$\\log r_{\\theta}(x)$ that is estimated through probabilistic classification as\n",
    "described above.</p>\n",
    "\n",
    "<h2 id=\"learning-in-implicit-generative-models\">Learning in Implicit Generative Models</h2>\n",
    "\n",
    "<p>Now let&rsquo;s take a look at where these ideas are being used in practice.\n",
    "Consider a collection of natural images, such as the MNIST handwritten\n",
    "digits shown below, which are assumed to be samples drawn from some implicit\n",
    "distribution $q(\\mathbf{x})$:</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"MnistExamples.png\" alt=\"MNIST hand-written digits.\" />\n",
    "\n",
    "\n",
    "\n",
    "<figcaption data-pre=\"Figure \" data-post=\":\" >\n",
    "  \n",
    "  <p>\n",
    "    MNIST hand-written digits.\n",
    "    \n",
    "    \n",
    "    \n",
    "  </p> \n",
    "</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "<p>Directly estimating the density of $q(\\mathbf{x})$ may not always be feasible&mdash;in\n",
    "some cases, it may not even exist.\n",
    "Instead, consider defining a parametric function $G_{\\phi}: \\mathbf{z} \\mapsto\n",
    "\\mathbf{x}$ with parameters $\\phi$, that takes as input $\\mathbf{z}$ drawn from\n",
    "some fixed distribution $p(\\mathbf{z})$.\n",
    "The outputs $\\mathbf{x}$ of this generative process are assumed to be samples\n",
    "following some implicit distribution $p_{\\phi}(\\mathbf{x})$. In other words,\n",
    "we can write</p>\n",
    "\n",
    "<p>$$\n",
    "\\mathbf{x} \\sim p_{\\phi}(\\mathbf{x}) \\quad\n",
    "\\Leftrightarrow \\quad\n",
    "\\mathbf{x} = G_{\\phi}(\\mathbf{z}),\n",
    "\\quad \\mathbf{z} \\sim p(\\mathbf{z}).\n",
    "$$</p>\n",
    "\n",
    "<p>By optimizing parameters $\\phi$, we can make $p_{\\phi}(\\mathbf{x})$ close to\n",
    "the real data distribution $q(\\mathbf{x})$. This is a compelling alternative to\n",
    "density estimation since there are many situations where being able to generate\n",
    "samples is more important than being able to calculate the numerical value of\n",
    "the density. Some examples of these include <em>image super-resolution</em> and\n",
    "<em>semantic segmentation</em>.</p>\n",
    "\n",
    "<p>One approach might be to introduce a classifier $D_{\\theta}$ that discriminates\n",
    "between real and synthetic samples.\n",
    "Then we optimize $G_{\\phi}$ to synthesize samples that are indistinguishable,\n",
    "to classifier $D_{\\theta}$, from the real samples. This can be achieved by\n",
    "simultaneously optimizing the binary cross-entropy loss, resulting in the\n",
    "saddle-point objective,</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  &amp; \\min_{\\phi} \\max_{\\theta}\n",
    "  \\mathbb{E}_{q(\\mathbf{x})} [ \\log D_{\\theta} (\\mathbf{x}) ] +\n",
    "  \\mathbb{E}_{p_{\\phi}(\\mathbf{x})} [ \\log(1-D_{\\theta} (\\mathbf{x})) ] \\newline\n",
    "  =\n",
    "  &amp; \\min_{\\phi} \\max_{\\theta}\n",
    "  \\mathbb{E}_{q(\\mathbf{x})} [ \\log D_{\\theta} (\\mathbf{x}) ] +\n",
    "  \\mathbb{E}_{p(\\mathbf{z})} [ \\log(1-D_{\\theta} (G_{\\phi}(\\mathbf{z}))) ].\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>This is, of course, none other than the groundbreaking <em>generative adversarial\n",
    "network (GAN)</em><sup class=\"footnote-ref\" id=\"fnref:goodfellow2014generative\"><a href=\"#fn:goodfellow2014generative\">4</a></sup>.\n",
    "You can read more about the density ratio estimation perspective of GANs in\n",
    "the paper by Uehara et al. 2016<sup class=\"footnote-ref\" id=\"fnref:uehara2016generative\"><a href=\"#fn:uehara2016generative\">5</a></sup>. For an even more general and complete treatment of learning in implicit models, I recommend the paper\n",
    "from Mohamed and Lakshminarayanan, 2016<sup class=\"footnote-ref\" id=\"fnref:mohamed2016learning\"><a href=\"#fn:mohamed2016learning\">6</a></sup>, which partially inspired this post.</p>\n",
    "\n",
    "<p>For the remainder of this section, I want to highlight a variant of this\n",
    "approach that specifically aims to minimize the KL divergence w.r.t. parameters\n",
    "$\\phi$,</p>\n",
    "\n",
    "<p>$$\n",
    "\\min_{\\phi} \\mathcal{D}_{\\mathrm{KL}}[p_{\\phi}(\\mathbf{x}) || q(\\mathbf{x})].\n",
    "$$</p>\n",
    "\n",
    "<p>To overcome the fact that the densities of both $p_{\\phi}(\\mathbf{x})$ and\n",
    "$q(\\mathbf{x})$ are unknown, we can readily adopt the density ratio estimation\n",
    "approach outlined in this post.\n",
    "Namely, by maximizing the following objective,</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  &amp; \\max_{\\theta}\n",
    "  \\mathbb{E}_{q(\\mathbf{x})} [ \\log D_{\\theta} (\\mathbf{x}) ] +\n",
    "  \\mathbb{E}_{p(\\mathbf{z})} [ \\log(1-D_{\\theta} (G_{\\phi}(\\mathbf{z}))) ] \\newline\n",
    "  = &amp; \\max_{\\theta}\n",
    "  \\mathbb{E}_{q(\\mathbf{x})} [ \\log \\sigma ( \\log r_{\\theta} (\\mathbf{x}) ) ] +\n",
    "  \\mathbb{E}_{p(\\mathbf{z})} [ \\log(1 - \\sigma ( \\log r_{\\theta} (G_{\\phi}(\\mathbf{z})) )) ],\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>which attains its maximum at</p>\n",
    "\n",
    "<p>$$\n",
    "r_{\\theta}(\\mathbf{x}) = \\frac{q(\\mathbf{x})}{p_{\\phi}(\\mathbf{x})}.\n",
    "$$</p>\n",
    "\n",
    "<p>Concurrently, we also minimize the current best estimate of the KL divergence,</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "\\min_{\\phi} \\mathcal{D}_{\\mathrm{KL}}[p_{\\phi}(\\mathbf{x}) || q(\\mathbf{x})]\n",
    "&amp; =\n",
    "\\min_{\\phi} \\mathbb{E}_{p_{\\phi}(\\mathbf{x})} \\left [ \\log \\frac{p_{\\phi}(\\mathbf{x})}{q(\\mathbf{x})} \\right ] \\newline\n",
    "&amp; \\approx\n",
    "\\min_{\\phi} \\mathbb{E}_{p_{\\phi}(\\mathbf{x})} [ - \\log r_{\\theta}(\\mathbf{x}) ] \\newline\n",
    "&amp; =\n",
    "\\min_{\\phi} \\mathbb{E}_{p(\\mathbf{z})} [ - \\log r_{\\theta}(G_{\\phi}(\\mathbf{z})) ].\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>In addition to being more stable than the vanilla GAN approach (alleviates\n",
    "saturating gradients), this is especially important in contexts where there is\n",
    "a specific need to minimize the KL divergence, such as in <em>variational inference\n",
    "(VI)</em>.</p>\n",
    "\n",
    "<p>This was first used in <em>AffGAN</em> by SÃ¸nderby et al. 2016<sup class=\"footnote-ref\" id=\"fnref:sonderby2016amortised\"><a href=\"#fn:sonderby2016amortised\">7</a></sup>,\n",
    "and has since been incorporated in many papers that deal with implicit\n",
    "distributions in variational inference, such as\n",
    "(Mescheder et al. 2017<sup class=\"footnote-ref\" id=\"fnref:mescheder2017adversarial\"><a href=\"#fn:mescheder2017adversarial\">8</a></sup>,\n",
    "Huszar 2017<sup class=\"footnote-ref\" id=\"fnref:huszar2017variational\"><a href=\"#fn:huszar2017variational\">9</a></sup>,\n",
    "Tran et al. 2017<sup class=\"footnote-ref\" id=\"fnref:tran2017hierarchical\"><a href=\"#fn:tran2017hierarchical\">10</a></sup>,\n",
    "Pu et al. 2017<sup class=\"footnote-ref\" id=\"fnref:pu2017adversarial\"><a href=\"#fn:pu2017adversarial\">11</a></sup>,\n",
    "Chen et al. 2018<sup class=\"footnote-ref\" id=\"fnref:chen2018symmetric\"><a href=\"#fn:chen2018symmetric\">12</a></sup>,\n",
    "Tiao et al. 2018<sup class=\"footnote-ref\" id=\"fnref:tiao2018cycle\"><a href=\"#fn:tiao2018cycle\">13</a></sup>), and many others.</p>\n",
    "\n",
    "<h2 id=\"bound-on-the-jensen-shannon-divergence\">Bound on the Jensen-Shannon Divergence</h2>\n",
    "\n",
    "<p>Before we wrap things up, let us take another look at the plot of the\n",
    "binary-cross entropy loss recorded at the end of each epoch.\n",
    "We see that it converges quickly to some value.\n",
    "It is natural to wonder: what is the significance, if any, of this value?</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"binary_crossentropy_vs_jensen_shannon.svg\" alt=\"Binary cross-entropy loss converges to Jensen Shannon divergence (up to constants).\" />\n",
    "\n",
    "\n",
    "\n",
    "<figcaption data-pre=\"Figure \" data-post=\":\" >\n",
    "  \n",
    "  <p>\n",
    "    Binary cross-entropy loss converges to Jensen Shannon divergence (up to constants).\n",
    "    \n",
    "    \n",
    "    \n",
    "  </p> \n",
    "</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "<p>It is in fact the (negative) Jensen-Shannon (JS) divergence, up to constants,</p>\n",
    "\n",
    "<p>$$\n",
    "- 2 \\cdot \\mathcal{D}_{\\mathrm{JS}}[p(x) || q(x)] + \\log 4.\n",
    "$$</p>\n",
    "\n",
    "<p>Recall the Jensen-Shannon divergence is defined as</p>\n",
    "\n",
    "<p>$$\n",
    "\\mathcal{D}_{\\mathrm{JS}}[p(x) || q(x)]\n",
    "= \\frac{1}{2} \\mathcal{D}_{\\mathrm{KL}}[p(x) || m(x)] +\n",
    "  \\frac{1}{2} \\mathcal{D}_{\\mathrm{KL}}[q(x) || m(x)],\n",
    "$$</p>\n",
    "\n",
    "<p>where $m$ is the mixture density</p>\n",
    "\n",
    "<p>$$\n",
    "m(x) = \\frac{p(x) + q(x)}{2}.\n",
    "$$</p>\n",
    "\n",
    "<p>With our running example, this cannot be evaluated exactly since the KL\n",
    "divergence between a Gaussian and a mixture of Gaussians is analytically\n",
    "tractable. However, like the KL, we can still estimate their JS divergence with\n",
    "Monte Carlo estimation<sup class=\"footnote-ref\" id=\"fnref:2\"><a href=\"#fn:2\">14</a></sup>:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">&gt;&gt;&gt; js = - tfp.vi.monte_carlo_csiszar_f_divergence(f=tfp.vi.jensen_shannon,\n",
    "...                                                p_log_prob=p.log_prob,\n",
    "...                                                q=q, num_draws=5000)\n",
    "</code></pre>\n",
    "\n",
    "<p>This value is shown in the horizontal black line in the plot above. Along the\n",
    "right margin, we also plot the a histogram of the binary cross-entropy loss\n",
    "values over epochs. We can see that this value indeed coincides with the mode of\n",
    "this histogram.</p>\n",
    "\n",
    "<p>It is straightforward to show that we have the upper bound</p>\n",
    "\n",
    "<p>$$\n",
    "\\inf_{\\theta} \\mathcal{L}(\\theta)\n",
    "\\geq\n",
    "- 2 \\cdot \\mathcal{D}_{\\mathrm{JS}}[p(x) || q(x)] + \\log 4.\n",
    "$$</p>\n",
    "\n",
    "<p>Firstly, we have</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  \\sup_{\\theta} &amp;\n",
    "  \\mathbb{E}_{p(x)} [ \\log D_{\\theta} (x) ] +\n",
    "  \\mathbb{E}_{q(x)} [ \\log(1-D_{\\theta} (x)) ] \\newline\n",
    "  &amp; =\n",
    "  \\mathbb{E}_{p(x)} [ \\log \\mathcal{P}(y=1 \\mid x) ] +\n",
    "  \\mathbb{E}_{q(x)} [ \\log \\mathcal{P}(y=0 \\mid x) ] \\newline\n",
    "  &amp; =\n",
    "  \\mathbb{E}_{p(x)} \\left [ \\log \\frac{p(x)}{p(x) + q(x)} \\right ] +\n",
    "  \\mathbb{E}_{q(x)} \\left [ \\log \\frac{q(x)}{p(x) + q(x)} \\right ] \\newline\n",
    "  &amp; =\n",
    "  \\mathbb{E}_{p(x)} \\left [ \\log \\frac{1}{2} \\frac{p(x)}{m(x)} \\right ] +\n",
    "  \\mathbb{E}_{q(x)} \\left [ \\log \\frac{1}{2} \\frac{q(x)}{m(x)} \\right ] \\newline\n",
    "  &amp; =\n",
    "  \\mathbb{E}_{p(x)} \\left [ \\log \\frac{p(x)}{m(x)} \\right ] +\n",
    "  \\mathbb{E}_{q(x)} \\left [ \\log \\frac{q(x)}{m(x)} \\right ] - 2 \\log 2 \\newline\n",
    "  &amp; = 2 \\cdot \\mathcal{D}_{\\mathrm{JS}}[p(x) || q(x)] - \\log 4.\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>Therefore,</p>\n",
    "\n",
    "<p>$$\n",
    "2 \\cdot \\mathcal{D}_{\\mathrm{JS}}[p(x) || q(x)] - \\log 4\n",
    "\\geq\n",
    "\\sup_{\\theta}\n",
    "\\mathbb{E}_{p(x)} [ \\log D_{\\theta} (x) ] +\n",
    "\\mathbb{E}_{q(x)} [ \\log(1-D_{\\theta} (x)) ].\n",
    "$$</p>\n",
    "\n",
    "<p>Negating both sides, we get</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  - 2 \\cdot \\mathcal{D}_{\\mathrm{JS}}[p(x) || q(x)] + \\log 4\n",
    "  \\leq &amp;\n",
    "  - \\sup_{\\theta}\n",
    "  \\mathbb{E}_{p(x)} [ \\log D_{\\theta} (x) ] +\n",
    "  \\mathbb{E}_{q(x)} [ \\log(1-D_{\\theta} (x)) ] \\newline\n",
    "  = &amp; \\inf_{\\theta}\n",
    "  - \\mathbb{E}_{p(x)} [ \\log D_{\\theta} (x) ]\n",
    "  - \\mathbb{E}_{q(x)} [ \\log(1-D_{\\theta} (x)) ] \\newline\n",
    "  = &amp; \\inf_{\\theta} \\mathcal{L}(\\theta),\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>as required.</p>\n",
    "\n",
    "<p>In short, this tells us that the binary cross-entropy loss is <em>itself</em> an\n",
    "approximation (up to constants) to the Jensen-Shannon divergence.\n",
    "This begs the question: is it possible to construct a more general loss that bounds any given $f$-divergence?</p>\n",
    "\n",
    "<h2 id=\"teaser-lower-bound-on-any-f-divergence\">Teaser: Lower Bound on any $f$-divergence</h2>\n",
    "\n",
    "<p>Using convex analysis, one can actually show that for any $f$-divergence, we\n",
    "have the lower bound<sup class=\"footnote-ref\" id=\"fnref:nguyen2010estimating\"><a href=\"#fn:nguyen2010estimating\">15</a></sup></p>\n",
    "\n",
    "<p>$$\n",
    "\\mathcal{D}_f[p(x) || q(x)]\n",
    "\\geq\n",
    "\\sup_{\\theta}\n",
    "\\mathbb{E}_{p(x)} [ f&rsquo;(r_{\\theta}(x)) ] -\n",
    "\\mathbb{E}_{q(x)} [ f^{\\star}(f&rsquo;(r_{\\theta}(x))) ],\n",
    "$$</p>\n",
    "\n",
    "<p>with equality exactly when $r_{\\theta}(x) = r^{*}(x)$.\n",
    "Importantly, this lower bound can be computed without requiring the densities of\n",
    "$p(x)$ or $q(x)$&mdash;only their samples are needed.</p>\n",
    "\n",
    "<p>In the special case of $f(u) = u \\log u - (u + 1) \\log (u + 1)$, we recover the\n",
    "binary cross-entropy loss and the previous result, as expected,</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  \\mathcal{D}_f[p(x) || q(x)]\n",
    "  &amp; = 2 \\cdot \\mathcal{D}_{\\mathrm{JS}}[p(x) || q(x)] - \\log 4 \\newline\n",
    "  &amp; \\geq \\sup_{\\theta}\n",
    "  \\mathbb{E}_{p(x)} [ \\log \\sigma ( \\log r_{\\theta} (x) ) ] +\n",
    "  \\mathbb{E}_{q(x)} [ \\log(1 - \\sigma ( \\log r_{\\theta} (x) )) ] \\newline\n",
    "  &amp; = \\sup_{\\theta}\n",
    "  \\mathbb{E}_{p(x)} [ \\log D_{\\theta} (x) ] +\n",
    "  \\mathbb{E}_{q(x)} [ \\log(1-D_{\\theta} (x)) ].\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>Alternately, in the special case of $f(u) = u \\log u$, we get</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  \\mathcal{D}_f[p(x) || q(x)]\n",
    "  &amp; = \\mathcal{D}_{\\mathrm{KL}}[p(x) || q(x)] \\newline\n",
    "  &amp; \\geq \\sup_{\\theta}\n",
    "  \\mathbb{E}_{p(x)} [ \\log r_{\\theta} (x) ] -\n",
    "  \\mathbb{E}_{q(x)} [ r_{\\theta} (x) - 1 ].\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>This gives us <em>yet</em> another way to estimate the KL divergence between\n",
    "implicit distributions, in the form of a direct lower bound on the KL divergence\n",
    "itself.\n",
    "As it turns out, this lower bound is closely-related to the objective of the\n",
    "<em>KL Importance Estimation Procedure (KLIEP)</em><sup class=\"footnote-ref\" id=\"fnref:sugiyama2008direct\"><a href=\"#fn:sugiyama2008direct\">16</a></sup>, and will be\n",
    "the topic of our next post in this series.</p>\n",
    "\n",
    "<h1 id=\"summary\">Summary</h1>\n",
    "\n",
    "<p>This post covered how to evaluate the KL divergence, or any $f$-divergence,\n",
    "between implicit distributions&mdash;distributions which we can only sample from.\n",
    "First, we underscored the crucial role of the density ratio in the estimation of\n",
    "$f$-divergences.\n",
    "Next, we showed the correspondence between the density ratio and the optimal\n",
    "classifier.\n",
    "By exploiting this link, we demonstrated how one can use a trained probabilistic classifier to construct a proxy for the exact density ratio, and use this to\n",
    "enable estimation of any $f$-divergence.\n",
    "Finally, we provided some context on where this method is used, touching upon\n",
    "some recent advances in implicit generative models and variational inference.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
