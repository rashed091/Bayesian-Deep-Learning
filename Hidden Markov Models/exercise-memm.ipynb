{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Information Extraction - Sequences - Maximum Entropy Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Requirements](#Requirements) \n",
    "  * [Knowledge](#Knowledge)\n",
    "  * [Modules](#Python-Modules)\n",
    "  * [Data](#Data)\n",
    "* [Feature Functions](#Feature-Functions)\n",
    "* [Exercises](#Exercises)\n",
    "  * [Build a tag vocabulary](#Build-a-tag-vocabulary)\n",
    "  * [Most frequent tag](#Most-frequent-tag)\n",
    "  * [Features of the word](#Features-of-the-word)\n",
    "  * [Constructing-feature-vectors](#Constructing-feature-vectors)\n",
    "  * [A MEMM Skeleton](#A-MEMM-skeleton)\n",
    "    * [Fit-function](#Fit-function)\n",
    "    * [Learning with Gradient Descent](#Learning-with-Gradient-Descent)\n",
    "    * [Decoding with Viterbi](#Decoding-with-Viterbi)\n",
    "  * [Trying it out](#Trying-it-out)\n",
    "  * [Label Bias Problem](#Label-Bias-Problem)\n",
    "* [Summary and Outlook](#Summary-and-Outlook)\n",
    "* [Literature](#Literature)\n",
    "* [Licenses](#Licenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will implement a Parts-of-Speech (POS) tagger using a Maximum Entropy Markov Model (MEMM).\n",
    "\n",
    "You will extract features from words, using properties of the word (e.g. prefixes) and its context (e.g. the previous word in the sentence).\n",
    "\n",
    "In the main part, you'll implement a POS tagger so it can fit to training to data, learn from the data using gradient descent and decode sentences, i.e. predict a best POS tag for each word of the sentence. You can then run your tagger on the test data.\n",
    "\n",
    "We'll wrap up with an example of the label-bias problem present in MEMMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Knowledge\n",
    "* You should have previous knowledge of classification using multinomial gradient descent. A recommended resource is [Chapter 5 : Logistic Regression](https://web.stanford.edu/~jurafsky/slp3/5.pdf) of Speech and Language Processing [[JUR18]](#JUR18).\n",
    "\n",
    "* You should be comfortable implementing gradient descent. There are many resources available on the topic, for example the blog post [Gradient Descent in a Nutshell](https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0) by [Niklas Donges](https://towardsdatascience.com/@n.donges). [[DON18](#DON18)]\n",
    "\n",
    "* Study [Chapter 8: Parts-of-Speech Tagging](https://web.stanford.edu/~jurafsky/slp3/8.pdf) of Speech and Language Processing [[JUR18]](#JUR18). The chapter deals with Hidden Markov Models, Maximum Entropy Markov Models and Parts-of-Speech Tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a fragment from the Penn Treebank dataset provided in the [NLTK toolkit](#http://www.nltk.org/nltk_data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = treebank.tagged_sents(tagset='universal')\n",
    "train_set, test_set = train_test_split(tagged_sentences,test_size=0.2,random_state=12)\n",
    "print('{} train sentences'.format(len(train_set)))\n",
    "print('{} test sentences'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a `list of list of (string, string)`. Each list is a sentence, each (string, string) tuple represents a (word, POS tag)\n",
    "\n",
    "#### Example usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in tagged_sentences:\n",
    "    for word, tag in sentence:\n",
    "        print(word.ljust(15), tag)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Hidden Markov Model, we used words as observations directly. For example, if we observed the sentence \"Hello lovely world\" the observation sequence would be\n",
    "> hello → lovely → world\n",
    "\n",
    "We might make some minor generalisations like turning all words lowercase or removing diacritics from letters like ê, é and è.\n",
    "\n",
    "A Maximum Entropy Markov Model on the other hand offers more liberty - We extract features from words in the context of the word sequence, i.e. the sentence.\n",
    "\n",
    "A feature function $\\phi{}(x_{1:i}, s_i, s_{i-1}, i)$ is a function of\n",
    "* $i$ the token/position in the range from 1 to the length of the sequence\n",
    "* $x_{1:i}$ The word sequence from 1 to and including $i$\n",
    "* $s_i$ the tag at the $i$th position\n",
    "* $s_{i-1}$ the tag at the $i-1$th position\n",
    "\n",
    "This allows us to condition not just on the word itself, but also properties of the word, the current and previous tag and the word sequence so far. Each feature function can return\n",
    "* A 1 or 0, e.g. a function that answers 'is the word capitalised?'\n",
    "* A vector of 1s and 0s, e.g. a function that returns the previous tag. The returned vector has the length of the tag vocabulary. At the index of the previous tag, it contains a 1, otherwise 0s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a whole host of feature functions on the input to extract meaningful information on the word and some of its context. When we concatenate all our findings, we get a binary feature vector representing the input. The observation sequence \"Hello lovely world\" may look like this once piped through our functions:\n",
    "> $[0,0,1,0]$ → $[0,1,1,1]$ → $[1,1,0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a tag vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all tags that occur in the training set `train_set` and store them in a list `tag_vocabulary`. Then we can use the list index as a unique ID for each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_vocabulary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run this test to verify your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tag_vocabulary(vocab):\n",
    "    expected_vocabulary = ['CONJ', 'NOUN', 'ADJ', 'PRT', 'DET', 'VERB', 'NUM', 'X', '.', 'ADV', 'PRON', 'ADP']\n",
    "    assert type(vocab) == type([]), \"Not a list\"\n",
    "    assert len(vocab) == len(set(vocab)), \"Duplicate tag found\"\n",
    "    assert set(vocab) == set(expected_vocabulary), \"Does not match expected vocabulary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag_vocabulary(tag_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is the first step towards constructing feature functions. The same word can have different tags depending on the context, for example 'book' can be a noun in\n",
    "> She is reading a book.\n",
    "\n",
    "or a verb in\n",
    "> Could you book the flight?\n",
    "\n",
    "The challenge is to find features in the word itself and the context to assess whether a specific instance of 'book' is a noun or a verb. Before we worry about the context though, we start with a strong indicator of what the POS tag for a term may be: Its most frequent tag in the training data.\n",
    "\n",
    "For each term, find the tag it was associated with most frequently in the training data. Store your result as a dict mapping each term to its most frequent tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_tag = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_most_frequent_tag(words_and_tags):\n",
    "    assert 'say' in words_and_tags.keys()\n",
    "    assert words_and_tags['be'] == 'VERB'\n",
    "    assert words_and_tags['book'] == 'NOUN'\n",
    "    \n",
    "test_most_frequent_tag(most_frequent_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suffixes and Prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suffixes and prefixes of a word can be a meaningful indicator of their tag. This is especially helpful in dealing with unfamiliar words. Even if our classifier has never encountered the word *serendipity* it could make an educated guess that it's a noun because of the suffix *-ity*. \n",
    "\n",
    "Research common suffixes and prefixes of various parts-of-speech categories and store them. Here is a small example. Expand the dictionary `common_suffixes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_suffixes = {\n",
    "    'able' : 'ADJ',\n",
    "    'ship' : 'NOUN',\n",
    "    'ity' : 'NOUN',\n",
    "    'ing' : 'VERB'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those suffixes and prefixes, implement a feature function that returns a binary vector. Each cell in the returned vector represents a tag (NOUN, VERB, ...). The value of a cell is 1 if the word contains a suffix/prefix that hints towards the correspoding tag.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "`contains_suffix('progamming')` should return:\n",
    "\n",
    "`array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])` \n",
    "\n",
    "when `tag_vocabulary[5]` returns `'VERB'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_suffix(word):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a suffix-tag dict like above, you can run this test to verify your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_contains_suffix():\n",
    "    noun_idx = tag_vocabulary.index('NOUN')\n",
    "    \n",
    "    msg = 'Only the noun suffix in \"divinity\" should be 1'\n",
    "    divinity = contains_suffix('divinity')\n",
    "    assert divinity[noun_idx] == 1 and divinity.sum() == 1, msg\n",
    "    \n",
    "    msg = 'The suffix hints for \"qwerty\" should be all zero'\n",
    "    qwerty = contains_suffix('qwerty')\n",
    "    assert np.all(qwerty == np.zeros_like(qwerty))\n",
    "\n",
    "test_contains_suffix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that checks whether or not certain criteria are true or false for a given word. Some criteria you may want to test are (you should implement more).\n",
    "* Is the word uppercase?\n",
    "* Is it a valid number (12, -31.31, thousand)?\n",
    "* Is it a short word?\n",
    "\n",
    "**Example:**\n",
    "\n",
    "For only the proposed features, `word_features(\"Yes\")` should return:\n",
    "\n",
    "`array([True, False, True])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(word):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and see if the features you extracted from the word itself match your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ('Italian', 'aims', 'R2-D2'):\n",
    "    print('{} {}'.format(word.ljust(10), word_features(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that transforms each word in a sentence into a feature vector. As mentioned before, you can stack together as many functions of the word, the word sequence so far, the previous tag and the current tag as you please. You may want to include the following features in your result:\n",
    "\n",
    "* If the term is known, its most frequent tag\n",
    "* Likely tags given any prefixes/suffixes of the term\n",
    "* Features of the word itself (is it uppercase, does it contain digits, etc.)\n",
    "* Whether or not the word follows certain keywords like *will* or *would*\n",
    "* The previous tag in the sequence\n",
    "\n",
    "Concatenating all these findings should produce a binary vector encoding an observation as features of the term and its context, which our classifier conditions on to predict POS tags.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "`word_to_feature_vector(\n",
    "    word_sequence = ['Hello', 'lovely', 'world'],\n",
    "    nth = 0,\n",
    "    previous_tag = -1,\n",
    "    current_tag = None\n",
    ")`\n",
    "\n",
    "could for exmaple return (depending on the features you use):\n",
    "\n",
    "`array([False, False, False, False, False, False, False, False, False,\n",
    "       False, False, False, False, False, False, False, False, False,\n",
    "       False, False, False, False, False, False,  True, False, False,\n",
    "       False, False, False, False, False, False, False, False, False,\n",
    "       False, False, False, False, False, False, False, False, False,\n",
    "       False, False, False, False, False, False, False, False, False,\n",
    "       True, False, False])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_feature_vector(word_sequence, nth, previous_tag, current_tag):\n",
    "    '''\n",
    "    Parameters:\n",
    "    word_sequence : list of string\n",
    "         a sentence\n",
    "    nth : int\n",
    "         the function returns the n-th word i.e. word_sequence[nth]\n",
    "         as a feature vector\n",
    "    previous_tag : int\n",
    "         id of the tag of word_sequence[nth-1], or -1 if the word is\n",
    "         at the beginning of the sequence\n",
    "    current_tag : int\n",
    "         id of the tag of word_sequence[nth]\n",
    "\n",
    "    Returns:\n",
    "    feature_vector : np.array of bool\n",
    "         a binary vector encoding information about the nth-word and the context\n",
    "         passed to this function\n",
    "    '''\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Hello' in the sentence 'Hello lovely world'\n",
    "# as a feature vector\n",
    "word_to_feature_vector(\n",
    "    word_sequence = ['Hello', 'lovely', 'world'],\n",
    "    nth = 0,\n",
    "    previous_tag = -1,\n",
    "    current_tag = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the train sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the tagged sentences is a list of list of (word : string, tag: string). We'll now\n",
    "* transform each word into its feature vector representation\n",
    "* transform each tag into its index in the tag vocabulary\n",
    "* flatten the tagged sentences into a single list of (word, previous tag)\n",
    "\n",
    "The training examples $X$ will be a list of tuples $(w_i,t_{i-1})$ where $w_i$ is the i-th word transformed into a feature vector and $t_{i-1}$ is the index of the tag associated with the previous word in the sentence. If $w_i$ is the first word of a sentence, $t_{i-1}$ will take on the special `START` index `-1`.\n",
    "\n",
    "Note that the previous tag $t_{i-1}$ in the tuple is not intended as a feature, we've already taken care of that in the `word_to_feature` function. In the learning process, our MEMM classifier uses a different set of weights for each previous tag. So $t_{i-1}$ determines which set of weights to train.\n",
    "\n",
    "The true labels $y$ is a list of int $[t_0 \\dots t_m]$ indicating the tag at the $i$th token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_train_corpus(tagged_sentences):\n",
    "    raise NotImplementedError()\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation of `transform_train_corpus` is correct, the next cell should print some output similar to this:\n",
    "\n",
    "```\n",
    "First word feature vector and index to previous toke (here start = -1):\n",
    "(array([False, False, False, False, False, False, False, False, False,\n",
    "       False,  True, False, False, False, False, False, False, False,\n",
    "       False, False, False, False, False, False,  True, False, False,\n",
    "       False, False, False, False, False, False, False, False, False,\n",
    "       False, False, False, False, False, False, False, False, False,\n",
    "       False, False, False, False, False, False, False, False, False,\n",
    "        True, False, False]), -1)\n",
    "\n",
    "First word tag:\n",
    "10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = transform_train_corpus(train_set)\n",
    "print('First word feature vector and index to previous toke (here start = -1):')\n",
    "print(X_train[0])\n",
    "print('\\nFirst word tag:')\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of elements in X_train with the previous tag -1 is equal\n",
    "# to the number of sentences in the training set\n",
    "assert sum(1 for w,t_prev in X_train if t_prev==-1) == len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A MEMM skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll implement the functions `fit`, `gradient_descent` and `decode` in the following exercises. `__init__` can remain blank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEMM():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def gradient_descent(self,X,y,epochs,learning_rate):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def decode_sentence(self,X):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how the class will be used.\n",
    "```python\n",
    "memm = MEMM()\n",
    "memm.fit(X_train,y_train)\n",
    "memm.gradient_descent(X_train,y_train,epochs=15,learning_rate=1.0)\n",
    "y_pred = memm.decode_sentence(X_test[0])\n",
    "accuracy = np.mean(np.array(y_pred) == np.array(y_test))\n",
    "print('accuracy for the first sentence: {}'.format(accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we compute scores over the output classes as the dot product of weights and the input. We pipe the scores through the [softmax function](https://en.wikipedia.org/wiki/Softmax_function) to obtain a probability distribution over the output classes.\n",
    "$$\n",
    "p(y) = softmax(W \\cdot x)\n",
    "$$\n",
    "Our MEMM classifier trains a different weights matrix $W^s$ for each possible previous state $s$. This allows you to compute a probability distribution over tags for a given input word, conditioned on the previous tag in the sequence.\n",
    "$$\n",
    "p(s_i \\mid s_{i-1}=s) = softmax(W^s \\cdot x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Implement the fit function which fits the instance of the MEMM to the training examples `X` and ground truth `y`.\n",
    "\n",
    "`n_classes` is the number of possible tags.\n",
    "`n_features` is the number of elements per feature vector (word). We can infer these parameters from the training examples $X$ and $y$.\n",
    "\n",
    "`W_given_prev` is an array of weights matrices, one for each possible previous tag in the sequence. `W_given_start` is a single weights matrix for the special case when the input is at the beginning of a sequence. Their shape depends on the number of features and classes.\n",
    "\n",
    "Initilise `W_given_start` and `W_given_prev` with normal distribution, e.g. `np.random.randn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self,X,y):\n",
    "        self.n_classes = None\n",
    "        self.n_features = None\n",
    "        self.W_given_start = None\n",
    "        self.W_given_prev = []\n",
    "        \n",
    "        \n",
    "MEMM.fit = fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memm = MEMM()\n",
    "memm.fit(X_train,y_train)\n",
    "assert memm.n_classes == len(tag_vocabulary)\n",
    "assert len(memm.W_given_prev) == len(tag_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our input $X$ is tuples of words (as feature vectors) and previous tags. We'll split the input $X$ into $K+1$ subsets, separated by their previous tag. Our classifier has $K+1$ weight matrices, given each previous state.The necessary steps in detail:\n",
    "\n",
    "Split `X` into $K+1$ subsets $x^i$ with $K$ the number of our classes (tags).\n",
    "\n",
    "Then calculate:\n",
    "$$\n",
    "z^i_j = x_1^i \\cdot w_{1,j} + x_2^i \\cdot w_{2,j} + \\ldots + x_n^i \\cdot w_{n,j}\n",
    "$$\n",
    "with \n",
    "* $j \\in \\{ 1, 2,\\ldots, K\\}$,\n",
    "* $n$ the number of features\n",
    "\n",
    "Implement the softmax function and use it inside of `gradient_descent` to calculate the predicted probabilies of $x^i$ belonging to class $j$:\n",
    "\n",
    "$$\n",
    "\\hat y^i_j = \\sigma(z^i)_j = \\frac{e^{z^i_j}}{\\sum_{k=1}^K e^{z^i_k}}\n",
    "$$\n",
    "\n",
    "For one epoch (or mini-batch) do:\n",
    "* Calculate the gradient of the cross entropy cost:\n",
    "\n",
    "$$\n",
    "\\nabla cost_j = \\begin{pmatrix}\n",
    "\\frac{\\partial cost_j}{\\partial w_{1,j}} \\\\\n",
    "\\frac{\\partial cost_j}{\\partial w_{2,j}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial cost_j}{\\partial w_{n,j}} \\\\\n",
    "\\end{pmatrix} =(\\hat y_j - y_j) \\cdot X^T\n",
    "$$\n",
    "\n",
    "* Update the wheight matrices with the update rule $\\forall e \\in \\{1,2,\\ldots,n\\}$ :\n",
    "$$\n",
    "w_{e,j}^{new} = w_{e,j}^{old} - \\alpha \\frac{\\partial cost_j}{\\partial w_{e,j}}\n",
    "$$\n",
    "    \n",
    "**Hint**\n",
    "\n",
    "* Avoid loops when possible, e.g. $Z$ or all $X$ (or minibatch) can efficiently be computed with the matrix dot product: $Z = W \\cdot X^T$.\n",
    "* Of course you are free to implement as many helper functions as you like, e.g. for the softmax function or the update rule\n",
    "Each subset of $X$ is used to train the corresponding weights using gradient descent. If you like, you can use mini-batch gradient descent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(self,X,y,epochs,learning_rate):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "MEMM.gradient_descent = gradient_descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding with Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to decode whole sequences at a time. We'll pass a sentence as a list of untransformed words to the decoding function. It computes and returns the most likely sequence of tags for the sentence.\n",
    "\n",
    "Remember that the decoding function deals with unfamiliar sentences, which means the tag sequence is unkown. For each word, the Viterbi algorithm considers each possible previous tag, which affects the value passed to the `previous_tag` parameter in the `word_to_feature` function.\n",
    "\n",
    "Implement the viterbi algorithm:\n",
    "\n",
    "#### 1. Initialisation\n",
    "For the first word of a sentence we multiply our trained `W_given_start` ($W_0$) probabilities with the feature vector of our first word $x^0$ and store the previous tag (`start = -1`) in the backpointer. In order to also receive probability values for our unknown sentences, also apply the sotmax function ($\\sigma$) here.\n",
    "$$\n",
    "\\vec{viterbi_{0}} = \\sigma(W_{0} \\vec x^{0^T})\\\\\n",
    "\\vec{backpointer}_{0} = -1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Recursion\n",
    "In the recursion step the computation of the next viterbi vector includes the provious one.\n",
    "\n",
    "We calculate the likelihood of moving from times step (or token postions) $t-1$ to $t$ and observing the different tags, producing a Matrix of likelihood vectors ${V}_t = (\\vec v_1, \\vec v_2, \\dots \\vec v_K)$.\n",
    "In the viterbi table, we store the maximum vector, in the backpointer its index, which equals the tag index.\n",
    "\n",
    "$$\n",
    "V_{t} = (\\vec v_1, \\vec v_2, \\dots \\vec v_K) = \\sigma(viterbi_{t-1}W_{b} \\vec x^{t^T})\\\\\n",
    "\\vec{viterbi_{t}} = \\max_{k=1}^K \\vec{v}_k \\\\\n",
    "\\vec{backpointer}_{t} = \\text{arg} \\max_{k=1}^K \\vec{v}_k\n",
    "$$\n",
    "\n",
    "#### 3. Finalisation\n",
    "\n",
    "To get the most likely tag sequence, trace back the indizes in $backpointer$ and keep track of them:\n",
    "\n",
    "The index (scalar) of the most likely last word is:\n",
    "$$\n",
    "backpointer_{Last, Max} = \\text{arg} \\max_{k=1}^K \\vec{viterbi}_{Last, k}\n",
    "$$\n",
    "\n",
    "$backpointer_{Last, Max}$ then points to the index of the most likely word in the vector $\\vec{backpointer}_{Last-1}$ and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentence(self, sentence):\n",
    "    # sentence : list of string, the word sequence to tag\n",
    "    # returns : list of int, the most likely tag sequence\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "MEMM.decode_sentence = decode_sentence\n",
    "print(memm.W_given_start.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_sentences(tagged_sentences):\n",
    "    X,y = [],[]\n",
    "    for sent in tagged_sentences:\n",
    "        words,tags = list(zip(*sent))\n",
    "        X.append(words)\n",
    "        y.append([tag_vocabulary.index(t) for t in tags])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,y_test = transform_test_sentences(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memm = MEMM()\n",
    "memm.fit(X_train, y_train)\n",
    "memm.gradient_descent(X_train,y_train,epochs=3,learning_rate=1.0)\n",
    "predicted_sentences = [memm.decode_sentence(x) for x in X_test]\n",
    "\n",
    "from itertools import chain\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = list(chain.from_iterable(y_test))\n",
    "y_pred = list(chain.from_iterable(predicted_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compare predicted tags with true tags for irst sentence:\n",
    "\n",
    "print('word\\t\\tlabel\\tpredicted')\n",
    "for i in range(len(X_test[0])):\n",
    "    print('{}\\t\\t{}\\t{}'.format(X_test[0][i], tag_vocabulary[y_true[i]], tag_vocabulary[y_pred[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementations are correct, the next cell should print something similar to the following. The numbers on the diagonal of the confusion matrix mark the correctly predicted words:\n",
    "    \n",
    "```\n",
    "[[ 641    0    0    7    0    4    9    0    0    0    0    2]\n",
    " [   0 1610    0   63    1    1    1    0    1    0    0   16]\n",
    " [   0    2 1325    1    0    1    1    9    1    1    0    2]\n",
    " [   9    0    0 1883    0    2   11    0    5    0    1    2]\n",
    " [   0    0    0    0  530    8    0    2    1    2    0    4]\n",
    " [   4    3    0   11    1 2451    6    0   28    9    2  178]\n",
    " [   9    6    0   33    1    8  529    0   64    2    2    3]\n",
    " [   1    3    2    4    0    3    0  711    3    0    1   11]\n",
    " [   1    6   52    3    0   38   20    0  966    7    0   97]\n",
    " [   1    0    0    0    0    0    0    0    0 2355    0    0]\n",
    " [   0    7    0    0    0    3    8    0    0    0  403    3]\n",
    " [   2   23   15   15    0   78   32    1  145   18    7 5274]]\n",
    "PRT     641 out of 663   (96.7 %)\n",
    "DET    1610 out of 1693  (95.1 %)\n",
    "X      1325 out of 1343  (98.7 %)\n",
    "ADP    1883 out of 1913  (98.4 %)\n",
    "PRON    530 out of 547   (96.9 %)\n",
    "VERB   2451 out of 2693  (91.0 %)\n",
    "ADV     529 out of 657   (80.5 %)\n",
    "NUM     711 out of 739   (96.2 %)\n",
    "ADJ     966 out of 1190  (81.2 %)\n",
    ".      2355 out of 2356  (100.0 %)\n",
    "CONJ    403 out of 424   (95.0 %)\n",
    "NOUN   5274 out of 5610  (94.0 %)\n",
    "Average accuracy:  0.9420012104095219 %\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_true,y_pred)\n",
    "np.set_printoptions(linewidth=float('inf'))\n",
    "print(conf_mat)\n",
    "for i in range(conf_mat.shape[0]):\n",
    "    print('{:<5} {:5} out of {:<5} ({:2.1f} %)'.format(tag_vocabulary[i], conf_mat[i,i], conf_mat[i].sum(), 100 * conf_mat[i,i]/conf_mat[i].sum()))\n",
    "print('Average accuracy: ', np.trace(conf_mat)/np.sum(conf_mat), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label-Bias Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEMMs suffer from a problem known as the *label bias problem*. Essentially,\n",
    "* states with few succeeding states are preferred\n",
    "* states with a single outgoing state ignore their observation entirely.\n",
    "\n",
    "Read up on the label bias problem in Chapter 2 of Lafferty et. al's [paper on Conditional Random Fields](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers) [[LAF01]](#LAF01). Then walk through the example below.\n",
    "\n",
    "Given is the following train data set $(x_{1:T}, h_{1:T})$:\n",
    "\n",
    "$$\n",
    "\\mathcal D_{train} = \\{(rib,123), (rib,123), (rib,123), (rob,456), (rob,456)\\}\n",
    "$$\n",
    "\n",
    "- $h_{1:T}$: Hidden sequence\n",
    "- $x_{1:T}$: Observed Sequence\n",
    "- Start state: $h_0 = *$\n",
    "- Hidden state vocabulary: $\\{*,1,2,3,4,5,6\\}$\n",
    "- Observation vocabulary: $\\{r,i,o,b\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:** Give the following maximum likelihood estimates of $p(h_t \\mid x_t, h_{t-1})$:\n",
    "\n",
    "- $P(1 \\mid r, *) = ?$\n",
    "- $P(4 \\mid r, *) = ?$\n",
    "- $P(2 \\mid i, 1) = ?$\n",
    "- $P(3 \\mid b, 2) = ?$\n",
    "- $P(5 \\mid o, 1) = ?$\n",
    "- $P(6 \\mid b, 5) = ?$\n",
    "\n",
    "**Note:** In the training data, state 2 is the only successor of state 1. Therefore $P(2 \\mid 1) = 1$, so $P(2 \\mid x, 1) = 1$ for all observations $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** What are the probabilities of the hidden state sequences 123 and 456 given the observation sequence 'rob'?\n",
    "- $P(123 \\mid rob) = ? $\n",
    "- $P(456 \\mid rob) = ?$\n",
    "\n",
    "Discuss the solution. Is that what you have expected? Relate the solution to the label-bias problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts-of-speech tagging is a disambiguation task. Sometimes a term unambiguously belongs to a certain part of speech. In those cases assigning a tag is trivial. With unfamiliar words, features of the word can provide a reliable hint, for example their suffix or if the word shape resembles a date. At other times, we have to look at the context to predict the part of speech. Recall the example from the introduction: Which feature could reveal whether an *book* is a noun or a verb? If the previous word is 'a' or 'the', this could indicate it's a noun in this specific sentence.\n",
    "\n",
    "Have a look at the confusion matrix and see which classification your MEMM classifier struggles with. I'll leave you with this `highlight_errors` function, which prints senteces in which a word in the category `actual` was incorrectly classified as `predicted`. Can you identify patterns in the sentence or word that hint, or even imply a certain parts of speech? You could expand your feature functions to extract those patterns and make more well-informed predicitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_errors(actual,predicted):\n",
    "    for i,sent in enumerate(y_test):\n",
    "        for j,tag in enumerate(sent):\n",
    "            if tag == tag_vocabulary.index(actual):\n",
    "                prediction = predicted_sentences[i][j]\n",
    "                if prediction == tag_vocabulary.index(predicted):\n",
    "                    sentence = X_test[i]\n",
    "                    word = sentence[j]\n",
    "                    print(sentence)\n",
    "                    print(word)\n",
    "                    print('Actual: {}, Predicted: {}'.format(actual,predicted))\n",
    "                    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_errors(actual='NOUN', predicted='ADJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"DON18\"></a>[DON18]\n",
    "        </td>\n",
    "        <td>\n",
    "            N. Donges and N. Donges, “Gradient Descent in a Nutshell – Towards Data Science,” Towards Data Science, 07-Mar-2018. [Online]. Available: <a href='https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0'>https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0</a>. [Accessed: 30-Jan-2019].\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"JUR18\"></a>[JUR18]\n",
    "        </td>\n",
    "        <td>\n",
    "            Daniel Jurafsky, James H. Martin. \"Speech and Language Processing\". 3rd ed. draft n.p. Draft of Sep 23, 2018 published at <a href='https://web.stanford.edu/~jurafsky/slp3/'>https://web.stanford.edu/~jurafsky/slp3/</a> [accessed Jan 19 2018]\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"LAF01\"></a>[LAF01]\n",
    "        </td>\n",
    "        <td>\n",
    "            Lafferty, John, Andrew McCallum, and Fernando CN Pereira. \"Conditional random fields: Probabilistic models for segmenting and labeling sequence data.\" (2001).\n",
    "            </a>.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "_Text Information Extraction - Sequences - Maximum Entropy Markov Model_ <br/>\n",
    "by _[Christian Herta](#https://www.htw-berlin.de/hochschule/personen/person/?eid=2605), Diyar Oktay_ <br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 _[Christian Herta](#https://www.htw-berlin.de/hochschule/personen/person/?eid=2605), Diyar Oktay_\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
